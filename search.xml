<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2017%2F12%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[数据结构在计算机科学中，数据结构(data structure)是计算机中存储、组织数据的方式。大多数数据结构都有数列、记录、可辨识联合、引用等基本类型构成。 数据结构意味着结构和封装，一个数据结构可被视为两个函数之间的接口，或是由数据类型联合组成的存储内容的访问方法和封装。数据结构可通过程序语言所提供的数据类型、引用及其它操作加以实现。不同种类的数据结构适合不同种类的应用，部分数据结构甚至是为了解决特定问题而设计。一个涉及良好的数据结构，应该尽可能使用较少的时间与空间资源的前提下，支持各种程序运行。 正确选择数据结构可以提高算法的效率，在计算机程序设计里，选择适当的数据结构是一项重要工作。 常见数据结构 数组(Array); 栈(Stack): 后进先出，线性表； 队列(Queue): 先进先出，线性表； 链表(Linked List): 每个节点包括两部分，一个存储数据元素的数据域，另一个存储下一个节点地址的指针域； 树(Tree)； 图(Graph)； 堆(Heap): 一种动态树形结构； 散列表(Hash)； 数组(Array)数组数据结构，是由相同类型的元素的集合所组成，分配一块连续的内存来存储。利用数组元素的索引(index)可计算出元素对应存储地址。 数组有 一维数组、二维数组、多维数组、可变长数组…。 栈(Stack)堆栈又称为栈，是计算机科学中一种特殊的串列形式的抽象资料类别。其特殊之处在于只能允许在链接串列或阵列的一端(栈顶指标:top)，进行加入数据(push)和取出数据(pop)。 由于栈数据结构只允许在一端进行操作，因为按照后进先出(LIFO, last-in-first-out)的原理运行。 队列(Queue)队列，是先进先出(FIFO, first-in-first-out)的线性表。在具体应用中通常用链表或数组来实现。队列只允许在后端(Rear)进行插入操作，在前端(Front)进行删除操作。 链表(Linked List)链表是一种线性表，但并不按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表再插入的时候可以达到 O(1)的时间复杂度，比另一种线性表顺序表快得多。但查找一个节点或访问特定节点则需要 O(n)的时间，而顺序表相应的时间复杂度分别是 O(logn)和O(1)。 是用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了节点的指针域，空间开销比较大。 链表有单向链表、双向链表、循环链表…。链表用来构建许多其它数据结构，如栈，队列和他们的派生。 树(Tree)树是一种抽象数据类型，用来模拟具有树状结构性质的数据集合。 树有有序树、无序树（二叉树，B树，霍夫曼树）。 图(Graph)在数学上，一个图是表示物体与物体之间的关系的方法，是图论的基本研究对象。 图有：有向图、无向图、简单图、多重图。 堆(Heap)堆是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵树的数组对象。在队列中，调度程序反复提取队列中的第一个作业并运行，因为实际情况中某些时间较短的任务将等待很长时间才能结束，或者某些不短小，但具有重要性的作业，同样应当具有优先权。堆即为解决此类问题设计的一种数据结构。 堆常用于排序，这种算法称作堆排序。 散列表(Hash)散列表也叫哈希表，是根据键(key)而直接访问在内存存储位置的数据结构。它通过计算一个关于键值的函数，将所需查询的数据映射到表中的一个位置来访问记录，这加快了查找速度。这种映射函数称为散列函数，存放记录的数组称为散列表。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB]]></title>
    <url>%2F2017%2F12%2F11%2FMongoDB%2F</url>
    <content type="text"><![CDATA[参考： MongoDB官方文档; MongoDB中文文档; https://zh.wikipedia.org/wiki/MongoDB; http://www.ywnds.com/?p=5635; https://www.centos.bz/2017/08/mongodb-secure-intro-user-auth/; http://www.03sec.com/3176.shtml; http://www.ywnds.com/?p=6502; http://wiki.jikexueyuan.com/project/the-little-mongodb-book/; 环境： CentOS7_x64； MongoDB3.4； NoSQLNoSQL(Not Only SQL)是对不同于传统的关系型数据库的数据库管理系统(DBMS)的统称。NoSQL不使用SQL作为查询语言，其数据结构可以不需要固定的表格模式，有横向可扩展性的特征。NoSQL用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无序多余操作就可以横向扩展。 关系型数据库的典型实现主要被调整用于执行规模小而读写频繁，或大批量极少写访问的事务。当代典型的关系型数据库在一些数据敏感的应用中表现了糟糕的性能。例如： 为巨量文档创建索引； 高流量网站的网页服务； 发送流媒体。 NoSQL数据库分类： 类型 栗子 特点 文档存储 MongoDB 用类似json的格式存储，存储的内容是文档型的。这样就有机会对某些字段建立索引，实现关系数据库的某些功能 图形关系存储 Neo4j 图形关系的最佳存储 键-值(ker-value)存储 最终一致性的键-值存储 架构性键-值存储 xxx 主机式服务 key-value硬盘存储 key-value RAM存储 MemcacheDB Redis 多数据库 OpenQM xxx 时序型数据库 Graphite xxx 对象数据库 ObjecStore 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据 列存储 HBase 顾名思义，按列存储数据。方便存储结构化和半结构化数据，方便做数据压缩，针对某一列或某几列的查询有很大的IO优势。 MongoDB简介 MongoDB(https://www.mongodb.com/)，是一种文档导向的数据库管理系统，由C++撰写而成，以此来解决应用程序开发社区中的大量现实问题。它是一种NoSQL。MongoDB支持的数据结构非常松散，是类似于json的bson格式，因此可以存储比较复杂的数据类型。MongoDB是一个开源文档数据库，提供高性能，高可用性和自动扩展。 预备知识： MongoDB中的database有和数据库一样的概念。一个MongoDB实例中，可以有零个或多个数据库，每个都作为一个高等容器，用于存储数据； MongoDB数据库中有零个或多个collections(集合)。集合类似于传统意义上的table(表)； MongoDB的集合是由零个或多个documents(文档)组成。文档类似于row(行)； MongoDB的文档由零个或多个fields(字段)组成。字段类似于columns(列)； MongoDB中Indexes(索引)扮演的角色与RDMS中一样； MongoDB中的Cursors(游标)很重要，当你向MongoDB取数据的时候，它会给你返回一个结果集的指针而不是真正的数据，这个指针我们叫它游标。我们可以用游标做任何事情，比如计数或跨行之类。 MongoDB特点不如这样认为，MongoDB是关系型数据库的一个代替案。比如用Lucene作为关系型数据库的全文检索索引的加强，或者是Redis作为持久性key-value存储。 无模式(Flexible Schema)：它不需要一个固定的模式，这使得他们比传统的数据库表要灵活更多。 写操作(Writes)：MongoDB可以胜任的一个特殊角色是在日志领域。有两点使得MongoDB的写操作非常快： 可以选择发送了写操作之后立刻返回，而无需等到操作完成； 可以控制数据持久性的写行为。 高性能(High Performance)：MongoDB提供了高性能的数据持久性。尤其是： 对嵌入式数据模型的支持减少了数据库系统上的I/O活动； 索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。 高可用(High Availability)：MongoDB的复制工具，称为副本集。提供：自动故障转移和数据冗余。 持久性(Durability)：在MongoDB中，日志(Journaling)是默认启动的，该功能允许快速恢复服务器，比如遭遇到了服务器奔溃或停电的问题。 丰富的查询语言(Rich Query Language)：MongoDB支持丰富的查询语言来支持读写操作(CRUD)，数据聚合(Data Aggregation)，全文搜索(Text Search)。 水平可伸缩性(Horizontal Scalability)：MongoDB提供了横向可伸缩性。 支持多个存储引擎(Support for Multiple Storage Engines)：在MongoDB3.2以后默认引擎为: WiredTiger Storage Engine，允许第三方为MongoDB开发存储引擎。 database和collectionMongoDB stores BSON documents. databasesIn MongoDB,databases hold collections of documents.如果一个数据库不存在，当你第一次存储数据时，MongoDB会自动创建数据库。这意味着可以切换到不存在的数据库。 默认情况下，集合不要求其文档具有相同的模式；文档不要求具有相同的字段集；字段的数据类型在集合的文档间可以有所不同。 123456789#select a dbuse &lt;db&gt;#create a dbuse newdbdb.newcoll.insert(&#123;name:'zhang'&#125;)db.newcoll.insert(&#123;filed01:'filed01', filed02:'filed02', filed03:'filed03', filed04:'filed04'&#125;)db.newcoll.insert(&#123;groups: ['A', 'B', 'C']&#125;)db.newcoll.find().pretty() collectionMongoDB stores documents in collections.collection类似于关系型数据库中的table。 12db.coll02.insert(&#123;x:1&#125;)db.coll03.createIndex(&#123;y:1&#125;) 显式创建(explicit creation)MongoDB提供了db.createCollection()方法来显式创建一个附带各种选项的集合。如设置document最大大小，文件验证规则等选项。如果不需要指定这些选项，就不需要使用显式创建集合，而直接向集合中插入数据即可。修改collection选项，使用collMod方法。 视图(View)视图的定义是公开的，视图的解释操作将包括定义视图的管道。因此，避免直接引用视图定义中的敏感字段和值。 创建/删除视图： 12345678910db.runCommand(&#123; crete: &lt;view&gt;, viewOn: &lt;source&gt;, pipeline: &lt;pipeline&gt;&#125;)db.createView(&lt;view&gt;, &lt;source&gt;, &lt;pipeline&gt;, &lt;collation&gt;)db.collection.drop() 视图行为： 视图存在以下行为： 视图只读，视图上的写操作将会出错； 视图使用底层集合的索引； 如果视图的基础集合被分割，视图也被认为可分割； 不能重命名视图； 视图上的字符串使用视图的默认排序规则。 限制集限制集是固定大小的集合支持基于文档插入顺序的高吞吐率的插入、检索、删除操作。限制收集工作在某种程度上类似于循环缓冲区：一旦一个文档填满分配给它的空间，它将通过在限制集中重写老文档来给新文档让出空间。 行为插入顺序限制集合能够保留插入顺序。因此，查询并不需要索引来保证以插入顺序来返回文档。减少了索引的消耗，限制集可以支持更高的插入吞吐量。 最旧文档的自动删除为了给新文档腾出空间，再不需要脚本或显示删除操作的前提下，限制集自动删除集合中最旧的文档。 例如replication set中的oplog.rs集合。考虑潜在用于集合封顶的用例： 存储高容量系统生成的日志信息。没有索引的情况下向限制集中插入文档的速度接近于直接在文件系统中写日志的速度； 在限制集中缓存少量的数据。 _id索引限制集合有一个_id字段并且默认在_id字段上创建索引。 限制和建议更新更新限制集中的文档，创建一个索引保证这些更新操作不需要进行集合扫描。 文档大小一个更新或替换操作改变了文档大小，操作将会失败。 文档删除不能从一个限制集中删除文档！为了从一个集合中删除所有文档，使用drop()方法来删除集合然后重新创建限制集。 分片不能对限制集分片。 查询效率用自然顺序监视限制集中大部分最近插入的文档。 程序创建一个限制集必须使用db.createCollection()方法创建限制集。且必须指定以字节为单位的最大集合大小。MongoDB将会预先分配集合。另外，可为限制集指定最大文档数据，用max字段。 大小参数是必须的。MongoDB会在达到最大限制前删除旧的文件。 1234567use &lt;db&gt;#限制集大小db.createCollection("log", &#123;capped: true, size: 1000000&#125;)#限制集和文档大小db.createCollection("log", &#123;capped: true, size: 5242880, max: 5000&#125;) 查询一个限制集如果没有对限制集指定排序，则MongoDB的结果顺序和插入顺序相同。 检查一个集合是否是限制集isCapped()方法 123db.collection.isCapped()#db.coll01.isCapped()#false 将集合转换为限制集convertToCapped()方法 123db.runCommand(&#123;"covertToCapped": "coll01", size: 1000000&#125;);#db.coll01.isCapped()#true 在规定的时间周期之后将自动移除数据通过设置MongoDB的TTL时集合中的数据过期。TTL collection与限制集不兼容。 Tailable游标类似于Unix中的taif -f documentMongoDB存储数据记录为BSON文档。BSON是JSON文档的二进制表示，因此它包含比JSON更多的数据类型。 document structureMongoDB字段由key-value对组成。字段值可以是任一BSON数据类型，包括其他文档，数组，阵列。 1234567891011121314151617181920212223&#123; filed1: value1; filed2: value2; ... filedN: valueN&#125;g#data typevar mydoc =&#123; _id: ObjectId("5099803df3f4948bd2f98391"), name: &#123; first: "Alan", last: "Turing" &#125;, birth: new Date('Jun 23, 1912'), death: new Date('Jun 07, 1954'), contribs: [ "Turing machine", "Turing test", "Turingery" ], views : NumberLong(1250000)&#125;_id是ObjectID；name是嵌入式文档；birth是日期类型；contribs是字符串数组；view是NumberLong类型。 字段名(field name)字段名是字符串。document对field name有以下限制: 字段名称_id保留用作主键(primary key)，它的值在collection中必须唯一，不可变。它的类型可以是数组外的任何类型； 字段名称不能以$字符开头； 字段名称不能包含.字符； 字段名称不能包含null字符。 BSON documents 可能有多个字段名称相同的字段。然而，大多数的MongoDB Interface，MongoDB结构（如hash表），并不支持重复字段名称。如果需要操作具有多个相同名称字段的文档，请参考 mongo driver。 一些由内部MongoDB进程创建的documents可能会有重复的字段，但是没有MongoDB进程会向一个已经存在的user document中添加重复字段。 字段值限制(field value limit)For indexed collections，indexed fields的值有一个最大索引值长度限制(maximum index key length)。 圆点表示法(dot notation)MongoDB使用圆点表示法来访问数组中的元素，访问嵌套文档中的字段。 数组(array)通过基于0的索引位置来指定或访问数组中的元素。 123456789&lt;array&gt;.&lt;index&gt;&#123; contribs: [ 'Turing machine', 'Turing test', 'Turingery' ]&#125;#contribs.0 == 'Turing machine'#contribs.1 == 'Turing test'#contribs.2 == 'Turingery' 嵌套文档(embedded documents)通过圆点表示法来指定或访问嵌套文档中的字段。 123456789&lt;embedded document&gt;.&lt;field&gt;&#123; name: &#123; first: 'AAA', last: 'ZZZ'&#125;, contact: &#123; phone: &#123; type: 'cell', number: '1-22-333' &#125;&#125;&#125;#name.first == 'AAA'#contact.phone.number == '1-22-333' 文档限制(document limitation)**文档大小限制(size limit)BSON document最大size为：16MB。 最大document size确保一个单一document不能使用过量的RAM，或是传输期间的过量带宽。MongoDB提供了GridFS API，用来保存超过最大size的文档。 文档字段序列(field order)MongoDB用write operation来作为document的序列，除了一下情况： _id字段总是document中的第一个field； 包含重命名的update操作，会导致document中的field重新排序。 _id字段在MongoDB中，每个保存在collection中的document都要求一个唯一的_id，用以担任主键(primary key)。如果向document中insert数据是忽略的_id字段，则MongoDB driver会为_id字段自动生成一个ObjectID。 1234567#默认生成_iddb.coll01.insert(&#123;name: 'zhang', sex: 'man', hobby: 'woman'&#125;)# "_id" : ObjectId("5a32166ebf2c986e8106f891")#自定义_iddb.coll01.insert(&#123;_id:'ZhangCustomDefine', name:'zhang', sex: 'man', arr: [0, 1, 2, 3], emmdoc: &#123;emm01:'Emm01', emm02: 'Emm02', emm03: 'Emmo3'&#125;&#125;)#"_id" : "ZhangCustomDefine" _id字段有以下行为和约束： 默认情况下，MongoDB在collection创建document时，会创建一个唯一的_id作为索引； _id字段总是document中的第一个字段。如果server接受的document中_id不在第一个字段，那么Server会移动_id到第一个字段； _id字段的数据类型除了数组外的任意BSON 数据类型； 不要存储BSON正则表达式的类型在_id字段中。 _id字段值的常用选项： 使用ObjectId； 使用了自然唯一的标识符，节省了空间并避免了额外的索引； 生成一个自动递增的数字； 在应用程序代码中生成UUID； 文档结构的其他用途查询过滤文档(query filter)使用:表达式来指定条件。 12345&#123; &lt;field1&gt;: &lt;value1&gt; &lt;field2&gt;: &lt;value2&gt; ...&#125; 更新特定文档(update)使用db.collection.update()操作更新数据。 BSON类型BSON是一个用来存储document和MongoDB进行远程调用的二进制序列化格式。BSON支持一下数据类型作为文档中的值。每个数据类型都有一个相应的数字和字符串别名，可与$type操作符一起使用，以便按照bson类型查询文档。 Type Number Alias double 1 “double” 字符串 2 “string” 对象 3 “object” 数组 4 “array” 二进制数据 5 “binData” 未定义 6 “undefined” ObjectId 7 “objectId” Boolean 8 “bool” 日期 9 “date” 空 10 “null” 正则表达式 11 “regex” DBPointer 12 “dbPointer” JavaScript 13 “javascript” 符号 14 “symbol” JavaScript(带范围) 15 “javascriptWithScope” 32位整数 16 “int” 时间戳 17 “timestamp” 64位整数 18 “long” Decimal128 19 “decimal” Min key -1 “minKey” Max key 127 — 如果你想要将BSON转换为JSON，参考Extended JSON。 ObjectIdObjcetIds are small, likely unique, fast to generate, and ordered.ObjectIds有12个字节组成，其中前4个字节是反映ObjectId创建的时间戳(timestamp)。 一个4字节的值，代表从Unix纪元开始的秒数； 一个3字节的机器标识符； 日期对象排在时间戳对象之前； MongoDB在比较过程中，会把一些类型看成相等。 栗子：{ &quot;_id&quot; : ObjectId(&quot;5a33354068b6c5e5fb6f213f&quot;), &quot;name&quot; : &quot;ZHANG&quot; }。 在mongo shell中，可以访问ObjectId的创建时间，使用ObjectId.getTimestamp()方法。在_id字段中存储的ObjectId值的排序，大致相当于按其创建时间排序。ObjectId的值顺序与生成时间之间并不严格。 字符串BSON字符串都是UTF-8编码。一般来说，每种编程语言的驱动程序在序列化和反序列化BSON的时候，都会从语言的字符串形式转化为UTF-8。这就使得使用BSON字符串简单存储大多数国际字符变为可能。 时间戳BSON有一个特殊的时间戳类型用于MongoDB内部使用，与普通的日期类型无关。而在应用开发中可使用BSON日期类型。时间戳值是一个64位的值： 前32位是与Unix纪元相差的秒数，后32位是在某秒总操作的一个递增的序列数。 在MongoDB复制集中，oplog有一个ts字段。这个字段的值使用BSON时间戳表示了操作时间。 1234db.coll02.insert( &#123; ts: new Timestamp() &#125; )db.coll02.find()#&#123; "_id" : ObjectId("5a333e3f68b6c5e5fb6f2141"), "ts" : Timestamp(1513307711, 1) &#125; 日期BSON日期是一个64位整数，表示利当前Unix新纪元(1970.01.01)的毫秒数，可到未来的2.9亿年。BSON日期类型是有符号的，负数表示1970年之前的时间。 123456var date1 = new Date()var date2 = ISODate()#date1#date2#ISODate(&quot;2017-12-15T03:28:08.227Z&quot;) MongoDB Extended JSONJSON只能表示BSON类型的一个子集。为了保留类型信息，MongoDB对JSON格式添加了如下扩展性： Strict mode： Any JSON parser can parse these strict mode representations as key/value pairs; mongo shell mode： The MongoDB internal JSON parser and the mongo shell can parse this mode. 多种数据类型的表示取决于JSON解析的上下文！ 解析器(parser)和支持的格式(format)Input in Strict mode如下可在严格模式下被解析并识别类型信息。 REST Interface; mongo import; –query; MongoDB Compass. Input in mongo shell mode如下可在严格模式下被解析并识别类型信息。 REST Interface; mongo import; –query; MongoDB Compass. Output in Strict modemongoexport, REST, HTTP Interfaces. Output in mongo shell modebsondump. BSON数据类型和关联表示Binary Strict mode mongo shell mode { “$binary”: ““, “$type”: ““ } BinData ( , ) 12&lt;bindata&gt;是二进制base64表示；&lt;t&gt;是由单字节的数据类型表示。 Date Strict mode mongo shell mode { “$date”: ““ } new Date ( ) 12In Strict mode, &lt;date&gt;是 ISO-8601的日期格式的时区字段，类型如**YYYY-MM-DDTHH:mm:ss.mm&lt;+/-offset&gt;;MongoDb JSON解析器目前暂不支持载入ISO-8601日期类型。 Timestamp Strict mode mongo shell mode { “$timestamp” ; { “t”: , “i” } } Timestamp( , ) 12&lt;t&gt;是32位无符号整数的JSON表现形式；&lt;i&gt;是增量的32位无符号整数。 Regular Expression Strict mode mongo shell mode { “$regex”: , “$options”: ““ } // 1234&lt;sRegex&gt;是有效地JSON字符串；&lt;jRegex&gt;是一个可能包含有效的JSON字符和未转义的双引号(&quot;)，但可能不包括未转义的斜杠(/)字符；&lt;sOptions&gt;是一个正则表达式选项；&lt;jOptions&gt;是一个只能包含字符&quot;g&quot;, &quot;i&quot;, &quot;m&quot;, &quot;s&quot;的字符串。 OID Strict mode mongo shell mode { “$oid”: ““ } ObjectId( ““ ) &lt;id&gt;是一个24字符的十六进制(hexadecimal)字符串 DB Reference strict mode mongo shell mode { “$ref”: ““, “$id”: ““ } DBRef(““, ““) 12&lt;name&gt;是一个有效的JSON字符；&lt;id&gt;是任一extended JSON type。 Undefined Type strict mode mongo shell mode { “$undefined”: true } undefined MinKey/MaxKey strict mode mongo shell mode { “$minkey”: 1 } MinKey { “$maxkey”: 1 } MaxKey NumberLong strict mode mongo shell mode { “$numberLong”: ““ } NumberLong( “number&gt;” ) 12Number是一个64位有符号整数。必须使用&quot;，否则它将被解释为浮点数，从而导致损失精度；db.json.insert&#123;&#123; longquoted: NumberLong(&quot;12345678901234345&quot;) &#125;) MongoDB安装参考: https://docs.mongodb.com/manual/administration/install-on-linux/; https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/; MongoDB有社区版(Community)和企业版(Enterprise)。社区版免费，企业版在商业方面收费。 MongoDB在仓库中提供官方支持的包，包含以下软件包： Package Description monogdb-org 将自动安装下面四个组件包 mongodb-org-server 包含mongod守护进程和相关配置和init脚本 mongodb-org-mongos 包含mongos守护进程 mongodb-org-shell 包含mongo-shell mongodb-org-tools 包含相关MongoDB工具，如mongoimport,mongoexport,mongodump,mongorestore… mongodb-org-server包提供了一个/etc/mongod.conf配置文件来开始和初始化mongod。默认配置文件默认bind_ip为 127.0.0.1，当你有需要和副本集时请修改它。 自建mongodb.repo仓库安装仓库地址：https://repo.mongodb.org 12345678910111213vim /etc/yum.repos.d/mongodb34.repo#编辑仓库[mongodb34]name=MongoDB34 Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/gpgcheck=0enabled=1#安装mongodbyum install -y mongodb-org 下载rpm包安装1234567cd /root/mongodbwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/3.4/x86_64/RPMS/mongodb-org-3.4.10-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/3.4/x86_64/RPMS/mongodb-xxx-3.4.10-1.el7.x86_64.rpm#共五个包yum ./mongo-org* 源码安装1234567wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.0.tgztar -axvf mongodb-linux-x86_64-rhel70-3.6.0.tgz -C ./#默认路径/usr/localmake &amp;&amp; make install 开启mongodb123456789101112131415161718192021#默认启动方式systemctl start mongod#指定配置文件启动#注意修改配置文件里面的某些路径和名称，不然会和默认配置文件冲突mongod -f /etc/mongo_27018.confmongod -f /etc/mongo_27019.conf``&lt;br&gt;## 卸载mongodb```shsystemctl stop mongodyum remove $(rpm -qa | grep mongodb-org)rm -rf /var/log/mongodbrm -rf /var/dbpath/mongo mongodb异常关闭后12345#首先查看日志文件tail /var/log/mongodb/mongod.log#删除rm /var/run/mongodb/mongod.pid /var/db/mongodb/mongod.lock MongoDB配置文件MongoDB的配置文件格式使用了YAML格式。YAML维基百科，Yet Another Markup Language。强调以数据为中心，而不是标记语言为重点，用方向缩略语重命名。 默认配置文件/etc/mongod.conf 的几个大块： 123456789101112131415161718192021systemLog: #日志storage: #存储processManagement: #进程管理net: #网络security: #安全operationProfiling: #性能分析器replication: #主从复制sharding: #架构setParameter: #自定义变量auditLog: #检测日志snmp: #简单网络管理协议 systemLog日志相关参数： 123456789101112131415systemLog: verbosity: &lt;int&gt; #日志级别，默认0,1-5均会包含debug信息 quiet: &lt;boolean&gt; #安静，true时mongod将会减少日志的输出量 traceAllExceptions: &lt;boolean&gt; #打印异常详细信息 syslogFacility: &lt;string&gt; #指定用于登录时信息到syslog Facility水平，前提是启用syslog path: &lt;string&gt; #日志路径，默认情况下，MongoDB将覆盖现有的日志文件 logAppend: &lt;boolean&gt; #mongod重启后，在现有日志后继续添加日志，否则备份当前日志，然后创建新日志 logRotate: rename|reopen #日志轮询，防止一个日志文件特别大。rename重命名日志文件，默认值；reopen使用Linuxrotate特性，关闭并重新打开日志文件，前提为logAppend: true destination: &lt;string&gt; #日志输出目的地，可为file或syslog，若不指定，则会输出到 std out timeStampFormat: &lt;string&gt; #指定日志格式的时间戳，有 ctime, Iso869-utc, iso8691-local component: #为不同的组件指定各自的日志信息级别 accessControl: verbosity: &lt;int&gt; command: verbosity: &lt;int&gt; storage存储引擎相关参数: 123456789101112131415161718192021222324252627282930313233storage: dbPath: &lt;string&gt; #mongodb进程存储数据目录，此配置进队此mongod进程有效，你使用配置文件开启的mongod就可以指定额外的数据目录 indexBuildRetry: &lt;boolean&gt; #当构件索引时mongod意外关闭，那么在此启动是否重建索引，默认true repairPath: &lt;string&gt; #在repair期间使用此目录存储临时数据，repair结束后此目录下数据将被删除 journal: enabled: &lt;boolean&gt; #journal日志持久存储，journal日志用来数据恢复，通常用于故障恢复，建议开启 commitIntervalMs: &lt;num&gt; #mongod日志刷新值，范围1-500毫秒，默认100，不建议修改 directoryPerDB: &lt;boolean&gt; #是否将不同的数据存储在不同的目录中，dbPath子目录 syncPeriodSecs: &lt;int&gt; #fsync操作将数据flush到磁盘的时间间隔，默认为60秒，不建议修改 engine: &lt;string&gt; #存储引擎 mmapv1: #mmapv1存储引擎，3.2前默认 preallocDataFiles: &lt;boolean&gt; nsSize: &lt;int&gt; quota: enforced: &lt;boolean&gt; maxFilesPerDB: &lt;int&gt; smallFiles: &lt;boolean&gt; journal: debugFlags: &lt;int&gt; commitIntervalMs: &lt;num&gt; wiredTiger: #WiredTiger存储引擎，3.2后默认 engineConfig: cacheSizeGB: &lt;number&gt; #最大缓存大小 journalCompressor: &lt;string&gt; #日志压缩算法，可选值有 none，snappy(默认)，zlib directoryForIndexes: &lt;boolean&gt; #是否将索引和collections数据分别存储在dbPath单独的目录中 collectionConfig: blockCompressor: &lt;string&gt; #collection数据压缩算法，可选none, snappy，zlib indexConfig: prefixCompression: &lt;boolean&gt; #是否对索引数据使用前缀压缩。对那些经过排序的值存储有很大帮助，可有效减少索引数据的内存使用量。 inMemory: #inMemory内存存储引擎，bate版 engineConfig: inMemorySizeGB: &lt;number&gt; processManagement进程相关参数: 123processManagement: fork: &lt;boolean&gt; #是否以fork模式运行mongod进程，默认情况下，mongod不作为守护进程运行 pidFilePath: &lt;string&gt; #将mongod进程ID写入指定文件，如未指定，将不会创建PID文件 net网络相关参数: 123456789101112131415161718192021222324252627282930net: prot: &lt;int&gt; #监听端口，默认27017 bindIp: &lt;string&gt; #绑定IP，如果此值是“0.0.0.0”则绑定所有接口 maxIncomingConnections: &lt;int&gt; #mongod进程允许的最大连接数，如果此值超过系统配置的连接数阈值，将不会生效(ulimit) wireObjectCheck: &lt;boolean&gt; #当客户端写入数据时，检查数据的有效性（BSON）。如果数据格式不良，update,insert等操作将会被拒绝 ipv6: &lt;boolean&gt; #是否支持多实例之间使用ipv6 unixDomainSocker: #适用于Unix系统 enabled: &lt;boolean&gt; pathPrefix: &lt;string&gt; filePermissions: &lt;int&gt; http: # enabled: &lt;boolean&gt; JSONEnabled: &lt;boolean&gt; RESTInterfaceEnabled: &lt;boolean&gt; ssl: sslOnNormalPorts: &lt;boolean&gt; mode: &lt;string&gt; PEMKeyFile: &lt;string&gt; PEMKeyPassword: &lt;string&gt; clusterFile: &lt;string&gt; clusterPassword: &lt;string&gt; CAFile: &lt;string&gt; CRLFile: &lt;string&gt; allowConnectionsWithoutCertificates: &lt;boolean&gt; allowInvalidCertificates: &lt;boolean&gt; allowInvalidHostnames: &lt;boolean&gt; disabledProtocols: &lt;string&gt; FIPSMode: &lt;boolean&gt; compression: compressors: &lt;string&gt; security安全相关参数: 1234567891011121314151617181920212223242526272829303132333435security: authorization: enabled #MondoDB认证功能 keyFile: /path/mongo.key #MongoDB副本集节点身份验证密钥文件 clusterAuthMode: &lt;string&gt; #集群members间的认证模式 transitionToAuth: &lt;boolean&gt; javascriptEnabled: &lt;boolean&gt; #是否允许执行JavaScript脚本 redactClientLogData: &lt;boolean&gt; sasl: hostName: &lt;string&gt; serviceName: &lt;string&gt; saslauthdSocketPath: &lt;string&gt; enableEncryption: &lt;boolean&gt; encryptionCipherMode: &lt;string&gt; encryptionKeyFile: &lt;string&gt; kmip: keyIdentifier: &lt;string&gt; rotateMasterKey: &lt;boolean&gt; serverName: &lt;string&gt; port: &lt;string&gt; clientCertificateFile: &lt;string&gt; clientCertificatePassword: &lt;string&gt; serverCAFile: &lt;string&gt; ldap: servers: &lt;string&gt; bind: method: &lt;string&gt; saslMechanism: &lt;string&gt; queryUser: &lt;string&gt; queryPassword: &lt;string&gt; useOSDefaults: &lt;boolean&gt; transportSecurity: &lt;string&gt; timeoutMS: &lt;int&gt; userToDNMapping: &lt;string&gt; authz: queryTemplate: &lt;string&gt; operationProfiling慢查询相关参数： 1234operationProfiling: slowOpThresholdMs: &lt;int&gt; #数据库profiler判定一个操作是“慢查询”的时间阈值，单位毫秒。mongod会把慢查询记录到日志中，默认100ms mode: &lt;string&gt; #数据库profiler级别，操作的性能信息将会被写入日志文件中，可选值“off”--关闭profiling，“slowOp”--只包包含慢操作，“all”--记录所有操作 #数据库profiling会影响性能，建议只在性能调试阶段开启 replication副本集： 12345replication: oplogSizeMB: &lt;int&gt; #replication操作日志的最大尺寸，如果太小，secondary将不能通过oplog来同步数据，只能全量同步 replSetName: &lt;string&gt; #副本集名称，副本集中所有的mongod实例都必须有相同的名字，Sharding分布式下，不同的sharding应该使用不同的repSetName secondaryIndexPrefetch: &lt;string&gt; #副本集中的secondary，从oplog中应用变更操作之前，将会先把索引加载到内存 enalbeMajorityReadConcern: &lt;boolean&gt; #允许readConcern的级别为“majority” sharding分片相关参数： 123sharding: clusterRole: &lt;string&gt; #在sharding集群中，此mongod实例可选的角色。configsvr,默认监听27019端口 和 shardsvr,默认监听27018端口 archiveMovedChunks: &lt;boolean&gt; #当chunks因为“负载均衡”而迁移到其他节点时，mongod是否将这些chunks归档，并保存在dbPath/movechunk目录下，mongod不会删除moveChunk下的文件 setParameter自定义变量： 1234setParameter: &lt;parameter1&gt;: &lt;value1&gt; &lt;parameter2&gt;: &lt;value2&gt; enableLocalhostAuthBypass: false #栗子 auditLog审计相关参数： 12345auditLog: destination: &lt;string&gt; #指定审计记录的输出方式，有syslog, console, file format: &lt;string&gt; #输出格式，有JSON 和 BSON path: &lt;string&gt; #如果审计时间输入为文件，那么就需要指定文件完整路径及文件名 filter: &lt;string&gt; #过滤器，可限制审计系统记录的操作类型，该选项需要一个表单的查询文档的字符串表示形式 Mongo Shellmongo shell是一个交互式的JavaScript结构的MongoDB。使用mongo shell来查询和更新数据以及执行管理操作。 mongo shell基础知识启动monso shell启动mongo shell前确保MongoDB实例正在运行。 1234567891011121314mongo [option] [db address] [.js]#以默认配置启动mongo#以特定配置启动mongo --port 27018#连接远程mongo shellmongo --host $host --port $port -u $user -p $passwdmongo &lt;db&gt;mongo &lt;host&gt;/&lt;db&gt;mongo &lt;hsot:port&gt;/&lt;db&gt; .mongorc.js文件mongo shell开始运行时，mongo将在用户主目录下检查.mongorc.js的js文件。如果找到，mongo将在首次命令行之前解释执行.mongorc.js的内容。如果你使用mongo shell执行一个js或表达式，无论是通过mongo --eval，或指定一个.js文件，mongo都将在js处理完成之后读取.mongorc.js文件。可使用 --norc选项禁止加载.mongorc.js。 12ll /root/.mongorc.js# -rw------- 1 root root 0 Dec 27 2016 /root/.mongorc.js 使用mongo shell可能在启动mongo shell的时候会警告: WARNING: /sys/kernel/mm/transparent_hugepage/defrag is ‘always’. We suggest setting it to ‘never’. WARNING: /sys/kernel/mm/transparent_hugepage/defrag is ‘always’. We suggest setting it to ‘never’ WARNING: Access control is not enabled for the database. hugepage(大内存页面)，是Linux操作系统一种管理内存的方式。和通常方式相比，hugepage模式下内存分配管理会有所差异。MongoDB显然不希望这个特定被启用。新版MongoDB增加了安全性设计，推荐用户创建使用数据库时进行验证。所以我们需要创建用户认证。 关闭hugepage: 123456vim /etc/rc.d/rc.localecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragchmox a+x /etc/rc.d/rc.local 创建用户认证: 1234567891011121314&gt;use admin&gt;db.createUser(&#123; user: "zhang", pwd: "zhang", roles: [&#123; role: "root", db: "admin"&#125;]&#125;)mongo -u zhang -p zhang --authenticationDatabase admin#或mongouse admindb.auth("zhang", "1314520") 12345678910mongo#显示当前使用数据库&gt;db#切换数据库&gt;use &lt;database&gt;#查看所有数据库&gt;show dbs 你可以切换到一个并不存在的数据库。当你第一次向数据库存储数据，如创建一个集合，MongoDB将自动创建数据库。 123use nodbdb.nocollestion.insert(&#123;x:1&#125;); 格式化打印结果db.collection.find()方法返回一个cursor(游标)。如果返回的游标未使用var关键字指定变量，则游标将自动迭代最多20次，以打印出与查询匹配的前20个documents 1234567#在操作中添加`.pretty()`，以格式化打印结果#使用.pretty显示结果很舒服db.collection.find().pretty()print() #无格式打印printjson() #用JSON打印 mongo shell中的多行操作mongo shell中如果你以( , { , [开始，那么知道你输入了对应的) , } , ]才算结束命令。 Tab命令补全和键盘快捷键mongo shell支持键盘快捷键，例如： 使用 上/下箭头 进行历史命令切换； 使用 Tab键 自动补全命令。 mongo shell批量操作123456mongo -u xxx -p xxx --authenticationDatabase=xxx &lt;&lt; EOFshow dbsuse zhangdb.coll01.drop()db.coll02.update( &#123; _id: "xxx" &#125;, &#123; name: "zhang" &#125;)EOF 退出mongo shell12345quit()exitCtrl+c 配置mongo shell可在mongo shell中设置变量prompt的值来修改提示符内容。prompt变量可以存储字符串以及JavaScript代码。 也可以在.mongorc.js文件中增加提示符的逻辑操作来设置每次启动mongo shell的提示符。 自定义提示符自定义提示符展示操作符： 在mongo shell中定义一下变量。 12345678910cmdCount = 1;prompt = function() &#123; return (cmdCount++) + '&gt; ';&#125;#效果1&gt;2&gt;... 自定义提示符显示数据库和主机名： 形式为：@$ 12345678host = db.serverStatus().host;prompt = function() &#123; return db+'@'+host+'$'&#125;#效果test@localhost$ 自定义提示符展示服务器启动时间和文档数： 1234567prompt = function() &#123; return 'Uptime:' + db.serverStatus().uptime + 'Documents:' + db.stats().objects + '&gt; ';&#125;#效果Uptime:1234 Documents:5 &gt; 注意：在mongo shell里面定义的prompt变量知识临时生效的，退出shell后便没有。如果想要当前用户永久生效，可写入~/.mongorc.js文件。则此用户每次启动mongo shell前都会执行这个文件。 123456vim ~/.mongorc.jshost = db.serverStatus().host;prompt = function() &#123; return db+"@"+host+"&gt; "; &#125; 在mongo shell中使用外部编辑器可在启动mongo shell之前设置EDITOR环境变量来在mongo shell中使用自己的编辑器。 12345678910111213export EDITOR=vimmongo#edit &lt;variable&gt;|&lt;function&gt;function myfunc()&#123;&#125;edit myfunc#此时是edit使用vim编辑myfuncfunction myfunc()&#123; print("It was edited by vim!")&#125;myfunc() 修改mongo shell批处理大小db.collection.find()是一种JavaScript方法，返回一个cursor(游标)。如果返回的游标未使用var关键字指定变量，则游标将自动迭代最多20次，以打印出与查询匹配的前20个documents。可以设置DBQuery.shellBatchSize属性来修改默认20篇文档。 1DBQuery.shellBatchSize = 10; 获取mongo shell帮助合理运用Tab键补全命令！ 1234567891011121314151617181920212223242526###命令行帮助mongo --help###mongo shell里查看帮助列表help###数据库帮助#db.&lt;method&gt;show dbsdb.help()###集合帮助#db.&lt;collection&gt;.&lt;method&gt;show collectionsdb.collections.help()###游标帮助db.collection.find().help()###封装对象帮助help misc 给mongo shell写脚本可使用JavaScript为mongo shell编写脚本，用于处理MongoDB中的数据或执行管理操作。 打开新连接在mongo shell或JavaScript文件中，可使用Mongo()构造函数来实例化数据库连接： 12345678910111213141516171819new Mongo()new Mongo(&lt;host&gt;)new Mongo(&lt;host:port&gt;)#栗子conn = new Mongo();db = conn.getDB('mydb'); #将全局db变量设置为mydb#连接db = connect('localhost:27017/mydb');#认证db.auth(&lt;user&gt;, &lt;passwd&gt;)db.auth(&#123; user: &lt;user&gt;, pwd: &lt;passed&gt;&#125;) 交互式和脚本化mongo的区别mongo shell中的帮助与JavaScript中帮助不一样！ mongo shell帮助 JavaScript等量 show dbs db.adminCommand(‘listDatabases’) use db = db.getSiblingDB(‘‘) show collections db.getCollectionNames() show users db.getUsers() show log db.adminCommand({‘getLog’ : ‘‘}) 脚本使用mongo shell来计算JavaScript的值。 –eval mongo执行 --eval后的js命令 1mongo test --eval &quot;printjson(db.getCollectionNames())&quot; 执行JavaScript文件 12345mongo localhost:27017/test myjs.js#在shell中执行.js&gt;load("myjs.js")&gt;loca("/root/mongo/myjs.js") mongo shell中的数据类型MongoDB BSON提供了除JSON之外的其它数据类型的支持。Driver提供了对这些数据类型在主机语言的本地化支持，mongo shell也提供了一些帮助类来支持这些数据类型在mongo javascript shell中的使用。 日期mongo shell提供了多种方法返回日期: Date() 方法返回当前日期为一个字符串； new Date() 构造函数返回一个使用ISODate()包装返回的Date对象； ISODate() 构造函数返回一个使用ISODate()包装返回的Date对象。 返回一个日期为字符串： 123var myDateString = Date();#查看变量值myDateString 验证类型： 12typeof myDateString()#string 返回Date： 123456var myDate = new Date();myDate#ISODate(&quot;2017-12-12T08:43:31.405Z&quot;)#验证myDate instanceof Date ObjectIdmongo shell对objectid数据类型提供objectId()包装类。 new ObjectId NumberLongmongo shell默认将所有数字处理为浮点值。 用numberlong()包装来处理64位整数。 1NumberLong("2090845886852") NumberInt用NumberInt()构造函数来显式指定32位整数。 NumberDecimalmongo shell默认将所有的数字处理为64位浮点的double值。mongo shell提供了NumberDecimal()构造函数限制指定128位基于十进制的浮点值，能够以精确的精度仿效十进制近似值。这个功能在金融、税务以及科学计算等方面应用。 12&gt;NumberDecimal('1000.55')#强烈建议加上引号，没加引号可能会存在精度丢失的情况 ### 在mongo shell中检查类型 instanceof返回一个bool值来验证一个值是否为某些类型的实例。 12mydoc._id instanceof ObjectId#true typeof返回一个字段的类型。 12typeof mydoc._id#object mongo shell快速参考mongo shell 历史命令mongo shell历史命令保存在~/.dbshell文件中，cat ~/.dbshell。也可以使用上/下键切换历史命令。 命令行选项 option description --help 显示命令行选项 --nodb 启动mongo shell而不连接到数据库 --shell 执行文件后运行mongo shell mongo shell命令助手 help methods and commands description help 显示帮助 db.help 显示数据库方法的帮助 db.collection.help() 显示集合方法的帮助 show dbs 打印服务器上的所有数据库列表 show databases 打印所有可获取的数据库列表 use &lt;db&gt; 切换数据库 show collections 打印当前数据库上的所有集合列表 show users 打印当前数据库的用户列表 show roles 打印当前数据库的所有角色(user-define and built-in)列表 show profile 打印花费1ms或更多时间的五个最近的操作 load() 在shell中执行一个JavaScript文件，建议使用绝对路径 mongo shell的基本JavaScript操作mongo shell为数据库操作提供了一个JavaScript API。db引用当的是前数据库的变量。 JavaScript db-operation description db.auth() 在安全模式下认证用户 coll = db.&lt;collection&gt; 将当前db中的特定collection设置为coll，可在此变量上执行操作，如coll.find(); db.collection.find() 查找集合中的所有文档，并返回一个游标 db.collection.insert() 插入一个新文档到集合中 db.collection.update() 更新集合中一个存在的文档 db.collection.save() 插入或更新 集合中的文档 db.collection.remove() 从集合中删除文档 db.collection.drop() 删除整个集合 db.collection.createIndex() 在集合中创建索引 db.getSiblingDB() 跨数据库查询 键盘快捷键 keysrtoke function Up/Down arrow 前/后 历史命令 Left/Right arrow 左右移动 Home/End 行首/行尾 Tab 自动补全 ctrl+c 退出 ctrl+L 清屏 mongo shell查询方法在mongo shell中，使用find()和findOne()方法执行读操作。 read-operations description db.collection.find(&lt;query&gt;) 查找集合中与匹配的文档，如果未指定或为空，则读取操作会选择集合中的所有文档 db.collection.find(&lt;query&gt;, &lt;projection&gt;) 查找与匹配的文档，返回特定字段 db.collection.find().sort(&lt;sort order&gt;) 返回排序结果 db.collection.find(&lt;query&gt;).sort(&lt;sort order&gt;) 返回匹配和排序结果 db.collection.find(...).limit(&lt;n&gt;) 限制输出结果为行 db.collection.find().pretty().limit() 匹配，格式化，限制输出 db.collection.find().limit().pretty() 同上 db.collection.find(...).skip(&lt;n&gt;) 跳过前行 db.collection.count() 返回集合中文档总数 db.collection.find().count() 返回匹配文档总数 db.collection.findOne(&lt;query&gt;) 查找并返回单一的文档，null表示未找到 管理命令助手 js db-administrative-methods description db.cloneDatabase(&lt;host&gt;) 从指定主机克隆当前数据库，noauth mode db.copyDatabase(&lt;from&gt;, &lt;to&gt;, &lt;host&gt;) copy db to db db.fromColl.renameCollection(&lt;toColl&gt;) rename collection db.repairDatabase() 修复当前db db.dropDatabases() 删除当前数据库 打开附加连接可以在mongo shell中创建一个新连接。 12345&gt;db = connect("&lt;host&gt;:&lt;port&gt;/&lt;db&gt;")#db = connect("192.168.1.11/admin")&gt;conn = new Mongo()&gt;db = conn.getDB("dbname") MongoDB CRUD操作CRUD操作就是创建(create)，读取(read)，更新(update)，删除(delete)文档(document)! 创建(create)操作创建或插入， 即是向 collection 添加新的 document。如果插入时集合不存在，插入操作会创建该集合。 123db.collection.insert()db.collection.insertOne()db.collection.insertMany() 读取(read)操作读操作，获取 collection 中的 document。 1db.collection.find() 更新(update)操作更新操作，修改 collection 中已经存在的 document。 1234db.collection.update()db.collection.updateOne()db.collection.updateMany()db.collection.replaceOne() 删除(delete)操作删除操作，是从一个 collection 中删除 document 的操作。 123db.collection.remove()db.collection.deleteOne()db.collection.deleteMany() 插入文档 插入方法MongoDB提供了如下插入方法向collection中插入document： db.collection.insert() db.collection.insertOne() db.collection.insertMany() db.collection.insert()db.collection.insert(),向collection中插入一个或多个document。要想插入一个document，传递一个文档给该方法；要想插入多个documents，传递文档数组给该方法。 12345678910111213141516171819#插入一个文档db.user.insert( &#123; _id: "ZhangTest", name: "zhang", age: 2017, sex: "man" &#125;)#插入多个文档db.user.insert( [ &#123; name: "AAA", age: 20, status: "A" &#125;, &#123; name: "BBB", age: 21, status: "B" &#125;, &#123; name: "CCC", age: 22, status: "C" &#125; ]) db.collection.insertOne()db.collection.insertOne(),向collection中插入单个document。 12345678910db.user.insertOne( &#123; name: "zhang", age: "2017", sex: "man", education: "bachelor" &#125;)#此处并未自定义_id字段，因此它会自动添加_id字段 db.collection.insertMany()db.collection.insertMany(),向collection插入多个documents。 123456789db.user.insertMany( [ &#123; name: "AAA", age: "20", status: "A" &#125;, &#123; name: "BBB", age: "21", status: "B" &#125;, &#123; name: "CCC", age: "22", status: "C" &#125; ])#自动生成3个document的_id字段 插入操作的行为表现创建集合插入的时候如果collection不存在，那么插入操作会创建collection。 _id字段在MongoDB中，存储于collection中的每一个document都需要一个唯一的_id字段作为primary_key。如果一个插入的document操作遗漏了_id字段，则MongoDB driver会自动生成一个ObjectId。 原子性MongoDB中所有的写操作在单一文档层级上是原子的。 查询文档]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求方法和状态码]]></title>
    <url>%2F2017%2F12%2F01%2FHTTP-method-status%2F</url>
    <content type="text"><![CDATA[常见HTTP请求方法HTTP协议的请求方法有：GET, POST, HEAD PUT DELETE, OPTIONS, TRACE, CONNECT Method Description GET 向Server请求文件 POST 向Server发送数据并让Server进行处理 PUT 向Server发送数据并存储在Server端 HEAD 检查一个对象是否存在 DELETE 从Server上删除一个文件 CONNECT 对通道提供支持 TRACE 跟踪到Server的路径 OPTION 查询Server的性能 HTTP Status Code当我们从Client向Server发送请求时，Server会向我们返回StatusCode。StatusCode会告诉我们Server的响应的状态，通过它，我们就可以知道当前请求是成功还是出现了问题。 HTTP StatusCode放置在HTTP Response报文中。 StatusCode由三位数字组成，第一个数字定义了响应类型，有五种可能值： 状态码 响应类别 描述 1xx 指示信息 服务器正在处理请求 2xx 成功 请求以正常处理完毕 3xx 重定向 需要进行额外操作以完成请求 4xx 客户端错误 客户端原因导致服务器无法处理请求 5xx 服务器错误 服务器原因导致处理请求出错 常见HTTP状态码 状态码 描述 200-OK 服务器成功返回网页，这是成功的HTTP请求返回的标准状态码 301 - Moved Permanently 永久跳转，所有请求的网页将永久跳转到被设定的新位置 400 - Bad Request 客户端请求有语法错误，不能被服务器理解 403 - Forbidden 禁止访问，这个请求时合法的，但是服务器端因为匹配了预先设置的规则而拒绝响应客户端的请求，此类问题一般为服务器权限配置不当所致 404 - Not Found 服务器找不到客户端请求的指定页面，可能是客户端请求了服务器不存在的资源所导致 500 - Internal Server Error 内部服务器错误，服务器遇到了意料不到的情况，不能完成客户的请求。这是一个较为笼统的报错，一般为服务器的设置或内部程序问题所致 502 - Bad Gateway 坏的网关，一般是代理服务器请求后端服务器时，后端服务不可用或没有完成响应网关服务器。一般为代理服务器下面的节点出了问题 503 - Service Unavailable 服务当前不可用，可能为服务器超载或停机维护所致，或者是代理服务器后面没有可以提供服务的节点 504 - Gateway Timeout 网关超时，一般是网关代理服务器请求后端服务时，后端服务没有在特定的时间内完成处理请求，一般为服务器过载所致，没有在指定的时间内返回数据给代理服务器 1xx1xx（临时响应），表示临时响应并需要请求者继续执行操作。 状态码 描述 100 - Continue 请求者应当继续提出请求 101 - Switching Protocols 请求者要求服务器更换协议，服务器已确认并准备更换 2xx2xx（成功），表示成功处理了请求。 状态码 描述 200 - OK Server已成功处理了请求 201 - Created 请求成功并且Server创建了新的资源 202 - Accepted Server以接受请求，但尚未处理 203 - Non-Authoritative Information Server已成功处理了请求，但返回的信息可能来自另一个来源 204 - No Content Server成功处理了请求，但没有返回任何内容 205 - Reset Content 没有新的内容，但浏览器应该重置它所显示的内容 206 - Partial Content 服务器成功处理了部分GET请求 3xx3xx（重定向），表示要完成请求需要进一步操作。 状态码 描述 300 - Multiple Choices 针对请求，Server可执行多种操作 301 - Moved Permanently 请求的网页已移动到新位置 302 - Found Server目前从不同位置的网页响应请求 303 - See Other 请求者对不同位置使用单独的GET请求来检索时 304 - Not Modified 自从上次请求后，请求的网页内容未修改过 305 - Use Proxy 请求者只能使用代理访问请求的网页 307 - Temporary Redirect Server从不同位置的网页响应请求，但请求者继续使用原有位置进行请求 4xx4xx（请求错误），表示请求可能出错，妨碍了Server的处理。 状态码 描述 400 - Bad Request Server不理解请求的语法 401 - Unauthorized 请求要求身份认证 403 - Forbidden Server拒绝请求 404 - Not Found Server找不到请求的网页 405 - Method Not Allowed 请求方法不被允许 406 - Not Acceptable 无法使用请求的恩日工特性响应请求的网页 407 - Proxy Authentication Required 请求需要代理授权 408 - Request Timeout Server等候请求时超时 409 - Conflict Server在完成请求时发生冲突 410 - Gone 请求的资源以永久删除 411 - Length Required Server不接受不含有效内容长度Header的请求 412 - Precondition Failed Server为满足请求者在请求中设置的一个前提条件 413 – Request Entity Too Large 请求实体太大，Server无法处理 414 - Request URI Too Long 请求的URI过长，Server无法处理 415 – 不支持的媒体类型 请求的格式不受支持 416 – Requested Range Not Satisfiable 页面无法提供请求的范围 417 – 执行失败 Server未满足期望请求Header的要求 451 基于法律上的的原因，不能像请求者展示网页内容 5xx5xx（服务器错误），表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求出错。 状态码 描述 500 - Internal Server Error Server遇到错误，无法完成请求 501 - Not Implemented Server不具备完成请求的功能 502 - Bad Gateway Server作为网关或代理时，从upstream收到无效响应 503 - Service Unavailable Server暂时无法使用 504 - Gateway Timeout Server作为网关或代理时，没有及时从upstream收到请求 505 - HTTP Version Not Supported Server不支持请求中所用的HTTP版本]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filesystem Hierarchy Standard]]></title>
    <url>%2F2017%2F11%2F27%2FFHS%2F</url>
    <content type="text"><![CDATA[FHS介绍FHS(Filesystem Hierarchy Standard)，文件系统层次化标准：http://www.pathname.com/fhs FHS主要目的是希望让用户了解安装文件通常放置的目录。所以希望软件开发商、系统制定者以及维护系统的用户，都能够遵循FHS的标准。 FHS-compliant system： - 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr /opt /etc /boot 可变的(variable) /var/mail /var/spool/news /var/run /var/lock shareable： 可分享给其他系统(主机)挂载使用； unshareable： 不适合分享给其他主机； static： 有些数据基本是不会变化的； variable： 进程变更的数据。 FHS针对目录树架构仅定义出三层目录下应该放置什么数据，这三个目录下所应该放置的目录也都有特定规定。 /： The root filesystem, 与开机系统有关； /usr: The /usr hierarchy, Unix software resource； /var: The /var hierarchy, 与系统运行过程有关。 The Root Filesystem根目录(/)是系统最重要的一个目录。不但所有目录都是由根目录衍生出来，同时根目录还与系统的启动、还原、修复等操作相关。若系统出现问题，根目录必须要包含能够修复文件系统的程序才行。破坏根文件系统上的数据的错误比破坏其他任何分区都要严重！ 为了平衡这些考虑，建议尽可能保持根分区小。应用程序不应在根目录中创建特殊文件或子目录！ The following dirs or symbolic-links, are required in / 目录 描述 /bin 必要的二进制命令 /boot boot-loader的静态文件 /dev 设备文件 /etc 主机特定的系统配置文件 /lib 基本的共享库(shared libraries)和内核模块(kernel modules) /media 可移除媒体的挂载点 /mnt 临时挂载文件系统的挂载点 /opt 第三方软件包放置目录 /sbin 必要的系统二进制命令 /srv 系统提供的服务数据 /tmp 临时文件 /usr /usr层次结构 /var /var层次结构 除了上面列出必须存在的目录，下面这些目录很也很重要。 目录 描述 /lost+found 在ext文件系统里，当文件系统发生错误时，将一些遗失的片段放置到此目录下 /home 用户家目录 /root root用户家目录 /proc 虚拟文件系统，放置的数据都在内存当中，不占磁盘空间 /sys 虚拟文件系统，记录内核相关信息，不占磁盘空间 另外需要注意的是，因为根目录与开机有关，开机过程中仅有根目录被挂载。其他分区则是在开机完成后才会持续进行挂载。因此，根目录下与开机过程有关的目录就不能放到不同的分区中去。 如： /etc /bin /sbin /dev /lib /bin/bin, 基本用户二进制命令文件，供所有用户（系统管理员和用户）使用。 /bin下不能有子目录(subdirectory)。 The following commands or symbolic-links to commands, are required in /bin 命令 描述 cat 将文件连接到stdout的实用程序(Utility) chgrp 更改文件所有权 chmod 更改文件访问权限 chown 更改文件所有者和和组 cp 复制文件和目录 date 打印或设置系统数据和时间 dd 转换和复制文件 df 磁盘使用情况 dmesg 打印或控制kernel消息缓冲区 echo 显示一行文本 false do nothing, 不成功 true do nothing, 成功 hostname 系统主机名 kill 发送信号到进程 ln 在文件之间创建链接 login 在系统上开始会话 ls 列出目录内容 mkdir 创建目录 mknod 创建block或character特殊文件 more 文本翻页 mount 挂载文件系统 umount 解挂文件系统 mv move/rename文件 ps 报告进程状态 pwd 打印当前工作目录 rm remove文件或目录 sed sed流编辑器 sh Bourne command shell stty 更改或打印终端设置 su change uid sync 刷新文件系统缓冲区 uname 打印系统信息 The following programs or symbolic-links to programs, must be in /bin if the corresponding-system is installed: 命令 描述 csh The C shell(可选) ed 编辑器(可选) tar tar归档(可选) cpio cpio归档(可选) gzip GNU压缩工具(可选) gunzip GNU解压缩工具(可选) netstat 网络统计(可选) ping ICMP网络测试(可选) /boot/boot :static file of the boot-loader 该目录包含引导过程所需所有内容，处理引导是不需要的配置文件和映射安装文件外。因此，/boot储存kernel开始执行用户模式之前使用的数据。 操作系统kernel必须位于 / or /boot /dev/dev :device files /dev 目录是特殊或设备文件的位置。 /etc/etc :host-specific system configuration 配置文件是用来控制程序操作的本地静态文件，不能是可执行的二进制文件。 The following files or symbolic-links to files, must be in /etc if the corresponding-subsystem is installed. 文件 描述 备注 csh.login C shell登录的系统范围初始化文件 Optional exports NFS文件系统访问控制列表 Optional fstab 文件系统静态信息 Optional ftpusers FTP守护进程用户访问控制列表 Optional gateways 路由网关文件 Optional gettydefs getty终端设置 Optional group 用户组文件 Optional passwd 密码文件 Optional host.conf 解析器配置文件 Optional hosts 主机域名的静态信息 Optional hosts.allow Tcp-wrapper的主机访问文件 Optional hosts.deny Tcp-wrapper的主机禁止文件 Optional hosts.equiv rlogin, rsh, rcp的可信主机列表 Optional hosts.lpd lpd的可信主机列表 Optional inetd.conf inetd配置文件 Optional inittab init配置文件 inittab is no longer used when using systemd id.so.conf 搜索共享库的额外目录 Optional issue 预登录消息和 CentOS Linux 7(core) kernel \r on an \m motd 登录后信息 Welcome to $host mtab 文件系统动态信息 Optional mtools.conf mtools配置文件 Optional networks 网络名称的静态信息 Optional printcap lpd打印机功能数据库 Optional profile sh shell login的系统范围初始化文件 Optional protocols IP协议列表 Optional resolv.conf 域名服务器解析文件 Optional rpc RPC协议列表 Optional securetty root登录的TTY访问控制 Optional shells 有效登录shell的路径名 Optional syslog.conf syslogd配置文件 Optional /etc/opt/etc/opt :/opt的配置文件 第三方应用程序软件的特定主机配置文件，必须安装在/etc/opt/ 中。 /etc/xml/etc/xml :XML的配置文件 这里安装和定义XML系统的高级参数同通用配置文件。 /home (Optional)/home :用户主目录 /home是一个相当标准的概念，但它显然是一个特定于站点的文件系统。设置会因主机而异。因此，任何程序都不应该依赖这个目录。 /lib/lib :基本的共享库和内核模块 /lib目录中包含引导系统和运行在根文件系统的命令，即/bin和/sbin中的命令。 至少需要包含以下文件(链接)： 文件 描述 libc.so.* 动态链接C库 ld* 执行时间 链接器/加载器 /lib (Optional)/lib&lt;qual&gt; : 不同格式的基本共享函数库如：64位的/lib64; 32位的/lib32。 用来存放与/lib不同格式的二进制函数库，如支持64位的/lib64函数库等。 /media/media :可移除媒体的挂载点 此目录包含的子目录，可作为各移动介质(USB,cdrom,floppy…)的挂载点。 尽管在 /mnt 中使用子目录作为挂载点已经很常见了，但与直接使用/mnt作为临时挂载点的传统相去甚远。 /mnt/mnt :临时挂载文件系统的挂载点 /opt/opt :为第三方软件包保留的目录 要安装在/opt中的软件包必须将其静态文件放置在单独的/opt/&lt;packge&gt;目录树中。 目录/opt/bin, /opt/doc, /opt/include, /opt/info, /opt/lib, /opt/man 是保留给本地系统管理员使用。如果第三方软件包含Unix手册，而手册必须放置于/opt//share/man/，必须使用与/usr/share/man相同的子结构。 /root (Optional)/root :root用户的主目录 /sbin/sbin :系统二进制文件 系统管理的实用程序(命令)，存储在/sbin, /usr/sbin, /usr/local/sbin中。/sbin包含启动，恢复，修复系统，以及/bin中二进制文件所必须的二进制文件。本地安装的系统管理程序应放置在/usr/local/sbin中。 The following commands or symbolic-links to commands are required in /sbin1shutdown #关闭系统 The following files or symbolic-links to files，must be in /sbin if the corresponding subsystem is installed 命令 描述 备注 fastboot 重启系统而不检查磁盘 Optional fasthalt 停止系统而不检查磁盘 Optional fdisk 分区表操作器 Optional fsck 文件系统检查和修理工具 Optional fsck.* 针对特定文件系统检查和修复 Optionaleg：fsck.ext3 getty getty程序 Optional half 停止系统 Optional ifconfig 配置网络接口 Optional init 初始化进程 Optional mkfs 创建文件系统 Optional mkfs.* 创建特定文件系统 OPtionaleg: mkfs.ext4 mkswap 设置swap分区 OPtional reboot 重启系统 OPtional route IP路由表实用程序 OPtional swapon 启用分页和交换 OPtional swapoff Disable paging and swapping Optional update 守护进程定期刷新文件系统缓冲区 Optional /srv/srv :系统提供的服务(service)的数据 /tmp/tmp :临时文件 /tmp目录为临时需要文件的程序提供。程序不能在程序的调用之间保留/tmp中的任何文件或目录。尽管/tmp中数据可能会以某种特定方式删除，但建议在系统启动时删除/tmp中所有文件。 The /usr Hierarchy/usr 里面放置的数据是可分享与不可变动的。这就意味着可在各种符合FHS的主机之间共享，但不能写入。大型软件包不应在/usr层次结构下使用直接子目录。 The following dirs of symbolic-links to dirs are required in /usr 目录 描述 /usr/bin 大多数用户命令 /usr/include C程序包含的头文件 /usr/lib 库文件 /usr/local 本地层次结构 /usr/sbin 非重要的系统二进制文件 /usr/share 独立于架构的数据 其他选项： 目录 描述 备注 /usr/lib&lt;qual&gt; 可选格式库 Optional /usr/src 源代码 OPtional /usr/games 游戏和教育二进制文件 OPtional /usr/bin/usr/bin :大多数用户命令这是系统上可执行命令的主要目录。 The following files or symbolic-links to files must be in /usr/bin, if the corresponding subsystem is installed 命令 描述 备注 perl 实用提取和报告语言 OPtional python python解释语言 Optional tclsh tcl解释器的简单shell OPtional wish 简单 tcl/tk windowing shell Optional expect 程序交互式对话 Optional 因为shell script解释器(在shell script脚本的第一行 #!)不能依赖路径，所以标准化它们的位置是有利的。Bourne shell 和 C-shell解释器已经被固定在/bin中，但 perl,python,tcl经常在许多不同的地方。 /usr/include/usr/include :标准C包含文件的目录 这是C语言所有系统的通用包含文件应该被放置的地方。 /usr/lib/usr/lib :编程和包的所需要的库 /usr/lib包括 不打算由用户或shell script直接执行的目标文件、库和内部二进制文件。 /usr/lib (Optional)/usr/lib&lt;qual&gt; :可选格式库 /usr/local/usr/local :本地层次结构 /usr/local是给系统管理员安装本地软件使用。当系统软件更新时，需保证安全。它可以用于在一组主机之间共享，但在 usr中找不到的程序和数据。 本地安装软件必须放在 /usr/local 而不是 /usr，除非安装它来升级或替换usr的软件 The following dirs or symbolic-links to dis must be in /usr/local 目录 描述 /usr/local/bin 本地二进制文件 /usr/local/etc 本地二进制文件的特定配置文件 /usr/local/games 本地游戏二进制文件 /usr/local/include 本地C头文件 /usr/local/lib 本地库 /usr/local/man 本地在线手册 /usr/local/sbin 本地系统二进制文件 /usr/local/share 本地独立架构层次结构 /usr/local/src 本地源码 /usr/local/share目录内容的要求应与/usr/share相同，唯一附加约束是/usr/local/share/man和/usr/local/man目录必须是同步的。（基本上就是符号链接了！） /usr/sbin/usr/sbin :非必要的标准系统二进制文件 该目录包含系统管理员专门使用的任何非必要的二进制文件。系统修复、恢复、挂载/usr等其他重要必要功能必须放在/sbin中。 /usr/share/usr/share :独立于架构的数据 /usr/share层次 是为了所有只读架构独立数据。该层次可以在给定OS的所有体系架构平台之间共享。如具有i386和PPC平台站点可能会维护一个集中安装的/usr/share目录。但/usr/share一般不打算由不同的操作系统共享，或由同一操作系统的不同版本共享。 The following dis or symbolic-links to dirs must be in /usr/share 目录 描述 man 在线手册 misc 其他独立于架构的数据 The following dis or symbolic-links to dirs must be in /usr/share, if the corresponding subsystem is installed 目录 描述 备注 dict 单词列表 Optional doc 各种文档 Optional games /usr/games的静态文件 Optional info GNU Info system’s primary dir Optional locale 支持的区域信息 Optional zoneinfo Timezone info and conf Optional NLS Native language support Optional sgml SGML数据 Optional terminfo terminfo数据库目录 Optional xml xml数据 Optional /usr/share/dict/usr/share/dict :单词列表这个目录是系统上单词列表的家目录，只包含英文单词，它们由look和各种拼写程序使用。它们是所有拼写检查器唯一通用的文件。 文件 描述 备注 words 单词列表 Optional linu.words linux可用单词列表 Optional /usr/share/man/usr/share/man :手册页它包含了/, /usr文件系统下的命令和数据的手册信息 手册页存储在 /usr/share/man/&lt;locale&gt;/man&lt;section&gt;/&lt;arch&gt;中。 每个部分的描述： man1: 可公开访问的命令的手册页，用户需要使用的大多数程序文档放置于此； man2: 系统调用部分，描述所有的系统调用(请求内核执行操作)； man3: 函数库和子例程部分，描述不直接调用内核服务的程序库例程； man4: 特定文件部分，描述系统中特定文件，相关驱动程序和网络支持。通常，这包含/dev中找到的设备以及网络协议支持的内核接口； man5: 文件格式部分，许多数据文件的格式记录在此； man6: 游戏，演示和一般小程序； man7: 各种难以分类的手册页； man8: 系统管理员用于操作和维护系统的程序记录在这。 The following dirs or symboli-link to dirs must be in /usr/share/man/&lt;locale&gt;, unless they are empty 目录 描述 备注 man1 用户程序 Optional man2 系统调用 Optional man3 函数库调用 Optional man4 特定文件 Optional man5 文件格式 Optional man6 游戏 Optional man7 混杂的手册页 Optional man8 系统管理 Optional 必须在/usr/share/man结构中作出规定，以支持用不同语言编写的手册页。这些规定必须考虑到手册页的存储和参考，相关因素包括语言和字符编码集。 栗子： Language Country CharacterSet Dir English - ASCII /usr/share/man/en English United Kingdom ISO 8859-15 /usr/share/man/en_GB English United States ASCII /usr/share/man/en_US /usr/share/misc/usr/share/misc :与架构无关的数据 /usr/share/sgml/usr/share/sgml :SGML数据 /usr/share/xml/usr/share/xml :XML数据 /usr/src/usr/src :源代码Source Code可能放置在此目录的子目录中，仅供参考。 /var Hierarchy/var 包含可变数据文件，包括假脱机目录和文件，系统管理和登录数据，以及临时文件。 如果/var不能成为一个单独的分区，最好将/var移出/分区并移入/usr分区。（为了减小根分区大小或当根分区空间不足时）也可将/var链接到/usr/var。 The following dirs or symbolic-link to dirs are required in /var. 目录 描述 /var/cache 应用程序缓存数据 /var/lib 可变状态信息 /var/local /usr/local的可变数据 /var/lock 锁文件 /var/log 日志文件 /var/opt /opt的可变数据 /var/run 与运行进程相关的数据 /var/spool 应用程序队列数据 /var/tmp 为系统重启保留的临时文件 The following dirs or symbolic-link to dir must be in /var,if the corresponding subsystem is installed. 目录 描述 备注 /var/account 进程账户日志 可选 /var/crash 系统奔溃转储 可选 /var/games 可变游戏数据 可选 /var/mail 用户邮箱文件 可选 /var/yp 网络信息服务数据库文件 /var/account/var/account :该目录保存当前活动的进程记账日志和复合进程数据。 /var/cache/var/cache :保存应用程序缓存的数据。应用程序必须能够重新生成或回复数据。与/var/spool不同，删除了缓存文件不会丢失数据。数据必须在应用程序调用和系统重启间保持有效。缓存目录的数据格式没有其他要求。 对于缓存数据单独存在的目录，系统管理员可从/var下其他目录设备不同的磁盘和备份策略。 目录 描述 备注 /var/cache/fonts 本地生成的字体 可选 /var/cache/man 本地格式化的手册页 可选 /var/cache/www www代理或缓存数据 可选 /var/cache/&lt;package&gt; 特定包缓存数据 可选 /var/lib/var/lib :可变状态信息。目录保存于应用程序或系统有关的状态信息。状态信息(state infofmation)，是程序在运行时修改的数据，属于一个特定的主机。 应用程序必须为其数据使用/var/lib/&lt;subdir&gt;，有一个必须的子目录/var/lib/misc用于不需要子目录的状态文件。 /var/lock/var/lock :锁文件，锁文件应该存储在此目录中。锁文件锁定多个应用程序共享的设备和其他资源。 这种锁文件内容的格式必须是HDB UUCP锁文件格式。HDB格式是将进程标识符(PID)存储为ASCII十进制数，并带有换行符。 /var/log/var/log :日志文件和目录，大多数日志必须写入此目录或适当子目录。 The following file or symbolic-link to file must be in /var/log. 文件 描述 lastlog 每个用户上次登录信息的记录 message syslogd的系统信息 wtmp 所有登录和注销的记录 /var/mail邮件缓存区必须通过/var/mail访问，邮件缓冲区文件必须采用的形式。 /var/run/var/run :运行时变化数据，此目录包含系统信息数据，描述系统启动以来的情况。此目录下的文件必须在引导过程开始时被清除。进程标识符(PID)文件放置于此目录或下的子目录里面。 /var/spool/var/spool :应用程序队列数据。此目录包含正在等待某种稍后处理的数据，/var/spool中的数据表示工作将在将来执行(通过程序，用户或管理员)，数据通常会在工作处理后被删除。 The following dirs or symbolic-link to dirs must be in /var/spool,if the corresponding subsystem is installed. 目录 描述 备注 lpd 打印机队列目录 可选 mqueue 发送邮件队列 可选 news 新闻假脱机目录 可选 rwho rwhod文件 可选 uucp uucp的假脱机目录 可选 /var/tmp/var/tmp :在系统重启之间保存的临时文件。存储在/var/tmp的数据比/tmp中的数据更持久。 OS Specific Annex本节是针对仅适用于特定OS的其他建议和要求。 LinuxLinux操作系统的附件 / :根目录在Linux系统上，如果内核位于/，建议使用Linux内核源代码包中使用的名称vmlinux或vmlinuz。 我的CentOS7中，内核文件默认是/boot/vmlinuz-$kernel-version.$arch /bin :基本用户命令二进制文件(供多有用户使用) /dev :设备和特殊文件 /dev/null : 写入该设备的所有数据都被丢弃。从这个设备读取将返回一个EOF条件。 /dev/zero : 该设备是归零数据的来源，写入该设备的所有数据被丢弃。从这个设备读取将返回包含zero的请求的字节数。 /dev/tty : 该设备类似于进程控制终端。一旦这个设备被打开，所有读写操作就好像实际的控制终端以及被打开一样。 /etc :主机的特定系统配置Linux系统要将附件文件放置到/etc中。 /lib64 和 /lib32 :64/32位库(依赖于体系结构)64位体系结构PPC64,AMD64,x86_64必须将64位库放置于/lib64中，将32位库放置于/lib中；64位体系结构IA64必须将64位库放置于/lib中。 /proc :内核和进程信息虚拟文件系统PROC文件系统是用于处理进程和系统信息的标准Linux方法，而不是/dev/kmem和其它类似方法。强烈建议使用PROC文件系统获取 存储，进程，内存，内核等信息。 /sbin :基本系统二进制文件Linux系统将这些附加文件放置于/sbin中： 第二扩展文件系统命令（可选）： 123456badblocksdumpe2fse2fsckmke2fsmklost+foundtune2fs boot-loader 映射安装程序（可选）：lilo 静态二进制文件： 123ldconfigsln(static ln)ssync(static sync) 出现问题时，sln（静态ln）和ssync（静态同步）非常有用；idconfig程序可以作为升级知道的手段；sln的主要用途，修复不良协调升级后/lib中不正确的符号链接动态库。 对于/sbin, idconfig二进制文件是可选的。因为站点可能会在启动时选择运行idconfig而不是仅在升级共享库时。以下是一些常见问题： 我刚刚删除了/lib/； 我无法找到库的名称，因为ls是动态链接。我使用的shell没有内置ls，我也不知道使用echo *作为替换； 我有一个静态ln，但我不知道怎么称呼这个链接。 杂项： 12345#ctrl+alt+delctrlaltdel#keyboard ratekbdrate 为了应对某些键盘出现如此高的重复速率一致无法使用,kbdrate可以安装在某些系统上的/sbin中； 由于ctrl+alt+del组合键在内核中的默认操作是硬重启，因此通常建议在将根文件系统挂在到读写模式之前禁用该行为。这就可能需要ctrlaltdel程序，它可以安装在系统的/sbin中。 /usr/include :C程序包含的头文件如果安装了C或C++编译器，则只有非 基于glibc的系统才需要这些链接符号。 12/usr/include/asm -&gt; /usr/src/linux/include/asm-&lt;arch&gt;/usr/include/linux -&gt; /usr/src/linux/include/linux /usr/src :源代码对于基于glibc的系统，此目录没有具体指导。 对于glibc之前基于linux libc修订版的系统： /usr/src/linux是唯一放置Linux内核源代码的位置。 /usr/spool/cron :cron和jobs此目录包含了cron和程序的可变数据。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>FHS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix]]></title>
    <url>%2F2017%2F11%2F14%2FZabbix%2F</url>
    <content type="text"><![CDATA[Zabbix简介Zabbix官方网站Zabbix中文文档Zabbix-repo仓库: http://repo.zabbix.com , 阿里云镜像: https://mirrors.aliyun.com/zabbix/zabbix/ . 本文系统环境是CentOS7x86_64, Zabbix 3.4。 Zabbix （音同 zæbix），是由 Alexei Vladishev 开发的一种网络监视、管理系统，基于 Server-Client 架构。Zabbix 的授权是属于 GPLv2。Zabbix可用于监视各种网络服务、服务器和网络机器等状态。是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。Zabbix也可经由SNMP、TCP、ICMP、SSH等对目标进行监视。 Zabbix的系统构成Zabbix系统由以下各独立模块组成： Zabbix Server，服务端(以C开发)。Server端通过收集SNMP和Agent发送的数据，写入数据库，再通过PHP+Apache在Web端展示； Zabbix Agent，客户端(基本支持所有操作系统)，并将监控主机数据发送给Server； Zabbix Frontend，Web管理端(以PHP和JavaScript构成)； Zabbix Proxy(可选组件)。用于分布式监控。 Zabbix的特点Zabbix是一个高度集成的网络监控解决方案，一个简单的安装包中提供多样性功能。 数据收集； 灵活的阀值(触发器)定义； 高度可配置化的告警； 实现图表绘制； Web监控功能； 丰富的可视化选项； 历史数据存储； 配置简单； 使用模板； 网络发现； Zabbix API； 权限管理系统； 功能强大并易于扩展的监控代理。 定义Zabbix的常用术语含义。 主机(host)：一台你想监控的网络设备，用IP或域名表示。 主机组(host group):主机的逻辑组，它包含主机和模板。 监控项(item):你想要接收的主机的特定数据，一个度量数据。 触发器(trigger):一个被用于定义问题阀值和评估监控项接收到的数据的逻辑表达式。 事件(event):单次发生的需要注意的事情。 异常(problem):一个处在异常状态的触发器。 动作(action):一个对事件作出反应的预定义的操作。 升级(escalation):一个在动作内执行操作的自定义场景。 媒介(media):发送报警通知的手段。 通知(notification):利用已选择的媒体途径把事情相关信息发送给用户。 远程命令(remote command):预先定义好的，满足一定条件后，可在被监控主机上自动执行的命令。 模板(template):一组可以被应用到一个或多个主机上的实体的集合。 应用(application):一组监控项组成的逻辑分组。 Web场景(Web scenario):利用一个或多个HTTP请求来检查网站的可用性。 前端(frontend):Zabbix提供的Web界面。 Zabbix API:Zabbix API允许你使用JSON RPC协议来创建、更新和获取Zabbix对象信息或执行任何其他的自定义的任务。 Zabbix server:Zabbix软件监控的核心程序，主要功能是与Zabbix proxies和agent进行交互、触发器计算、发送告警通知，并将数据集中保存等。 Zabbix agent:部署在监控对象上，能够主动监控本地资源和应用。 Zabbix proxy:帮助Zabbix server收集数据，分担Zabbix server的负载。 Zabbix进程Agentzabbix agent部署在监控的目标上，主动监测本地的资源和应用（硬件驱动，内存，处理器统计等）。zabbix agent手机本地的操作信息并将数据报告给zabbix server用于进一步处理。 zabbix agent有被动(passive)和主动(active)两种检查方式。 Serverzabbix server是zabbix软件的核心程序。它通过轮询和捕获数据，计算是否满足触发器条件，向用户发送通知。它是zabbix监控代理和Proxy代理报告系统可用性和完整性数据的核心组件。zabbix server自身可以通过简单远程检查网络服务(如Web服务器和邮件服务器)。 server是一个包含了被存储了所有配置，统计方面的和可操作数据的中央仓库，它是监控系统问题升级以致于激活警告管理器的zabbix中的实体。 基本的zabbix server分三个不同的组件：zabbix server，web前端，数据库存储。zabbix的所有配置信息都存储在服务器和web前端进行交互的数据库中。 zabbix server进程是以守护进程（Daemon）运行的。 Proxyzabbix proxy是一个可以从一个或多个受监控的设备设备收集监控数据，并将信息发送到zabbix server的进程，基本上是代表server工作。所有收集的数据都在本地进行缓存，然后传送到proxy所属的zabbix server。 zabbix proxy是完成远程区域、分支机构、没有本地管理员的网络的集中监控的理想解决方案。 zabbix proxy需要使用独立的数据库，以守护进程的方式运行。 Java gatewayzabbix守护进程原生支持监控JMX程序，它被称为zabbix java gateway。zabbix gateway是用Java语言写成。 要查得一台主机特定的JMX计数器值，zabbix server向zabbix java gateway发送请求，后者使用JMX管理API去请求远程的有关应用。应用不许额外安装软件，只需要启动时在命令行指定 -Dcom.sun.management.jmxremote即可（是在java程序）。 每个zabbix server或zabbix agent只能配置一个java gateway。 Senderzabbix sender是一种命令行应用，它可以将性能数据发送到zabbix server进行处理。该应用通常用在长时间运行的用户脚本，用于定期发送可用性和性能数据。 123456zabbix_sender -z zabbix -s &quot;xxx&quot; -k db.connections -0 43-z :server主机-s :受监控主机的技术名称-k :监控项的键-o :要发送的值 Getzabbix get也是一种命令行应用，用于与zabbix agent进行通信，并从agent那里获取所需的信息。该应用通常被用于zabbix agent故障排除 12345678zabbix_get -s $host -p xxx -k system.cpu.load[all,avg15]-s --host-p --port-I --source-address-k --key-h --help-V --version 安装ZabbixZabbix安装要求硬件： 内存，最小128MB； 磁盘，最小256MB； CPU，可能需要大量CPU资源； SMS(短信)通知服务，串行通讯口(serial communication port)和串口GSM调制解调器(serial GSM modem)。可选项。 支持平台： Linux; IBM AIX; FreeBSD; NetBSD; OpenBSD; Mac OS X; Solaris; Windows(Only Agent). 软件：Zabbix基于Apache Web服务器、领先的数据库引擎和PHP脚本语言进行构建。 数据库管理系统： MySQL 5.0.3 及以上； Oracle 10g 及以上； PostgreSQL 8.1 及以上； SQLite 3.5及以上； IBM DB2 9.7 及以上。 前端： Apache 1.3.12 及以上； PHP 5.4.0及以上； PHP-Extension: 软件 版本 备注 gd 2.0及以上 PHP GD扩展包必须支持PNG图片 bcmatch php-bcmatch ctype php-ctype libXML 2.6.15及以上 php-xml xmlreader php-xmlreader xmlwrite php-xmlwriter session php-session sockets php-net-socket mbstring php-mbstring gettext php-gettext ldap php-ldap mysqli 使用MySQL作为Zabbix后端数据库所需的组件 pgsql 使用PostgreSQL作为Zabbix后端数据库所需的组件 sqlite3 使用SQLite作为Zabbix后端数据库所需的组件 客户端浏览器：必须启用Cookie和JavaScript功能。 服务器： 要求 描述 OpenlPMI 支持IPMI功能所需组件 libssh2 支持SSH功能 fping 支持ICMP ping功能 libcurl 支持Web监控，VMware监控及SMTP认证 libiksemel 支持Jabber功能 libxml2 支持VMware监控 net-snmp 支持SNMP监控 Java网关：Java gateway编译和运行在Java 1.6 及以上版本。 数据库容量：Zabbix配置数据需要使用固定的磁盘空间，而这个空间不会过多增长。 Zabbix数据库容量主要依赖于以下参数： 每秒处理值的数量(Number of processed values per second); 历史(History)数据的回收清理设置(Housekeeper); 趋势(Trends)数据的回收清理设置(Housekeeper); 事件(Events)数据的回收清理设置(Housekeeper)。 时钟同步：对于Zabbix稳定运行而言，服务获取精确的系统时间是非常重要的。对于所有运行Zabbix组件的系统，强烈建议这些系统的时间保持同步。ntpd是一个临幸的用于同步主机和其他服务器之间的时间的后台程序。 安装、启动、配置ZabbixZabbix-repo仓库：repo.zabbix.com该仓库服务器同时提供yum和apt源码库。 配置源码库1. 从官方下载源码库 1234567#rpm -ivh http://repo.zabbix.com/zabbix/$version/rhel/7/$arch/$zabbix-release.rpmrpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm#阿里云镜像#rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.noarch.rpm#镜像失效的话自己去官网找 2. 手动配置zabbix.repo 1234567vim /etc/yum.repos.d/zabbix.repo[zabbix]name=Zabbix-Repobaseurl=http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/gpgcheck=0enable=1 安装Zabbix部署包使用MySQL数据库安装Zabbix Server、Web前端：1yum install -y zabbix-server-mysql zabbix-get 注意：此处Zabbix数据库使用MySQL，请自行安装MySQL。 安装Zabbix Agent：1yum install -y zabbix-agent 安装初始化数据库查看刚刚安装的 zabbix-server-mysql：解压得到的sql脚本create.sql只会在对应的数据库中初始化zabbix所需要的数据库表，但是不会创建zabbix数据库。所以后面我们还需要手动创建zabbix数据库。1234567rpm -ql zabbix-server-mysqlcd /usr/share/doc/zabbix-server-mysql-3.x.xx/#有一个create.sql.gz的压缩文件gunzip create.sql.gz#得到create.sql 在MySQL中创建zabbix数据库：123456789101112msyql -uxxx -pmysql&gt;CREATE DATABASE 'zabbix' DEFAULT CHARACTER SET 'utf8';mysql&gt;SHOW DATABASES;mysql&gt;GRANT ALL ON zabbix.* TO 'zabbix'@'localhost' identified by 'zabbix';mysql&gt;FLUSH PRIVILEGES;#导入sql脚本mysql -uroot -p -Dzabbix &lt; ./create.sqlUSE zabbix;SHOW TABLES; 配置zabbix server并启动编辑zabbix server配置文件：123456789101112131415161718192021vim /etc/zabbix/zabbix_server.conf#常会修改的参数#数据库配置DBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=zabbixDBPort=3306DBSocket=/var/lib/mysql/mysql.sock#服务监听端口ListenPort=10051#服务端源IPSourceIP=#日志记录方式，file使用指定文件作为日志文件，system将日志发往syslog，console将日志发送控制台LogType=fileLogFile=/var/log/zabbix/zabbix_server.log 启动zabbix服务端：1234567891011121314systemctl start zabbix-server#此处可能由于没有关闭SELinux而报错tail /var/log/zabbix/zabbix_server.logcannot set resource limit: [13] Permission denied#关闭SELinuxsetenforce=0vim /etc/selinux/configSELINUX=disabled#查看zabbix-server默认监听的10051端口netstat -nltp 安装zabbix webzabbix web可以安装在单独的主机上，只要能连接到zabbix database所在数据库就行。但为了方便，都安装在了server上。 zabbix web需要LAMP环境：12#可能需要自己配置PHP remi源，注意PHP及扩展版本问题yum install -y httpd php php-mysql php-mbstring php-gd php-bcmatch php-ldap php-xml 安装zabbix web所需的两个包：1234yum install -y zabbix-web zabbix-web-mysqlrpm -ql zabbix-web#zabbix-web位于/usr/share/zabbix/ 编辑zabbix的前端Apach-PHP配置文件zabbix前端的Apache配置文件位于 /etc/httpd/conf.d/zabbix.conf:1234567891011121314151617181920212223242526272829vim /etc/httpd/conf.d/zabbix.conf#需修改时区php_value max_execution_time 300php_value memory_limit 128Mphp_value post_max_size 16Mphp_value upload_max_filesize 2Mphp_value max_input_time 300php_value always_populate_raw_post_data -1php_value date.timezone Asia/Shanghai#建议顺便修改/etc/php.ini的时区vim /etc/php.inidate.timezone = Asia/Shanghai#添加httpd的虚拟主机访问zabbix web&lt;VirtualHost IP:80&gt;servername zabbix.medocumentroot /usr/share/zabbix默认数据&lt;/VirtualHost&gt;#开启httpd服务systemctl start httpd 添加hosts后就可以利用域名访问zabbix-web端了。 1echo -e "192.168.1.9 \t zabbix.me" &gt;&gt; /etc/hosts 在web端配置zabbix在浏览器访问 http://zabbix.me 初始化zabbix配置。配置好后就需要用账号密码进行登录zabbix-web端dashboard。 默认用户名是：admin，密码是配置文件里面设置的。 登录进Dashboard后，可修改语言为中文。 如果你的Zabbix无法看到中文选项，那么可能需要如下操作：1234vim /usr/share/zabbix/include/locales.inc.php#修改'zh_CN' =&gt; ['name' =&gt; _('Chinese (zh_CN)'), 'display' =&gt; true], 如果又遇到中文乱码的问题，则可以从windows中挑选一些好看的中文字体，将对应字体文件放置到zabbix web的字体目录中。windows中字体后缀.TTF，Linux中为.ttf。注意修改大小写。123456789101112131415cd /usr/share/zabbix/fonts#只有一个默认字体 graphfont.ttf#将新字体放置到此目录下#修改配置文件中对应字体名称vim /usr/share/zabbix/include/define.inc.php#将默认字体名字修改为字体目录下 你需要的字体名define('ZBX_FONT_NAME', 'graphfont');define('ZBX_GRAPH_FONT_NAME', 'graphfont'); // font file name#栗子，如perpetua字图PER.ttfdefine('ZBX_FONT_NAME', 'PER');define('ZBX_GRAPH_FONT_NAME', 'PER'); // font file name Zabbix Web界面菜单： 管理菜单，用于管理zabbix自身及zabbix相关设置； 配置菜单，用于配置监控相关设置； 报表菜单，为管理员生成一段时间内的监控统计信息； 检测中菜单，用于查看被监控的相关数据； 资产记录菜单，查看被监控的主机有哪些，以及相关的资产信息。 安装zabbix agentAgent端安装也非常方便，直接在Client上安装两个包即可。 1234567891011#配置zabbix源rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm#aliyun镜像#rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.4-1.el7.noarch.rpm#安装yum install -y zabbix-agent zabbix-senderrpm -ql zabbix-agent#/etc/zabbix/zabbix_agentd.conf zabbix的“主动模式”与“被动模式”都在/etc/zabbix/zabbix_agentd.conf中定义。配置最常用的agent端：12345678910111213141516171819202122232425262728293031vim /etc/zabbix/zabbix_agentd.conf####GENERAL PARAMETERS 通用配置PidFile=LogFile=####Passive checks related 被动模式配置#指定允许哪台服务器拉取本机数据Server=#指定agent端工作于被动模式时监听的端口号ListenPort=10050(默认)#指定agent端工作与被动模式时所监听的IP地址ListenIP=0.0.0.0(默认)#指定预生成的agent进程数量StartAgents=####Active checks related#agent工作于主动模式时，将消息推送到哪台Server上ServerActive=IP1,IP2...#指定当前主机主机名，Server端通过对应的主机名识别主机Hostname=#指明agent端每隔多少秒将采集的数据发往Server端RefreshActiveChecks=#栗子Server=192.168.1.9ServerActive=192.168.1.9Hostname=zabbix.me 启动zabbix-agent1234systemctl zabbix-agent start#查看状态,默认端口10050netstat -nltp 快速开始zabbix-web菜单zabbix-web界面中包含有监测中、资产记录、报表、配置、管理五项菜单。 登录和配置用户在浏览器输入 zabbix.me (修改hosts)，登录zabbix-web后台。默认用户名：Admin，密码：zabbix。它是超级管理员。 为了防止暴力破解和词典攻击，连续尝试五次登录失败，zabbix界面将暂停30秒。 可以通过管理(Management)菜单下的用户(User)，新建、查看、管理用户信息。 zabbix在安装后自定义了两个用户： Admin用户是zabbix的超级管理员，拥有所有权限； Guest用户是一个特殊的默认用户。如果你没有登录，你访问zabbix的时候其实就是“guest”权限。guest默认没有任何权限。 你可以创建一个用户(user)并将其加入特定的用户组(Group)以提升用户权限。 新建主机zabbix中的主机(host)是一个你想要监控的网络实体(物理的、虚拟的)。对于主机的定义非常灵活。它可以是一台物理服务器，一个网络交换机，一个虚拟机或一些应用。 可以通过配置(Configuration)菜单下的主机(Host)，查看已配置主机相关信息。默认有一个“Zabbix Server”的定义好的主机。 点击创建主机(Create host)后，填写对应的主机名称、添加对应的主机群组，zabbix-agent的IP地址和端口，以及其它信息。 新建监控项监控项是zabbix中获得数据的基础。没有监控项，就没有数据。因为一个主机中只有监控项定义了”单一的指标“或者”需要获得的数据“。 可以通过配置(Configuration)菜单下的主机(Item)，找到需要配置监控项(Item)的主机，然后创建监控项。主机默认是没有定义任何监控项的。 填写对应的监控名称、类型、键值、主机接口、信息类型等等信息。 可在监控(Monitoring)菜单中最新数据(Latest data)查看之前定义的监控项和获得的值。还可选择以图形(Graph)或值来查看监控项的相关信息。 同样也还以在Zabbix-Server端获得数据信息：12#zabbix_get -s $ip -k $valuezabbix_get -s 192.168.1.9 -k system.cpu.load 新建触发器监控项只用于收集数据。如果要自动评估收到的数据，我们则需要定义触发器(trigger)。触发器包含了一个表达式，这个表达式定义了数据的可接受的阈值级别。 如果收到的数据超过了定义好的级别，触发器将被触发，或者进入异常状态(problem)。从而引起我们的注意，让我们知道有问题发生。如果数据再次恢复到合理范围，触发器将会转到正常状态(OK)。 可以通过配置(Configuration)菜单下的主机(Hosts)选项，找到某主机的触发器(Triggers)创建触发器。 填写对应的触发器名称、表达式、描述等信息。 获取问题通知当监控项收集了数据后，触发器会根据异常状态触发报警。根据一些报警机制，它也会通知我们一些重要的事情，而不是直接在zabbix-web端进行查看。这就是通知(Notification)的功能。E-mail是最常用的异常通知发送方式。当然还有SMS（短信），脚本等媒体类型。 可以通过管理(Administration)菜单中的报警媒体类型(Media types)，点击预定义媒体类型列表中的Email，来配置Email。 为了建立一个通知，我们需要在配置菜单下动作中，创建动作(Create action)。 一旦满足了触发器的条件，变回触发执行动作。如收到E-mail等… 新建模板如果我们配置上前台主机，一些自动化操作会带来更多便利性。没错，模板(templates)功能就可以实现。模板允许对有用的监控项、触发器和其他对象进行分组，只需要一步就可以对监控主机应用模板，已达到反复重用的目的。 当一个模板链接到一个主机后，主机会继承这个模板中的所有对象。简单而言，一组预先定义好的检查会被快速应用到主机上。 Zabbix为各种操作系统、设备以及应用准备好了一些预定义的模板。你可以快速部署使用他们。但是请注意，一些模板需要根据你的实际情况和使用环境进行适当俄调整。 比如，一些检查项是不需要的，一些轮询周期过于频繁等。 在配置菜单下的模板(Templates)下，点击创建模板(Create template)。填写对应的模板名称，群组等信息。 创建模板完毕后，可将模板链接到主机。之后，模板及其所有对象被添加到了主机。 配置(Configuration)主机和主机组(Hosts and groups)一般来讲，zabbix主机是指你希望监控的那些设备。如服务器、工作站、交换机等。创建主机是使用zabbix过程的首要任务。 配置一台主机配置–主机–创建主机–填写相关参数信息。 可以在已经存在的主机上使用 Clone或Full Clone创建一个新主机。 Clone将保留所有的主机参数和模板链接；Full Clone将额外保留指数实体(应用集、监控项、触发器、视图、规则、Web场景)。 新建主机下： 主机(Host)：包含了通用的主机属性； 模板(Template)：允许将模板链接诶到主机，所有实体将从模板继承； IPMI：包含IPMI管理属性； 宏(Macros)：允许定义主机级别的用户宏； 主机资产记录(Host inventory)：允许为主机收工输入库存信息； 允许你请求与主机的加密的连接。 资产管理(Inventory)你可以将联网设备的资产信息保存在zabbix里。资产信息实在配置主机时人工录入建立的资产信息数据，或者通过使用某些自动填充选项完成的录入。 构建资产库： 手动模式： 在配置一台主机的时候，手动输入资产信息； 自动模式： 在配置主机的时候，选择自动。 之后便可以在资产记录菜单中的概述，主机项中查看相关信息。 批量更新(Mass update)有时候可能需要一次更改多个主机的某些属性，使用批量更新(mass update)功能来代替打开每个主机进行编辑。 可批量处理主机、模板、IPMI、资产、加密相关信息。 监控项(Items)监控项是从主机收集的数据信息。配置主机后，需要添加一些监控项以开始获取数据。快速添加多个监控项的一种方法是将预定义的模板附加到主机。 在单个监控项中，可指定从主机收集哪些数据信息。为此，可使用监控项key。 如system.cpu.load将收集处理器负载的数据。要给 key 指定更过参数，请在后面添加方括号[]。 如system.cpu.load[avg5]， 返回最近5分钟的CPU负载平均值。 创建一个监控项可在主机中新建一个监控项。不支持的监控项：如果由于某种原因无法检索该值，则该监控项可能不被支持。这些监控项仍然以固定的间隔重新检查。 监控项的key: key名称允许使用字符： 0-9a-zA-Z_-. key参数，用 逗,号 分隔： xxx[par1,par2…] key参数也可以为空，此时使用默认值： key key参数带引号，则允许任何Unicode字符，如果包含双引号则需要 \反斜杠 转义 key参数是一个数组，它需要包含在方括号中 自定义间隔(Custom intervals) 创建关于监控项的自定义时间规则。灵活间隔被设计为重新定义默认监控项的的更新间隔，但调度间隔用于指定独立执行的检查计划。 灵活的间隔(Flexible intervals)：允许重定义特定时间段的默认间隔。 间隔(Interval)： 指定时间段的更新间隔； 期间(Period)： 灵活间隔有效的时间段； 举个栗子： 60(interval), 1-7,00-24(period)。监控项每隔60s检查一次。 调度间隔(Scheduling intervals)：用于在特定时间检查监控项。 调度间隔定义为， md&lt;filter&gt;wd&lt;filter&gt;h&lt;filter&gt;m&lt;filter&gt;s&lt;filter&gt;。 md: month days(1-31) wd: week days(1-7) h: hours(0-23) m: minutes(0-59) s: seconds(0-58) : 指定其前缀的值—-[from-to/step]。 其实类似于Linux中定时任务的写法，只不过这里把单位(md,wd,h,m,s)写在了数值的前面。举个栗子：123456789101112md1-15 #1-15号wd3 #星期三h0-12 #上半天m1,3,5,7,9 #每个1,3,5,7,9分钟s/10 #每个10s#组合体wd1-5h9-18m/10 #每个工作日的上班时间每个10分钟 监控项类型(Items type)监控项类型包含从系统获取数据的多种方式。每个监控项类型都有一组自己支持的监控项key和所需的参数。 zabbix提供的监控项类型： zabbix代理检查(agent checks) SNMP代理检查 SNMP traps IPMI检查 简单检查(simple checks) VMware监控(monitoring) 日志文件监控 计算监控项(Calculated items) zabbix内部检查(internal checks) SSH检查 Telnet检查 外部检查(External checks) 汇总检查(Aggregate checks) 捕捉器监控项(Trapper items) JMX监控 ODBC监控 zabbix代理(zabbix agent)：这些检查与zabbix代理进行通信实现数据的采集。 zabbix agent-passive： 被动模式，Server向Agent索要数据； zabbix agent-active： 主动模式，Agent主动上报数据给Server。 可支持的监控项，可在新建监控项是在键值里面查看。 SNMP代理(SNMP agent)： 在启用SNMP的设备(如打印机，交换机，路由器…)上使用SNMP监控，为了能够监控SNMP代理在这些设备上提供的数据，zabbix服务器初始化配置时必须具有SNMP支持。仅通过UDP协议执行SNMP检查。 配置SNMP监控： 使用SNMP接口为设备创建一个主机； 找出要监控项目的SNMP字符串； 创建一个监控项。 IPMI检查：你可以在zabbix中监控 智能平台管理接口(IPMI) 设备的运行状况和可用性。要执行IPMI检查，zabbix服务器必须首先配置IPMI支持。 简单检查：简单检查通常用于远程无代理监控服务。 日志文件监控：zabbix可用于集中监控和分析 具有/不具有 日志转动能力的日志文件。当日志文件包含某些字符串或字符串模式时，通知信息可用于警告用户。 计算监控项：计算监控项是创建虚拟数据源的一种方式。这些值将根据算术表达式定期计算。所有计算都由Server完成。 内部检查：内部检查可以监控zabbix的内部检查。即Server或Agent Server的运行情况。 SSH检查：运行SSH检查是作为无代理监控的，SSH检查不需要zabbix代理。执行SSH检查zabbix服务器必须初始化配置为SSH2支持。 SSH检查提供两种身份验证方法，一种是用户/密码，另一种是基于密钥文件。 zabbix SSH 密钥配置:1234567891011vim /etc/zabbix/zabbix_server.conf#SSHKeyLocation=SSHKeyLocation=/home/zabbix/.sshusermod -m -d /home/zabbix zabbixchown zabbix:zabbix /home/zabbixchmod 700 /home/zabbixcd /home/zabbix &amp;&amp; su zabbixssh-keygen -t rsa 外部检查：外部检查是由zabbix Server通过运行shell脚本或二进制的检查。外部检查不需要再被监控的主机上运行任何代理。 汇总检查：在汇总检查中，zabbix通过直接从数据库中查询监控信息，然后进行信息聚合。聚合检查不需要再被监控的主机上运行任何代理。 捕捉器监控项：捕捉器监控项接收传入的数据，而不是查询它。对于想要推送到zabbix的任何数据都是适用的。 要使用捕捉器监控项，需要在zabbix中建立一个捕捉器监控项，将数据送给zabbix。 JMX监控项：JMX监控可用于监视Java应用程序的JMX计数器。JMX监视器以zabbix守护进程方式运行，名为zabbix java gateway。 ODBC监控：ODBC监控对应于zabbix web管理端中的数据库监控器监控项类型。ODBC是用于访问 数据库管理系统(DBMS) 的C语言中间件API。 zabbix可以查询ODBC支持的任何数据库。为了实现监控，zabbix不直接连接到数据库，而是使用ODBC中设置的ODBC接口和驱动。该功能允许为多个目的更加有效地监控不同的数据库。 历史与趋势(history and trends)历史与趋势是zabbix中存储数据的两种方式。历史保持每个收集的值，而趋势是每小时的平均信息。 建议保持的历史数据尽可能少，但可以保留更多的趋势数据。 用户自定义参数(user parameter)有时你想运行一个代理检查，但它不是zabbix预定义的。这时就能用到用户参数。用户参数是由zabbix代理之星的命令，最多可以返回512KB的数据。key 是唯一的。 用户参数用法：12345678910111213141516171819202122UserParameter=&lt;key&gt;,&lt;command&gt;#栗子UserParameter=ping,echo 1#使用ping键为一个监控项返回 1#复杂栗子UserParameter=mysql.ping,mysqladmin -uroot -ppwd ping | grep -c 'alive'#mysqld状态为alive返回1，否则0#灵活的用户参数UserParameter=key[*],command#[*]定义该key接受括号内的参数#栗子UserParameter=ping[*],echo $1UserParameter=mysql.ping[*],mysqladmin -u$1 -p$2 ping | grep -c 'alive'#mysql.ping[zabbix,passwd]UserParameter=wc[*],grep -c "$2" $1#wc[/etc/passwd,root] 用户自定义参数扩展zabbix代理：是将key添加到被监控的主机哦！123456789101112131415161718#编写命令--SQL查询总数mysqladmin -uxxx -pxxx status | cut -f4 -d":" | cut -f1 -d"S"#将命令添加到zabbix_agentd.confvim /etc/zabbix/zabbix_agentd.conf#找到如下字段### Option: UserParameterUserParameter=mysql.totalquery,mysqladmin -uroot -pxxx status | cut -f4 -d":" | cut -f1 -d"S"#mysql.totalquery这个key是唯一的标识符#测试此参数##测试参数可用与否很重要哈zabbix_agentd -t mysql.totalquery#重启zabbix-agent，将重新加载配置zabbix_get -s $host -k mysql.totalquery 可加载模块(loadable modules)可加载模块提供了一种关于zabbix性能扩展的选项。 可加载模块基本上只zabbix守护程序使用的共享库，并在启动时加载。可加载模块具有很多优点，卓越的性能和可实现任何逻辑的能力，更重要的是使用和共享了zabbix模块的开发能力。 windows性能计数器(windows perfomance counter)使用perf_counter[]key有效的监控windows性能计数器 批量更新(mass update)使用批量更新功能，可一次更改多个监控属性。 值映射(value mapping)对于接收值更人性化的表示，可以使用包含数值和字符串之间的映射的值映射。 如： 0 —&gt; error 1 —&gt; true F —&gt; Full D —&gt; Differential I —&gt; Incremental … 应用集(Application)应用集对逻辑组中的监控项进行分组。 如，对MongoDB的可用性，空间，负载，慢查询，执行命令…，可归于 MongoDB应用于中。 队列(queue)队列显示正在等待刷新的监控项。队列只是一个逻辑表达的数据。 队列显示的统计信息是zabbix服务器性能是否健康的指标。在 管理–队列 下对去队列。 值缓存(value cache)为了计算触发表达式，以及让计算/聚合监控项和一些宏更快，zabbix服务器支持值的缓存选项。 在内存中的缓存可用于访问历史数据，而不用之间调用数据库。如果缓存中不存在历史值，则从数据库请求缺少的值，并相应地跟新缓存。 要启用值缓存功能，修改zabbix_server.conf中可选的ValueCacheSize参数。 触发器(Trigger)触发器是评估有项目采集的数据并表示当前系统状况的逻辑表达式。触发器表达式允许定义一个什么状况的数据是“可接受”的阈值。如果超过了可接受状态，则触发器会被触发。 配置一个触发器(configuring a trigger)在主机里面配置触发器。 触发器表达式(trigger expression)一个简单有效的表达式看起来像：1234&#123;&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)&#125;&lt;operator&gt;&lt;constant&gt;#如&#123;192.168.1.7:agent.ping.time()&#125;=0 函数参数(function parameters)：大多数数字型的函数接受秒数来作为参数。 1234567891011121314#600s内所有值的总和sum(600)#随后5个值总和sum(#5)avg()count()last()min()max()#5m 可被 300s 代替#1k 代表 1024bytes 运算符(operators)： 优先级 运算符 定义 1 - 负号(minus) 2 not 逻辑非(NOT) 3 *, / 乘，除 4 +, - 加，减 5 &lt;, &lt;=, &gt;, &gt;= - 6 =, &lt;&gt; 相等，不等于 7 and 逻辑与 8 or 逻辑或 触发器示例：12345678910111213&#123;www.zabbix.com:system.cpu.load[all,avg1].last()&#125;&gt;5&#123;www.zabbix.com:system.cpu.load[all,avg1].last()&#125;&gt;5 or &#123;www.zabbix.com:system.cpu.load[all,avg1].min(10m)&#125;&gt;2&#123;www.zabbix.com:net.if.in[eth0,bytes].min(5m)&#125;&gt;100k&#123;$url1:net.tcp.service[smtp].last()&#125;=0 and &#123;$url2:net.tcp.service[smtp].last()&#125;=0&#123;$host:icmpping.count(30m,0)&#125;&gt;5&#123;$host:system.cpu.load[all,avg1].min(5m)&#125;&gt;2 and &#123;$hsot:system.cpu.load[all,avg1].time()&#125;&gt;000000 and &#123;$host:system.cpu.load[all,avg1].time)()&#125;&lt;060000... 滞后(Hysteresis):有时候需要一个触发器状态OK和PROBLEM之间的间隔，而不是简单的阈值。 要做到这一点，我们首先定义一个PROBLEM事件的触发器表达式，然后为OK选择 ‘Recovery expression’，并未OK事件书如不同的表达式 如：1234567#Problem expression&#123;server:temp.last()&#125;&gt;20#Recovery expression&#123;server:temp.last()&#125;&lt;=15#两者之间便有了几个滞后值 触发器依赖(trigger dependency)有时候，一台主机的可用性取决于另一台主机。如一台路由器后的上网设备。这就是主机之间某些依赖关系可能有用的地方，依赖关系设置的通知可能会被抑制，而只发送根本问题的通知。 zabbix中触发器的依赖，一个触发器可能有多个依赖于它的触发器。 路由器和路由器后的Server同时宕机，如果有依赖关系，则zabbix不会执行服务器的触发动作。值得注意的是，如果触发器所依赖的触发器被禁用，则次触发器的事件和动作将不会被抑制。 批量更新使用批量更新，可一次更改一些触发器的某些属性。 触发器严重性(trigger severity)触发器严重性定义了触发器的重要程度: 未分类(not classified), 灰色 信息(information), 淡蓝 警告(warning), 黄色 一般严重(average), 橙色 严重(High), 淡红 灾难(disaster), 红色 自定义触发器严重性(customising trigger)在 管理 – 一般 – 触发器严重性，里面自定义触发器严重性。 预测触发功能(predictive trigger function)有时候有即将到来的问题的迹象。可以发现这些迹象，以便提前采取行动，以减小影响。 zabbix具有基于历史数据预测受监视系统的未来行为的工具，这些工具通过预测触发功能实现。 事件标签(event tag)在zabbix中可以自定义事件标签，在触发器级别上定义事件标签。在事件标签定以后，相应的新事件被标记为时间标签数据。在拥有自定义时间标签的情况下，可以变得更加灵活。 例如： 识别日志文件中的问题并单独关闭他们； 用它来过滤通知； 查看前端的事件标签信息； 从项目值中提取的信息作为标签值； 在通知中更好地识别问题； 通过使用模板级别的标签来建华配置任务； 使用低级别发现的标签创建触发器。 事件(Events)zabbix可以生成一下几种类型的事件： trigger events-触发器事件； discovery events-发现事件； auto registration events-自动注册事件； internal events-内部事件； 事件以时间戳，并可以发送Email等基础动作。在 监控-问题 里面查看信息信息。 触发器事件生成(trigger events generation)触发器状态的变化是事件最常见和最重要的来源。每次触发器的状态改变时，都会生成一个事件。改时间包含了触发器状态变更的详细信息、发生时间以及信息的状态。 触发器会创建两种类型的事件：问题(problem)和正常(OK) 手动关闭问题事件(manual closing of problems)当触发器状态从“问题(problem)”变成“正常(OK)”时，很难判断是通过触发器表达式的方式解决。这时就需要手动解决。 只有在触发器中启用 “允许手动关闭” 选项，问题事件才可以被手动关闭。 其他事件来源(other event source)zabbix定期扫描网络发现规则中定义的IP范围，可以为每个规则单独配置检查频率。一旦发现主机或服务，就会生成一个发现事件。 zabbix可以生成以下事件：1234Service Up/DownHost Up/DownService Discovered/LostHost Discovered/Lost 事件关联(event correlation)通常，在zabbix中正常事件会关闭所有的问题事件，但在某些情况下需要更细致的方法。可以根据事件标签关联问题事件。如，当监控日志文件时，在日志文件中想要发现某些问题，并将它们单独关闭，而不是一起关闭。 可视化(visualisation)图形(graphs)大量的监控数据被采集到zabbix中，如果能用可视化的表现形式来查看，那就直观和容易多了。 zabbix为用户提供了如下图形： 监控项数据的内置简单图形 “simple graphs”； 创建更复杂的自定义图形 “customised graphs”； 特定图形 “ad-hosc graphs”快速访问几个监控项的数据比较。 简单图形(simple graphs)：zabbix提供的简单图形，用来可视化显示监控项采集到的数据。并不需要配置就可以查看。 通过 监控-最新数据-图形 来展示图形。 自定义图形(customised graphs)：自定义图形，提供定制功能。这就有点厉害了。这个是手动配置的。可以为单个主机、多个主机、单个模板、多个模板创建自定义图形。 在 配置-主机-图形-创建图形 里编辑图形属性；图形编辑后可点击预览。 特设图形(ad-hoc graphs)：简单图形和自定义图形都不允许快速创建多个监控项目数据的比较图形，工作量小且没有维护。 在 检测-最新数据-旋转监控项前复选框-显示数据图(显示堆叠数据图) 下， 里面也包含了 正常和层积 的图形风格。 拓扑图(networking maps)运维人员如果想要了解网络环境的基础设施状况，可以在zabbix中创建网络拓扑图。 配置拓扑图(configurating network maps): 在 监控-拓扑图 下，可以创建拓扑图。点击拓扑图中的 构造函数 选项，来打开编辑区域。然后在编辑区域中添加元素和链接元素。 链接指示器(link indicators):可以为网络拓扑图中的元素之间的链接分配一些触发器，当这些触发器状况为“Problem”时，可以在链接上体现出来。如果多个触发器进入”Problem”状态，则严重程度最高的将决定链接的颜色和样式。 聚合图形(screen)在zabbix的聚合图形页面上，你可把各种来源的信息聚集到一起，一边在单个屏幕上快速查看。在 监测-图形聚合 下，对其进行创建、配置、管理和查看。 基本上，聚合图形是一个表格，你选择把每个表格有多少单元格以及其中要显示的元素。元素如下： 简单图形； 简单图形原型； 用户自定义图形； 自定义图形原型； 拓扑图； 其他聚合图形； 纯文本信息； 服务器信息； 触发器信息； 主机/主机组信息； 系统状态； 数据概述； 时钟； 事件历史； 动作历史； URL。 幻灯片演示(slide shows)在幻灯片演示中，可以配置多个聚合图形以设定的间隔逐个显示。在 监测-聚合图形-幻灯片演示 下。 模板(template)模板是可以方便地应用于多个主机的一组实体。 配置模板(configuring a template)：配置模板需要首先通过定义一些参数来创建模板，然后添加实例。在 配置-模板-创建模板 链接模板(linking)：链接是将模板应用于主机的过程，之后主机将拥有模板的所有实体。 嵌套(nesting)：嵌套是一种包含一个或多个其它模板的模板方式。可以在一个嵌套模板中奖一些模板链接在一起。 嵌套的好处在于，您只需要讲一个模板链接到主机，并且主机会自动继承链接的模板的所有实体。 事件通知(notifications upon events)当配置了一些项目和触发器，并且由于触发器改变状态，现在正在发生一些事件，之后就要考虑 action。发送通知是zabbix提供的主要操作之一。 为了能够发送和接收通知，必须： 定义一些media； 配置action，向指定的media发送消息。 action由condition和operation组成。当条件满足是，执行操作。操作主要是 发送消息和执行远程命令。 media类型媒体是zabbix中发送通知和警报的传送通道。 E-mail:在 管理-媒体类型 下，配置Email。 SMS：zabbix支持使用连接到zabbix-server的串行端口的串行GSM调制解调器发送SMS消息。 确保： 串行设备的速度(在Linux下通常为/dev/ttyS0) 与 GSM调制解调器的速度相匹配。zabbix没有设置串行链路的速度，它使用默认设置。 zabbix用户对串行设备有读写访问权限。 GSM调制解调器输入PIN码，并在电源复位后保留PIN码。或者在SIM卡上禁用PIN。 管理-媒体类型下要为用户分配电话号码：管理-用户-报警媒介，添加报警媒介(如电话号码等) Jabber：zabbix支持发送jabber消息。 Ez Texting：可以使用 zabbix技术合作伙伴 Ez Texting发送信息。 脚本：警报脚本在zabbix服务器上执行，这些脚本位于服务器配置文件中定义的目录中(AlertScriptsPath)。12345678910111213141516cat /etc/zabbix/zabbix_server.confAlertScriptsPath=/usr/lib/zabbix/alertscripts#创建报警脚本vim /usr/lib/zabbix/alertscripts/zabbix_test.sh#!/bin/bashto=$1subject=$2body=$3cat &lt;&lt;EOF | mail -s &quot;$subject&quot; &quot;to&quot;$bodyEOF 然后我们在创建脚本媒体的时候，写入相关参数。 actions可以根据所有支持的类型的时间定义操作： 触发事件：当trigger的状态从OK转到Problem或回转时； 发现事件； 自动注册事件； 内部事件； 配置-动作-创建动作 条件(condition):只有在事件与定义的条件匹配的情况下才执行操作。 注意运算类型：似与非似 操作(operation)：操作：发送信息，执行远程命令。 1.发送消息；2.远程命令(不支持在zabbix-agent上执行远程命令，需要在zabbix-server到代理的命令才能直接连接。远程命令限制255字符，可以将过个命令放置于新行上来执行过个命令。及时目标主机处于维护状态，也会执行远程命令)； 配置-动作-操作，在操作细节中修改操作类型为远程命令。支持自定义脚本、SSH、Telnet等方式。 在信息中使用宏(using macros in messages)：在消息主题和消息文本中，可使用宏来更有效的问题报告。 恢复操作(recovery operation):恢复操作允许在问题解决时通知我们。恢复操作支持消息和远程命令。 宏(macros)zabbix支持许多在多种情况下使用的宏。宏是一个变量，由如下特殊语法标识。 {MACRO} 根据在上下文汇总，宏解析为一个特殊的值。有效地使用宏可以节省时间，病史zabbix更加高效。 宏可以在监控项键值参数中使用。宏只能用在监控项键值参数的一部分中。如item.key[server_{HOST.HOST}_local] 。 宏函数(macro function)：宏函数能提供自定义宏值的功能。 宏函数语法：12345678&#123;&lt;macro&gt;.&lt;func&gt;(&lt;params&gt;)&#125;#&lt;macro&gt;, 要定义的宏#&lt;func&gt;, 要应用的函数#&lt;params&gt;, 以逗号分隔的函数参数列表#栗子&#123;&#123;ITEM.VALUE&#125;.regsub&#123;pattern, output&#125;&#125; 用户宏(user macro)：除了支持开箱即用的宏之外，zabbix还支持更灵活的用户宏。 用户宏可在全局、模板和主机级别进行定义。有一个特殊语法：1&#123;$MACRO&#125; 用户宏可用于： 监控项名称； 监控项键值参数； 触发器名称和描述； 触发器表达式参数和常量； 许多其他位置。 自动发现宏：有一种自动发现(LLD)函数中使用的宏类型，可用于创建监控项、触发器和图形原型。然后，当发现真实的文件系统、网络接口等，这些宏将替换为真实的值，并且以这些值来创建真实的监控项、触发器和图形。1&#123;#MACRO&#125; 用户和用户组(user and group)zabbix中所有用户都通过web前端去访问zabbix应用程序。并为每一个用户分配唯一的登录名和密码，被加密储存于zabbix数据库中。 配置用户(configuring user)管理-用户，创建和管理用户。 权限(permission)可定义相应的用户类型，如用户，管理员和超级管理员。 用户组(groups)管理-用户组，创建和配置用户组。 服务监控(service monitoring)服务监控，旨在帮助那些想要高级业务监控的人。在很多情况下，我们关注的不是底层细节，而是提供的可用性服务。 服务是分层表示监控数据。 IT Workstations workstation1workstation2 Services 配置-服务，最高节点的服务是’root’。你可以通过添加低级服务节点和各个节点服务创建下层层次结构。 Web监控(web monitoring)配置-主机-web监测，创建或修改web监测信息。可使用zabbix检查几个网站可用性方面。(zabbix中包含libcurl库才行) 要使用web监控，需要定义web场景。包括一个或多个HTTP请求或步骤。Zabbix-Server根据预定义的命令周期性的执行这些步骤。 所有web场景会手机下列数据： 整个场景中所有步骤的平均下载速度； 失败的步骤数量； 最后一次错误信息 web场景的所有步骤，都会收集下列数据： 平均下载速度； 响应时间 HTTP状态吗 Web监控项(web monitoring items)在创建web场景时，会自动添加一些新监控项进行监控。 创建场景后，zabbix会自动添加以下监控项进行监控，将它们链接到所选的应用程序。 场景的下载速度； 场景的失败步骤； 场景的最后一个错误消息； 举个栗子：1234567891011121314151617181920212223242526272829303132333435363738394041##创建Web监测#配置-主机-Web监测-创建web监测URL：web.zabbix.me/monitor.php要求的状态码：200超时：20s##创建web监测触发器#配置-主机-触发器-创建触发器严重性：一般严重#触发条件：状态码!=200表达式：N&lt;&gt;200##创建触发报警对应的动作#配置-动作-创建动作#触发条件触发器示警度=一般严重 or 触发器=web.zabbix.me#操作：发送Email发送给zabbix administrator用户群组仅送到Email默认信息/自定义信息##在媒体类型中定义Email相关信息#管理-报警媒体类型-EmailSMTP服务器：smtp.xxx.comsmtp端口：465SMTP电邮：发件人Email安全链接：SSL/TLS认证：Usernameand passwd用户名：xxx密码： xxx##接下来就可以测试接收报警Email了 虚拟机监控(VM monitoring)zabbix支持对VMware的监控，使用low-levle-discovery(LLD)自动发现VMware hypervisors和虚拟机，并根据事先定义的主机原型，为这些虚拟机建立主机，添加监控。 zabbix中提供了几个模板，可以直接用来解控VMware vCenter 或 ESX hypervisor。 虚拟机监控分为两个步骤： 首先，zabbix是通过VMware collector进程来监控虚拟机。这些进程通过SOAP协议从VMware服务获取必要的信息，对其进行预处理并储存到zabbix-server共享内存中； 然后，zabbix-pollers通过zabbix简单检查VMware keys来检索这些数据。 要使虚拟机监控正常工作，需要libxml2库和libcurl库的支持。 配置-自动发现-创建自动发现配置-主机-自动发现 维护(maintenance)可在zabbix中为主机和主机组定义维护周期。有两种维护类型：“继续对目标进行监控数据的收集” 和 “停止对目标进行监控数据的收集” 要在维护期间正常接收问题通知，必须在动作配置中的选项中取消选择暂停操作。为了确保定期维护按照预期的时间进行，需要对zabbix的所有部分使用通用时区。 配置-维护-创建维护期 维护期的主机显示的是橙色背景！ 事件确认(event acknowledgment)zabbix中的问题事件可以由用户确认。 如果用户获得了有关问题时间的通知，可以访问zabbix前端，从时间导航到确认屏幕并确认问题。当他们确认时，可输入评论或其他一些相关描述。这样其他系统用户同样的问题，他们便会立即看到是否已被解决和目前的评论。 以这种方式，可以更协调的进行解决多个系统用户的问题的工作流程。 要确认事件，用户必须至少要有对相应触发器的读取权限。 在Dashboard下，在出现的问题里，点击确认，进入确认事件。也可在监控-问题下查看问题详细信息。 配置导出/导入(Configuration export/import)zabbix导入/导出功能，使得可以在一个zabbix系统与另一个zabbix系统之间交换各种配置实体。类似于数据库的导入导出。即也可以对zabbix做备份。 可导出/导入的对象有：主机组； 模板； 主机； 拓扑； 图片； 聚合图形； 值映射。 数据也可导出： XML - 在前端 XML or JSON - 在zabbix API 导出的详细信息： 所有支持的元素都导出到一个文件中； 不导出从连链接模板继承的主机和模板实体； 由低级别发现创建的实体依赖于他们的任何实体不会导出。 导入详细信息： 第一次遇到错误停止导入； 导入支持XML和JSON文件； 使用“删除缺失”选项导入主机/模板时，导入的XML文件中不存在主机/模板宏也将被删除。 将Zabbix展现在Nginx上毕竟现在Nginx用的多，那就把Apache换成Nginx吧！ Nginx仓库:http://nginx.org/packages/ 自己安装Nginx: 下载nginx-release-xx.rmp仓库源来安装； 手动创建/etc/yum.repo.d/nginx.repo； 直接下载ngix.rpm来安装； 直接下载源码来安装。 相较于Apache，Nginx也只是配置个server就行了。优化什么的自己弄。12345678910111213141516171819202122232425vim /etc/nginx/conf.d/zabbix.confserver &#123; listen 80; server_name zabbix.me; root /usr/share/zabbix; location / &#123; if (!f $request_filename) &#123; rewrite ^([^\?]+)$ /index.php?1=$1 last; &#125; &#125; location ~ \.php$ &#123; root /usr/share/zabbix; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125;&#125;nginx -tsystemctl start nginx 下载就可以正常访问zabbix-web端了! Zabbix监控Zabbix自带的templates基本涵盖了大部分监控信息。 大部分操作系统： OS Linux; OS AIx; OS FreeBSD; OS Solaris; OS Windows; … 大部分服务： CPU; Filesystems; HTTP/HTTPS service; Memory; Network interfaces; Processes; Secutity; Zabbix server/agent/Proxy; SMTP,POP,SSH,NTP, service; ICMP Ping; SNMP; … 虚拟机： VM VMware; VM WMware Hypervisor; … 网络设备： Cisco; Huawei; TPLink; HP; … 除了Zabbix自带的templates，你还可以下载templates并导入zabbix-server。 例如PHP, MongoDB, Apache, Nginx, Redis等额外软件的监控就需要下载额外templates。 监控MongoDB感谢大神： MongoDB-templates: https://share.zabbix.com/databases/mongodb/mongodb-for-zabbix-3-2 ; GitHub: https://github.com/oscm/zabbix/tree/master/mongodb 此github-repo中还包含了Oracle, php-fpm, postfix, redis, Nginx。可参看README.md来配置zabbix对它们的监控。 安装步骤1. 在zabbix-agent安装jqjq - Command-line JSON processor; 1yum install -y jq 2. 在zabbix-agent的MongoDB中创建用于监控的账号创建用于读取MongoDB相关信息的账户及其权限。 12345678910111213mongo&gt;use admin&gt;db.createUser( &#123; user:'zabbix', pwd:'zabbix', roles:[&#123; role:'clusterMonitor', db:'admin'&#125;] &#125;) 3. 在agent下载github仓库的MongoDB模板等文件 12345678910wget https://codeload.github.com/oscm/zabbix/zip/master#这里面不仅仅有mongodb，还有redis,php等。#我们只需要进入mongodb目录就好cd ./zabbix-master/mongodbls#mongodb.sh , 执行脚本#userparameter_mongodb.conf ，配置脚本#zbx_export_templates.xml，zabbix模板文件 4. 移动并配置mongodb.sh 1234567891011cp ./mongodb.sh /etc/zabbixchmod a+x /etc/zabbix/mongodb.shvi mongodb.shDB_HOST=127.0.0.1DB_PORT=27017DB_USERNAME=zabbixDB_PASSWORD=zabbix 5. 移动并修改userparameter_mongodb.conf 123456cp ./zabbix-master/userparameter_mongodb.conf /etc/zabbix/zabbix_agentd.dvi ./userparameter_mongodb.confUserParameter=mongodb.status[*],/etc/zabbix/mongodb.sh $1 $2 $3 $4 $5#修改为mongdb.sh真实位置 6. 重启zabbix-agent 1systemctl restart zabbix-agent 7. 在zabbix-web导入mongodb模板 配置-模板-导入模板； 选择./master/mongodb/zbx_export_templates.xml模板文件，并导入； 接下来便可以在 templates中看到”Template App MongoDB”这个模板； 可将此模板链接到某个主机上监控，并到最新数据里查看相关MongoDB信息； 如果相对此模板就行修改，可编辑zbx_export_templates.xml文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Love at First Sight]]></title>
    <url>%2F2017%2F11%2F06%2FLove-at-First-Sight%2F</url>
    <content type="text"><![CDATA[——波兰诗人维斯拉瓦·辛波丝卡(Wislawa Szymborska) They’re both convincedthat a sudden passion joined them. Such certainty is beautiful,but uncertainty is more beautiful still. Since they’d never met before,they’re sure that there’d been nothing between them. But what’s the word from the streets, staircases, hallways –perhaps they’ve passed each other a million times? I want to ask themif they don’t remembera moment face to facein some revolving door?perhaps a “sorry” muttered in a crowd?a curt “wrong number” caught in the receiver?but I know the answer. No, they don’t rememberThey’d be amazed to hearthat Chance has been toying with themnow for years. Not quite ready yetto become their Destiny,it pushed them close, drove them apart,it barred their path, stifling a laugh,and then leaped aside. There were signs and signals,even if they couldn’t read them yet. Perhaps three years agoor just last Tuesdaya certain leaf flutteredfrom one shoulder to another? Something was dropped and then picked up.Who knows, maybe the ball that vanished into childhood’s thicket? There were doorknobs and doorbellswhere one touch had covered another beforehand. Suitcases checked and standing side by side.One night, perhaps, the same dream,grown hazy by morning. Every beginning is only a sequel,after all,and the book of eventsis always open halfway through.]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>Poetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinuxShell脚本攻略]]></title>
    <url>%2F2017%2F10%2F24%2FLinuxShell%E8%84%9A%E6%9C%AC%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[小试牛刀简介1234567891011#bash(Bourne Again Shell)，shell环境使得用户能与操作系统的内核进行交互操作#!/bin/bash#date#descriptioncmd1; cmd2cmd3#sh /path/xx.sh#Bash还有一个历史记录文件 ~/.bash_history 终端打印12345678910111213141516171819#终端作为交互式工具，用户可以通过它与shell环境进行交互echo '$var'echo $varecho -e "1\t2\t3"echo -e '\e[1;31m Red color \e[0m' #彩色echo &#123;1..10&#125; #输出1到10echo &#123;A..H&#125; #for i in &#123;a..z&#125;cat &lt;&lt; EOF112233EOF# \转义字符printf "%-5s %-10s $-4.2f\n" 001 Zhang 56.789#格式替代符%s %d %c %f, -左对齐 玩转变量和环境变量123456789101112131415161718#Bash中，每一个变量默认值值都是字符串形式#环境变量和自定义变量echo $SHELLecho $UIDvar=value #这是赋值#var = value这是相等操作echo $varecho $&#123;var&#125;echo $&#123;#var&#125; #字符数#export用来设置环境变量，此后，任何shell中的程序都会继承环境变量ZHANG=Gentlemanexport ZHANGPATH="$PATH:/home/zhang/bin"export $PATH 通过shell进行数学运算12345678910111213141516171819202122232425262728#let, expr, bc, [], (())#要注意默认是字符串类型哦n1=1;n2=2let sum=n1+n2let n1++;let n2-=1sum=$[ n1 + n2 ]sum2=$(( sum + 3 ))sum=`expr 3 + 4`#浮点计算 bcecho "8 * 1.1" | bc#设置小数点精度echo "scale=2; 3/8" | bc#进制转换num=100echo "obase=2; $num" | bcnum=1100100echo "obase=10; ibase=2; $num" | bc#平方和平方根echo "sqrt(100)" | bcecho "10^2" | bc 文件描述符重定向1234567891011121314#最常用的文件描述符是 stdin(0), stdout(1), stderr(2); 通过内容过滤将输出重定向到文件echo "This is a sample text 1" &gt; temp.txt #覆盖echo "This is sample text 2" &gt;&gt; temp.txt #追加ls + &gt;stdout.txt 2&gt;stderr.txtcmd 2&gt;&amp;1 /dev/null == com &amp;&gt; /dev/null #null设备也被称为黑洞#当一个command发生错误并退回时，它会返回一个非0的状态码echo $?#tee命令，一方面可将数据重定向到文件，另一方面还可提供一份重定向数据的副本作为后续命令的stdin#tee默认覆盖文件，-a选项追加cat temp.txt | tee tee.txt | cat -n 数组和关联数组123456789101112131415161718#数组借助索引将多个独立的数据存储为一个集合#普通数组只能使用整数作为数组索引，而关联数组可以使用字符串作为数组索引#还可将数组定义成一组索引-值(index-value)arr=(1 two 3 four 5)echo $&#123;arr[0]&#125;arr[0]=Oneindex=3echo $&#123;arr[$index] #arr[3]echo $&#123;arr[*]&#125;echo $&#123;#arr[*]&#125; #arr-length#关联数组可用任意文本作为数组索引declare -A ass_arrass_arr=([index1]=val1 [index2]=val2 ...) #内嵌索引-值ass_arr[index3]=val3 #独立索引-值echo $&#123;!ass_arr[*]&#125; #列出数组索引 别名123456789#alias作用是暂时的，关闭终端后别名就失效；#为使别名一直保持，可将其写入 ~/.bashrc，因为每一个新的shell都会执行~/.bashrc中的命令#新设置的别名将取代已有别名alias vi=vim; unalias viecho "alias ll='ls -l --color=auto'" &gt;&gt; ~/.bashrc#\对别名命令进行转义，执行原本的命令。避免攻击者利用别名将某些特权命令替换成别有用心的命令\vi test.sh 获取、设置日期和延时12345678910111213141516#很多应用程序需要以不同的格式打印日期，设置日期和时间，以及根据日期和时间执行操作;#延时通常用于在程序执行过程中提供一段等待时间;#在Unix-like系统中，日期被存储为一个整数，其大小为世界标准时间1970年1月1日0时0分0秒起所流逝的秒数；#这种计时方式被称之为 纪元时或Unix时间；#通过纪元时间，可知道两个日期之间相隔了多少秒#编写以循环方式运行的监视脚本时，设置时间间隔是必不可少的date +%s#!/bin/bashstart=$(date +%s)commandssleep 1end=$(date +%s)diff=$((end - start))echo "$diff seconds" 调试脚本12345#调试功能能在出现一些异常情况时生成运行信息#!/bin/bash -xvsh -xsh -n 函数和参数123456789101112131415161718192021function fname()&#123;statements&#125;fname()&#123;echo $1, $2 #访问第参数1和参数2,$n第n个参数echo "$@" #以列表的形式一次性打印所有参数echo "$*" #类似于$@，但参数被作为单个实体return 0 #f返回值&#125;fname 1 22 333 #返回上面定义的变量#递归函数，能够调用自身，不断地生成新的进程，最终会造成xx#导出函数，使用export导出，这样函数作用域就可以扩展到子进程export -f fname#读取命令返回值echo $? 读取命令序列输出1234567#输入通常是stdin，输出stderr或stdout,这些命令称为 过滤器(filter)。我们使用 管道(pipe) 来连接每一个过滤器cmd1 | cmd2 | cmd3#子shell，子shell生成独立的进程，不会对当前shell有任何影响，所做改变仅限于子shell内zhang=$(ls | cat -n)#反引用zhang=`ls | cat -n` 读取字符123456789101112131415161718#read是一个重要的从标准输入中读取文本的命令#可以使用read以交互的形式来读取用户的输入read -n 5 zhang #读取字符数echo $zhangread -s passwd #不回显echo $passwdread -t 5 zhang #超时时间echo $zhangread -p zhang #显示提示信息echo $zhangread -d ":" zhang #定界符结束输入123：echo $zhang 字段分隔符和迭代器12345678910111213141516171819#内部字段分隔符(Internal Field Separator, IFS)是shell中的一个重要概念#IFS的默认值为空白字符(换行符、制表符、空格)awk -F: '&#123;print $1,$3&#125;' /etc/passwd #IFS=":"#对一些列值进行迭代，循环非常有用for i in &#123;1..10&#125;docmddonewhile conditiondocmddoneuntil conditiondocmddone 比较与测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546#程序中的流程控制是由比较和测试语句来处理的if condition1 || condition2then cmd1elif condition3 &amp;&amp; condition4then cmd2else cmd3fi#算术比较if [ $num -ge 10 -a $num -lt 20 ]-eq-gt-ge-lt-le-a-o#文件系统相关if [ -f $file1 -o -x $file2]-x-w-r-f-d-e-b #block-l#字符串比较[[ $str1 = $str2]]= #=号旁有空格--是比较关系；=号旁没空格，是赋值语句!=&gt;&lt;-z #空字符-n #非空字符#使用test命令来执行条件检测if [ $num -eq 0 ] -- if test $num -eq 0 命令之乐简介各种命令可谓Unix-Like系统中优美的部分，它能帮我们搞定各种繁杂的任务。一旦你尝试过Linux提供的这些利器，你一定会感到惊讶：以前没有这些命令的时候，自己是什么熬过来的。最钟爱的莫过于 grep, awk, sed, find 命令了！ 本章将会为你介绍一些最有趣同时也是最实用的命令。 用cat进行拼接12345678#cat命令通常用于读取、显示或拼接文件内容，不过它所具备的能力远不止此#cat(concatenate, 拼接)cat file1 file2 ···echo "Ahaha" | cat - file1 file2 #-指stdin文本文件名cat -s file3 -- cat file3 | tr -s '\n' #压缩空白行cat -T test.py #将制表符显示为 ^I, 避免制表符和连续空格误用, 产生错误缩进cat -n file4 #显示行号 录制与回放终端会话当你需要准备一个命令行教程时，如果将我们输入命令后的一切按照先后次序记录下来，再进行回放，是不是很nice！通过 script, scriptreplay 命令, 把终端会话记录到文件，并回放。 12345678#-t,将时间数据输出到标准错误； -a,追加输出script -t 2&gt; timing.log -a output.session #两个文件随意取名, 如不将错误重定向会显示在屏幕上导致很乱输入命令cmd2···exit #退出录制scriptreplay -t timing.log output.session #播放 文件查找与文件列表find 是Unix/Linux命令行工具箱中最棒的工具之一。find 命令沿着文件层次结构向下遍历，匹配符合条件的文件，并执行相应的操作。 find - search for files in a directory hierarchy1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#基于文件名及正则表达式搜索find /home/zhang #列出/home/zhang目录及其子目录线所有文件和文件夹find /home/zhang -name "*.txt"find . -name "*.sh" -o -iname "zhang*" #匹配多个find /home/zhang -path "201710*" #-path将文件路径作为一个整体进行匹配find . -regex ".*\(\.txt|\.[0-9]+\)$" #匹配以.txt或数字结尾的文件#使用-maxdepth, -mindepth参数，来限制find的遍历深度#-type, 根据文件类型搜索。 f(普通文件)，d(目录)，b(块设备)，l(符号链接)，s(套接字)等find /home -maxdepth 1 -type f(d) #参数顺序也会影响find的查找效率#根据文件类型搜索find /dev -type b #查看/dev及其子目录下设备文件find / -maxdepth 1 -type l #查找/下链接文件#根据文件时间进行搜索#Unix/Linux文件系统中的每一个文件都有三种时间戳(timestamp),-表示小于，+表示大于#Unix中并没有所谓的 "创建时间" 的概念#访问时间(-atime,以天为单位； -amin,以分钟为单位):用户最近一次访问文件时间；#修改时间(-mtime,以天为单位； -mmin,以分钟为单位):文件最后一次修改时间；#变化时间(-ctime,以天为单位； -cmin,以分钟为单位):文件元数据(如权限，所有权)最后一次变化时间；find /home/zhang -type f -mtime 7 #7天前被修改的普通文件find /home/zhang -type f -amin -10 #搜索10分钟内被修改的普通文件find . -type f -newer file1.txt #找出比file1.txt新的文件#基于文件大小的搜索#b(块，512字节), c(字节), w(字，2字节), k(千字节), M(兆字节), G(吉字节)find . -type -f -size +100k#删除匹配的文件find . -type f -name "*.swp" -delete#基于文件权限和所有权的匹配find . -type f -perm 644find /var/apache -type f -name "*.php" -perm 644 #搜索基于权限的文件find /var -maxdepth 2 -type f -user zhang #搜索基于用户的文件#执行命令或动作#find命令可以借助-exec与其他命令进行结合#&#123;&#125;是一个特殊字符串，将替换为相应文件名find . -type f -perm 764 -user zhang -exec chmod 644 &#123;&#125; \; #将所属用户zhang，权限764的文件权限修改为644find . -type f -mmin +30 -name "*.txt" -exec cp &#123;&#125; &#123;&#125;.old \; #复制最近30内修改的名字为.txt的文件#-exec结合多个命令#我们无法在-exec参数中直接使用多个命令，不过我们可以把多个命令写到一个shellscript中，然后执行-exec ./test.sh &#123;&#125; \;find . -type f -name "*.sh" -mmin -10 -exec sh &#123;&#125; \;#让find跳过特定目录-prune 利用stat命令查看atime, mtime, ctime :123456#stat - display file or file system statusstat 1.txt#Access:#Modify:#Change: 利用touch命令修改atime, mtime, ctime：123456#touch - change file timestamps#-a change only the access time#-m change only the modification time#-d instead of current time#-t instead of current time 玩转xargsxargs - build and execute command lines from standard input1234567891011121314151617181920212223242526#xargs能够处理stdin并将其转换为特定命令的命令行参数#也可以将单行或多行输入文本转换成其他格式(如多行变单行)cmd | xargs#将多行输入转换为单行输出echo -e "1\n2\n3" | xargs #将换行符替换为空格#将单行输入转换成多行输出echo "1 2 3" | xargs -n 1 #每行一个参数echo "hahaZhahaZhahaZhaha" | xargs -n 2 -d Z #-d指定分隔符#读取stdin，将格式化参数传递给命令cat test.txt | xargs -n 1 ./zhang.sh #zhang.sh arg1; zhang.sh arg2... 每次提供一个参数cat test.txt | xargs -n X ./zhang.sh #X为参数个数，一次提供全部参数#指定替换字符串cat test.txt | xargs -I &#123;&#125; ./zhang.sh &#123;&#125;#结合find使用xargsfind . -type f -name "*.txt" -print0 | xargs -0 ls #-print0无换行输出, -0将\0作为输入界定符#统计某文件行数find /path -type f -name "*.c" -print0 | xargs -0 wc -l#结合stdin，运用while和子shellcat file.txt | while read arg; do cat $arg; done == cat file.txt | xargs - &#123;&#125; cat &#123;&#125;cmd0 | (cmd1; cmd2; cmd3) | cmd4 #子shell 用tr进行转换tr - translate or delete characters12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#tr命令经常用来编写优美的单行命令#tr可对来自stdin的字符 进行替换、删除以及压缩echo "AH WONDERFUL" | tr 'A-Z' 'a-z' #转换大小写echo "AH WONDERFUL" | tr 'A-Z' 'a-b' --&gt; ab bbbbbbbbb#tr [option] set1 set2#如果两个字符集长度不相等，那么set2会不断重复其最后一个字符，直到长度与set1相同echo 12345 | tr '0-9' '9876543210' #数字加密echo 87654 | tr '9876543210' '0-9' #数字解密echo 'He is a cool boy, and she is a beautiful girl' | tr 'A-Za-z' 'NOPQRSRUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm' #加密echo 'Ur vf n pbby obl, naq fur' | tr 'NOPQRSRUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm' 'A-Za-z' #解密cat 1.txt | tr '\t' ' ' #将制表符转换为空格#删除字符echo "Hello 530 World" | tr -d '0-9' #-d删除，删除数字Hello Worldecho "Hello 520 World" | tr -d -c '0-9' #-c补集 520#压缩字符，将连续的重复字符压缩为单个字符echo "GNU's not Unix" | tr -s ' ' #-s压缩，压缩空格GNU's not Unixecho -e "1\n2\n3\n4\n5" &gt; sum.txtcat sum.txt | echo $[ $(tr '\n' '+') 0 ] -- echo $[1+2+3+4+5+0]#tr字符类\a 终端鸣响\b 退格\f 换页\n 换行\r 回车\t 水平制表符\v 垂直制表符string1-stringN #从字符1到字符N升序过程中的所有字符[字符*次数][:alnum:] #所有字母和数字[:alpha:] #所有字母[:digit:] #所有数字[:lower:] #所有小写字母[:upper:] #所有大写字母[:graph:] #所有可打印字符，不含空格[:print:] #所有可打印字符，包含空格[:blank:] #所有水平排列的空白字符[:cntrl:] #所有控制字符[:punct:] #所有标点字符[:space:] #所有空白字符[:xdigit:] #所有十六进制数[=字符] #指定字符 校验和 与 核实文件完整性12345678910111213141516171819#校验和(checksum)程序从文件中生成校验和密钥，然后利用校验和密钥核实文件的完整性#校验和对于编写备份脚本或系统维护脚本非常重要，因为它们都会涉及通过网络传输文件#通过使用校验和核实，我们就可以识别那些在网络传输过程中出现损坏的文件，并重传，从而确保数据完整性#校验和对于核实数据完整性非常有用#广泛使用的校验和技术有：md5sum, sha1sum#对单个文件进行校验md5sum sum.txt &gt; sum.md5#302c28003d487124d97c242de94da856 sum.txtmd5sum -c sum.md5 #-c检查#sum.txt: 确定#对目录进行校验#对目录计算校验和意味着我们需要对目录中的所有文件以递归的方式进行计算yum install -y md5deepmd5deep -r ./dir &gt; dir.md5 #recursive递归md5sum -c dir.md5#可以将测试dir下某个文件更改一下，校验的时候会报错 排序、单一、重复12345678910111213141516171819202122232425262728#sort - 对文本文件进行行排序#uniq - 删除排序文件中的重复行echo -e "333\n1" &gt; 1.txt; echo -e "22\n22" &gt; 2.txtsort 1.txt 2.txt -o ./sorted.txt#1#22#22#333cat sortec.txt | uniq#1#22#333sort -n #按数字进行排序sort -r #逆向排序sort -M #按月份排序sort -C #检查是否排序sort -b #忽略空白#依据键或列进行排序sort -k 2 data.txt #依据第二列来排序#uniq要么使用管道，要么使用排过序的文件作文输入uniq -u sorted.txt #只显示唯一的行(即没有重复出现的行)uniq -d sorted.txt #只显示重复的行uniq -s 2 -w 2 sorted.txt #-s忽略前2个字符，-w指定用于比较的最大字符数 临时文件命名、随机数123456#在编写shell脚本时，我们经常需要存储临时文件。最适合存储临时数据的位置是 /tmp#/tmp目录中的内容会在系统重启后被清空filename=$RANDOM #RANDOM返回一个随机数filename2=$$ #当前shell的PIDfilename3=$((date +%F)) #通过日期命令 分割文件和数据123456789101112131415161718#某些情况下，需要把文件分割成多个更小的片段dd if=/dev/zero bs=100k count=1 of=./data.file #生成一个大小100k内容全是0的文件split -b 20k data.file #-d指定分割大小#data.file xaa xab xac xad xae,这五个文件都为20k#我测试了一下，几个文件加起来数据没变，几个文件总行数没变#单位有 k, m, G, c(byte), w(word)#-d以数字为后缀， -a指定后缀长度split data.file -b 20k -d -a 2 spt #增加前缀名'spt'#data.file spt00 spt01 spt02 spt03 spt04split -l 10 data.file #-l按行数来分割文件#split只能根据大小或行数分割文件#csplit可以根据文件本身特点进行分割-f #指定分割后文件前缀-n #指定分割后文件后缀数字个数-b #指定后缀格式 根据扩展名切分文件名12345678910111213141516171819202122232425#借助%操作符将名称从 “名称.扩展名” 格式中提取出来file="zhang.txt"name1=$&#123;file%.*&#125; #删除位于%右侧的通配符(.*)所匹配的字符串，通配符从右向左进行匹配#zhang#*号通配符，.号#%属于非贪婪匹配(non-greedy),它会匹配通配符最短结果#%%属于贪婪匹配(greedy)，它会匹配符号条件的最长字符串name2=$&#123;file#*.&#125; #删除位于#右侧的通配符(*.)所匹配的字符串，通配符从左向右进行匹配#txt# #属于非贪婪匹配# ##属于贪婪匹配#栗子URL=“www.google.com”echo $&#123;URL%.*&#125; #非贪婪匹配，移除最右边.及其后面内容www.googleecho $&#123;URL%%.*&#125; #贪婪匹配wwwecho $&#123;URL#*.&#125; #非贪婪匹配，移除最左边.及其前面内容google.comecho $&#123;URL##*.&#125; #贪婪匹配com 批量重命名和移动1#综合运用find、rename、mv命令 拼写检查与词典操作123456#Linux大多数发行版都含有一份词典文件，另外还有一个被称为aspell的拼写检查命令#words --&gt; /usr/share/dict/linux.wordsgrep "^good" /usr/share/dict/linux.wordsaspell 交互输入自动化123456789101112131415161718192021222324252627282930#写一个读取交互式输入脚本vi jiaohu.sh#!/bin/bashread -p "Input a number:" numread -p "Input name:" nameecho "You have enterd number:$num, name:$name"echo -e "1\nzhang" | ./jiaohu.shYou have entered number:1, name:hello#orecho -e "1\nzhang" &gt; input.txt./jiaohu.sh &lt; input.txt#交互式输入自动化#用expect实现自动化yum install -y expectvim auto_expect.sh#!/bin/expectspawn ./jiaohu.sh #spawn指定需要自动化哪一个命令expect "Input a number:" #expect提供需要等待的消息send "1\n" #send是要发送的消息expect "Input name:"send "zhang"expect eof #expect eof指明命令交互结束./auto_expect.sh 以文件之名简介Unix将操作系统中的一切都视为文件。 生成任意大小的文件由于各种原因，可能需要生成一个包含随机数据的文件。 1234#dd命令会克隆给定的输入内容，然后将一模一样的副本写到输出#如果不指定if，dd会从stdin中读取输入；如果不指定of，dd会输出到stdout#/dev/zero是一个字符设备，它会不断返回0值字节(\0)dd if=/dev/zero of=junk.data bs=1M count=1 文本文件的交集与差集1234567891011#comm命令用于两个文件之间的比较#交集(intersection),差集(set difference), 求差#comm必须使用排过序的文件作为输入echo -e "1\n2\n3" &gt; A.txt &amp;&amp; echo -e "3\n2\n3" &gt; B.txtsort -n A.txt -o A.txt &amp;&amp; sort -n B.txt -o B.txtcomm A.txt B.txt#输出第一列为A独有，第二列为B独有，第三列为交集comm A.txt B.txt -1 -2#-1从输出中删除第一列，-2删除第二列，-3删除第三列 查找并删除重复文件1234#重复文件指的是那些虽然名字不同但内容却一模一样的文件ls -lS #以文件大小排序，识别大小相等的文件md5sum #接下来计算这些文件的校验和 创建长路径目录1mkdir -p /home/zhang/1/22/333 2&gt;/dev/null 文件权限、所有权和粘滞位12345678910111213141516171819202122232425262728293031#用户(user)，用户组(group)，其他用户(other)ll ./*#d目录，c字符设备，b块设备，l符号链接，s套接字，p管道，-普通文件#用户还有一个称为setuid(S)的特殊权限，它出现在用户的x位置#setuid权限允许用户以其拥有者的权限来执行可执行文件，即便这个文件是由其他用户运行的-rwSrw-r--#组也拥有一个setgid(S)权限，它出现在组的x位置#它允许以同该目录拥有者所在组相同的有效组权限来运行可执行文件-rwxrwSr--#目录有一个特殊权限，叫做粘滞位(sticky bit)(T或t)，出现在其他用户的x位置#当一个目录设置了粘滞位，只有创建该目录的用户才能删除目录中的文件,即便group和other有w权限-rwxr--rwTchmod u=rwx g=rw o=r file1chmod u+x g-w file2chmod 744 file3chmod a+x . -R #以递归方式设置权限chown user.group . -R #以递归方式设置所有权chmod a+t dir1 #设置粘滞位chmod +s fiel4chown root.root file4chmod +s file4./file4 #每次file4都是以root运行#setuid的使用不是无限制的，它只能应用在Linux ELF格式二进制，而不能用于脚本文件。 创建不可修改文件123456#不可修改(immutable),是保护文件不被修改的安全手段之一。#一旦文件被设置为不可修改，任何用户(包括root)都不能修改，除非将其不可修改属性移除chattr #修改文件在Linux第二扩展文件系统(E2fs)上的特有属性chattr +i file1 #这样就无法删除file1chattr -i file1 批量生成空白文件1234567#touch命令可用来生成空白文件，如果文件存在，则可以用它修改文件的时间戳for name in &#123;1..100&#125;.txt;dotouch $namedonetouch -a/-m #更改文件访问/修改时间touch -d "Thu Oct 31 14:20:13 CST 2017" file1 #指定特定时间戳 查找符号链接及其指向目标123456#符号链接(软链接)只不过是指向其他文件的指针ln -s /usr/bin /binls -l / | grep "^l"find / -maxdepth 1 -type lreadlink /bin #找出链接目标 列举文件类型统计信息123#在Unix/Linux系统中，文件类型并不是由文件扩展名决定的file /etc/passwdfile -b /etc/passwd 环回文件与挂载123456789#环回文件系统是指那些在文件中而非物理设备中创建的文件系统dd if=/dev/zero of=loopback.file bs=1G count=1mkfs.ext4 loopback.filemount -o loop loopback.file /mnt/loopback #-o loop来挂载环回文件df -humount /mnt/loopback#将ISO文件作为环回文件挂载mount -o loop linux.iso /mnt/iso 生成ISO文件以及混合ISO123456#可引导光盘自身具备引导能力，也可以运行操作系统或其他软件。不可引导光盘则做不到这些。cat /dev/cdrom &gt; /dev/sdc #sdc指U盘dd if=/dev/cdrom of=/dev/sdc #将ISO写入usb存储设备mkisofs -V "Label" -o /dev/sdc /dev/cdromcdrecord -v dev=/dev/cdrom image.iso 查找文件差异并进行修补diff - compare files line by line123456789101112#补丁文件(patch file)#diff命令可以生成差异文件diff -u file1 file2 #一体化形式输出diff -u file1 file2 &gt; diff.patchpatch -p1 file1 &lt; diff.patch #得到file2patch -p1 file2 &lt; diff.patch #得到file1patch -R file1 &lt; diff.patch; patch -R file2 &lt; diff.patch #还原#diff也能够以递归的形式作用于目录，它对目录中所有内容生成差异输出diff -Naur dir1 dir2#-N将所有确实文件视为空文件， -a将所有文件视为文本文件#-u生成一体化输出， -r遍历目录下所有文件 head与tail123456head file1; tail file1 #head与tail默认打印10行head -n 5 file1; tail -n 6 file1 #指定行数head -n -5 file1 #打印除了最后5行外所有行tail -n +(5+1) file1 #打印除了开始5行外所有行tail -f /var/log/nginx/access.log #--follow，动态关注文件 只列出目录的其他方法1234ls -d .ls -l . | grep &quot;^d&quot;ls -F . | grep &quot;/$&quot;find . -maxdepth 1 -type d pushd和popd12345678910111213#在命令行中使用pushd和popd快速定位，pushd和popd以栈的方式运作#当没有鼠标时，复制粘贴就不怎么实用了#pushd和popd可以用于在多个目录之间进行切换而无需复制并粘贴目录路径pushd /home/user1; pushd /home/user2; pushd /home/user3 #将路径添加到栈pushd +2 #切换到/home/user3popd #移除最近添加入栈的目录cd /root; cd /home/usercd - #回到上次的目录cd .. #切换到上一级目录cd ~ #切换到用户主目录 统计文件的行数、单词数、字符数123456#wc(word count)，是一个统计工具wc -l file1 #统计行数wc -w file1 #统计单词数wc -c file #统计字符数wc -L file #打印最长行长度wc file1 #行、单词、字符数 目录树12345678#tree命令是以图形化的树状结构打印文件和目录,在Linux发行版中默认未安装yum install -y treetree /home/zhangtree /home/zhang -P "*.sh" #只标记出.sh文件tree /home/zhang -I "*.sh" #标记出除.sh文件外所有文件tree /home/zhang -h #显示大小tree /home/zhang -H http://localhost -o tree.html #以html形式输出目录树 让文本飞简介shell脚本可以将sed, awk, grep, cut等这类优美的工具组合在一起，用于解决文本处理相关问题。 正则表达式正则表达式是一种用于文本匹配的形式小巧、具有高度针对性的编程语言。只依靠通配符技术，能够匹配的文本范围相当有限。 正则表达式基本组成 正则表达式 描述 ^ 行起始标记 $ 行尾标记 . 匹配任意一个字符 [] 匹配包含在[]中的任意一个字符 [^] 匹配出[^]之外任意一个字符 [-] 匹配[]中范围内的任意一个字符 ？ 重复0或1次 + 重复&gt;=1次 * 重复&gt;=0次 () 创建一个用于匹配的子串 {n} 重复n次 {n, } 重复&gt;=n次 {n,m} 重复n到m次 \ 转义字符 竖线l 匹配竖线l两边任意一项 POSIX字符类 POSIX字符类(POSIX character class),是一个形如[:…:]的特殊元序列，它用于匹配特定的字符范围。 正则表达式 描述 [:alnum:] 字母与数字字符 [:alpha:] 字母字符 [:blank:] 空格与制表符 [:digit:] 数字字符 [:lower:] 小写字母 [:upper:] 大写字母 [:punct:] 标点符号 [:space:] 所有空白字符 元字符 元字符(meta character)，是一种Perl风格的正则表达式，只有一部分文本处理工具支持它。 正则表达式 描述 \b 单词边界 \B 非单词边界 \d 单个数字字符 \D 单个非数字字符 \w 单个单词字符(数字，字母和_) \W 单个非单词字符 \s 单个空白字符 \S 单个非空白字符 \n 换行符 \r 回车 12345#匹配一个ipv4地址[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;#匹配一个邮箱地址[\w]+@[\w]\.com 用grep在文件中搜索文本grep命令是Unix中用于文本搜索的工具，它能够接受正则表达式和通配符。12345678910111213141516grep "匹配文本/通配符" file1 file2... --color=auto #重点标记匹配grep -E "正则表达式" fileegrep "正则" filegrep -v #反向匹配grep -c #统计匹配行数grep -n #打印出匹配的行号grep -o #唯一匹配grep -l "匹配" file1 file2 #返回匹配的文件名grep -R #递归匹配grep -i #忽略大小写grep -e "匹配1" -e "匹配2" #匹配多个样式grep -f match.txt file1 #从match.txt文件读取匹配grep "匹配" --include=*.&#123;sh,txt&#125; --exclude=*.log --exclude-dir=/home/user -r /home #包括或排除文件-A/-B n #输出匹配 之后/之前 n行-c n #输出匹配 前后 n行 用cut按列切分文件cut是一个将文本按列进行切分的小工具，它也可以指定每列定界符。在cut的术语中，每列都是一个字段。 1234567#制表符'\t' 是cut默认的定界符cut -d' ' -f1 1.txt #-d指定分隔符，-f打印第几个字段cut -f1,2,3 #打印1,2，3列-c字符； -b字节；cut -c 1-5 1.txt #打印1-5字符cut -c -2 1.txt #打印前2个字符cut -c 3- #打印第3个字符到行尾 统计特定文件词频123#单词解析可以用 关联数组,正则表达式配合sed,awk,grep等工具来完成#关联数组中，将单词作为数组索引，单词次数作为数组值egrep -o "\b[:alpha:]+\b" #匹配单词 sed入门sed是stream editor(流编辑器)的缩写，它是文本处理中非常重要的工具。能够完美地配合正则表达式使用。1234567891011121314151617181920212223242526272829303132333435363738#sed - stream editor for filtering and transforming text#字符/在sed中最为定界符使用#替换#sed 's/匹配样式/替代字符串/'sed 's/pattern/repalce/' file #替换sed -i 's/pattern/repalce/' file #将替换应用于fileecho "1.txt" &gt; 1.txt &amp;&amp; sed 's/txt/haha' 1.txt #在输出中用haha替换txtsed -i 's/txt/haha/' 1.txt #将1.txt文件中的txt用haha替换掉#-i选项替换原文件echo "hahaha" | sed 's/ha/HA/g' #全部替换echo "hahaha" | sed 's/ha/HA/2g' #指定位置替换，从第2处开替换全局#移除匹配样式的行sed '/pattern/dsed '/^$/d' ##移除空白行#在sed中用&amp;标记已匹配字符串echo "A wonderful goal" | sed 's/\w\+/[&amp;]/g' #\w\+匹配每一个单词#子串匹配标记\1,\2...echo "1st 2nd 3rd" | sed 's/\(\w\+\) \(\w\+\) \(\w\+\)/\2 \1 \3/'2nd 1st 3rd#将\2和\1交换次序，(),+等在sed中要转义，否则要报错#组合多个表达式sed 'expression1; expression2; ...echo "aabbcc" | sed 's/a/A/; s/b/B/; s/c/C/g'AaBbCC#双引号 " " 内的特殊符号（如$等），可以保有原本的特性#单引号 ' ' 内的特殊字符则仅为一般字符（纯文本）#引用text=helloecho 'hello world' | sed "s/$text/HELLO/"HELLO world awk入门awk被设计用于数据流，它可以对列和行进行操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#awk ‘begin&#123;print "start"&#125; pattern &#123;command&#125; end&#123;print "end"&#125;’ fileawk '&#123;sum += $1&#125;; &#123;print sum&#125;'#awk脚本由:begin块、end块和能使用模式(pattern)匹配的通用语句块 组成#3个部分都是可选的#awk也可以从stdin中读取内容cat /etc/passwd | awk -F: '&#123;print $1&#125;' #-F指定界定符#awk中的特殊变量#NR：记录数量(number of records)，对应于当前行号#NF：字段数量(number of fields)，对应于当前行的字段数#$0：执行过程中当前行的文本内容#$1,$2...$NF：第1个/2个.../最后一个 字段的内容echo -e "L1 1\nL2 22\nL3 333" | awk '&#123;print NR NF $0 $1 $2&#125;'# NR NF $0 $1 $2 $NF=最后一个=$2 1 2 L1 1 L1 1 1 2 2 L2 2 L2 2 2 3 2 L3 3 L3 3 3#将外部变量传递给awk#-v选项可将外部值传递给awk# -v var=val --assign=var=valvar='12345'echo | awk -v v1=$var '&#123;print v1&#125;'#多个变量var1=111; var2=222echo | awk '&#123;print v1,v2&#125;' v1=$var1 v2=$var2#变量来自文件而非标准输入awk '&#123;print v1,v2&#125;' v1=$var1 v2=$var2 file#用样式对awk进行过滤处理awk 'NR &lt; 3,NR==4' 1.txt #行号&lt;5的行awk '/linux/' 1.txt #匹配带有linux的行（可用re）awk '!/linux/' 1.txt #!匹配不带linux的行#设置定界符awk -F: '&#123;print $1&#125;' /etc/passwdawk '&#123;FS=":"&#125; &#123;print $1&#125;' /etc/passwdawk '&#123;FS=":"; print $1&#125;' /etc/passwd#从awk中读取命令输出，用getline读取行echo | awk '&#123;"grep root /etc/passwd" | getlin out; print out&#125;'root:x:0:0:root:/root:/bin/bash#在awk中使用循环awk '&#123;for(i=1;i&lt;4;i++) &#123;print $i&#125;&#125;' 2.txt #输出第1,2,3列 对文件中的行、单词、字符进行迭代123456789101112131415161718192021222324252627282930#迭代文件中的每一行echo -e "1\n22\n333" | while read line;do echo $line;donegrep "bash" /etc/passwd | while read line;do echo $line;done#1#22#333#迭代一行中的每一个单词echo "1 22 333" | while read line;do for word in $line;do echo $word;done;done#1#22#333#迭代一个单词中的每一个字符echo "abc" | while read line;do for word in $line; do for((i=0;i&lt;$&#123;#word&#125;;i++)); do echo $&#123;word:i:1&#125;; done; done;done#写成一行echo "abc" | while read line; do for word in $line; do for((i=0;i&lt;$&#123;#word&#125;;i++)); do echo $&#123;word:i:1&#125;; done; done; done#a#b#c#$&#123;#word&#125;返回变量word的长度 按列合并文件可以使用paste命令实现列拼接12345678#paste - merge(整合) lines of filesecho -e "1\n2\n3" &gt; 1.txt &amp;&amp; echo -e "Line1\nLine2\nLine3" &gt; 2.txtpaste 1.txt 2.txt1 Line12 Line23 Line3#默认定界符是制表符，用-d指定paste 1.txt 2.txt -d',' 打印文件或行中的第n个单词或n列12awk -F':' '&#123;print $1,$3&#125;' file1cut -d':' -f 1,3 file1 打印不同行或样式之间的文本123456awk 'NR==1,NR==10' /etc/passwdawk 'NR==1,NR==10' /etc/passwd | awk -F":" '&#123;print $1,$NF&#125;' #打印特定行内的特定列awk '/start_pattern/, /end_pattern/' file #打印start到end之间的内容,可使用reawk '/root/, /zhang/' /etc/passwd #打印root到zhang之间内容awk '/^ro.?t'/, /bash$/' /etc/pass 以逆序形式打印行可以使用awk, tac完成。tac就是反过来的cat。123#tac - 反转显示文件中的行，行内的内容无法用tac反向排列tac 1.txtawk '&#123;lifo[NR]=$0; lno=NR&#125; END&#123; for(;lno&gt;-1;lno--) &#123;print lifo[lno]&#125;;&#125;' 1.txt 解析文本中的电子邮件和URL从给定的文件中解析出所需要的文本是我们从事文本处理时的一项任务。 grep, egrep, fgrep - print lines matching a pattern1234567#egrep#匹配一个邮箱地址egrep -o '[a-zA-Z0-9.]+@[0-9a-zA-Z.]+\.[a-zA-Z]&#123;2,4&#125;' emails.txt#匹配一个URL地址egrep -o "http://[a-zA-Z0-9.]+\.[a-zA-Z]&#123;2,3&#125;" urls.txt 打印某个样式之前/之后n行123grep "zhang" /etc/passwd -A 5 #Atergrep "zhang" /etc/passwd -B 5 #Beforegrep "zhang" /etc/passwd -C 5 #前后五行都打印 在文件中移除包含某个单词的句子只要能写出正确的正则表达式(Regular Expression)，那就手到擒来 1sed 's/[^.]*handsome boy[^.]*\.//g' file.txt #句子以.结束 文本切片与参数操作1234567891011121314#替换变量内容中的部分文字var="One two three"echo $&#123;var/t/T&#125; #只替换了一个#One Two three#指定字符串起始位置和长度#$&#123;变量:开始部分:长度&#125;$&#123;vari:start:length&#125;echo &#123;var:0:2&#125; #Onecho &#123;var:1:6&#125; #ne two#起始字符的索引是0,将最后一个字符索引记为-1echo $&#123;var:(-1)&#125; #eecho $&#123;var:(-3):3&#125; #ree 一团乱麻？没这回事入门本章会研究一些用于解析网站内容、下载数据、发送数据表单以及网站颇为任务自动化之类的实例。我们可以仅用几行脚本就将很多原本需要通过浏览器交互进行的活动管理自动化。通过命令行工具利用HTTP协议所提供的功能，我们可以用脚本解决大部分Web自动化的问题。 网站下载使用一些命令行下载工具，从给定的URL中下载文件或网页。 wget是一个用于文件下载的命令行工具，选项多且用法灵活。 123456789101112131415161718192021222324252627282930313233#Wget - The non-interactive(非交互式) network downloaderwget URL1 URL2...wget http://xxx.com/nginx-1.12.0.tag.gzwget https://xxx/a.rpm http://xxxx/bb.rpm#指定文件名，指定信息输出(wget默认是stdout)wget http://mirrors.aliyun.com/repo/Centos-7.repo -O aliyun.repo -o ./wget.logwget URL -t 5 #-t，重试次数#下载限速wget --limit-rate=10m URL #下载限速wget -Q 100m URL #指定下载配额#端点续传#wget进行的下载在完成前被中断，从断点开始下载wget -c URL#用cURL下载#cURL是一个比wget更强大的高级命令工具#和wget不同，curl并不将下载数据写入文件，而是写入stdout，因此必须重定向到文件#复制或镜像整个网站#wget有一个选项可以使其像爬虫一样以递归方式手机网页上所有URL链接，并逐个下载#这样一来就可以下载一个网站的所有页面wget --mirror URL#-m(--mirror) -N -r -l inf --no-remove-listing 的缩写形式。或 wget -r -N -l DEPTH URL#-r递归下载，-l指定递归深度，-N(timestamp)只获取比本地时间新的文件#访问需要认证的HTTP或FTP页面wget --user "username" --password "pass" URL#如未在命令行内输入密码，则会由网页提示手动输入 以格式化纯文本下载网页网页其实就是包含HTML标记和其他诸如Javascript，CSS等元素的HTML页面。HTML标记是网页的基础，也许需要解析网页来查找特定的内容。 links,是一个基于命令行的Web浏览器 12345678910#links - lynx-like alternative character mode WWW browser#在命令行中浏览一个网页links www.baidu.com#以ASCII形式下载网页links --dump URL &gt; URL.txt#打开本地html文件links 1.html cURL入门cURL支持包括HTTP、HTTPS、FTP在内的众多协议。它还支持POST、cookie、认证、从指定偏移处下载部分文件、参照页(referer)、用户代理字符串、扩展头部(extra header)、限速、文件大小限制、进度条等特性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#curl - transfer a URL#cURL通常将下载文件输出到stdout，将进度信息输出到stderr#要想避免显示进度信息，可使用--silent#curl可用来下载、发送各种HTTP请求、指定HTTP头部等操作curl URL --silent #输出到stdout#-O写入文件，文件名从URL中解析curl http://www.baidu.com/index.html -O --silent #创建index.html#-o将数据写入指定文件curl URL -o baidu.html --progress #--progress显示进度条links baidu.html#端点续传#和wget不同，cURL包含更高级的下载恢复特性，能够从特定的文件偏移处继续下载#curl可以通过指定一个偏移量来下载部分文件手动：curl URL/file -C offset #偏移量以Byte为单位的整数自动：curl -C -URL #自动续传#用cURL设置参照页字符串, --referer#参照页(referer)是位于HTTP头部中的一个字符串，用来标识用户从哪个页面到达当前页面的#如果用户点击网页A中某个链接，转到了网页B。那么网页B头部的referer会包含网页A的URLcurl --referer Referer_URL target_URLcurl --referer http://www.baidu.com http://jianshu.com#用cURL设置cookie, --cookie#可以用curl来存储HTTP操作过程中使用到的cookie#cookie用key=value形式，指定多个用 分号 分隔curl URL --cookie "user=AAA;name=bbb"curl URL --cookie-jar cookie.txt #将cookie另存为#用cURL设置用户代理字符串, --user-agent#如果不指定代理，一些需要用户代理的网页就无法显示curl URL --user-agent(-A) "Mozilla"#用-H "头部信息"传递多个头部信息curl -H "Host:www.haha.com" -H "Accept-language: en" URL#限定cURL可占用的带宽curl URL --limit-rate 10m#指定最大下载量curl URL --max-filesize 大小(Bytes)#用cURL进行认证，-u username:password指定用户名和密码curl -u user:pass URLcurl -u user URL #手动输入密码#只打印响应头部信息(无数据部分), -Icurl -I URL 从命令行访问163邮箱12curl -u user http://mail.163.com#手动输入密码 制作图片抓取器及下载工具可以用脚本解析图像文件并将图片自动下载下来。 123456curl -s URL | grep -o "&lt;img src=[^&gt;]*&gt;" | sed 's/&lt;img src=//g; s/&gt;//g' &gt; img.list#匹配图片的URL，可能还需要细化修改#不同的URL可能有不同的规则，根据实际情况取出img的URL#下载图片wget $URL 或 curl -s -O $URL 查找网站中的无效链接将查找无效链接的工作自动化，那就比纯手动厉害多了！ 123456789lynx -traversal URL #会将URL中所有链接生成到reject.dat文件中sort -u reject.dat | while read linkdo output=`curl -I $link -s | grep "HTTP/.*OK"` if [[ -z $output ]] then echo $link fidone &lt; links.txt 跟踪网站变更可以编写一个定期运行的变更跟踪器(change tracker)，一旦发生变更，跟踪器便会发出声音或发送提示信息。在不同时间检索网站，然后利用 diff 命令进行比对。 123curl URL --silent -o `date +%F`.html #第一次curl URL --silent -o `date +%F`.html #第二次diff -u 第一次 第二次 以POST方式发送网页并读取响应POST 和 GET 是HTTP协议中用于发送或检索信息的两种请求类型。在GET请求方式中，利用网页的URL来发送参数(“键-值”)；而POST方式用于提交表单，如提交用户名、密码以及检索登录页面等。 1234curl URL -d “postarg=AABBCC” #-d,http post datacurl URL -d "post1=key1&amp;post2=key2&amp;post3..." #指定多个数据wget URL -post-data "post1=key1" Plan B 简介提取快照和备份数据都是重要的工作，我们可以通过shell脚本来实现备份自动化。归档和压缩对于SA来说同样很重要，有多种压缩格式。加密是一种保护数据的方法，为了减少加密数据的大小，文件在加密前通常需要先归档和压缩。 用tar归档tar命令可以用来归档文件(tar archives tar)。可以将多个文件和文件夹打包为单个文件，同时还能保留所有的文件属性。由tar命令创建的文件通常称为tarball。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#归档文件，-c(create file)tar -cf 1.tar [sources] #-f(specify filename)指定文件名#文件名必须紧跟在-f之后tar -cvf txt.tar *.txt #-v(verbose)详细信息#向已归档文件中添加文件，-rtar -rvf txt.tar *.html#列出归档文件中的内容，-ttar -tf txt.tar #列出归档内容tar -tvf txt.tar #列出内容详细信息#从归档文件中提取文件或文件夹，-x(exact)tar -xf txt.tar #默认提取到当前目录#-C指定提取目录tar -xvf txt.tar -C /dir/path#只提取归档中特定文件tar -xf txt.tar 1.txt 1.html -C /tmp #只会提取1.txt和1.html文件#在tar中使用stdin和stdouttar -cvf - *.text | tar -xvf - -C /tmp#拼接两个归档文件，-Atar -Af txt.tar html.tartar -tvf txt.tat #验证是否成功#添加选项，可以将指定的任意文件加入到归档文件中。如果同名文件已存在，不会覆盖源文件，那么结果就是归档中包含了多个同名文件#通过检查时间戳来更新对党文件中的内容，-u#只有比归档文件中同名文件 更新(newer) 才添加tar -uvf html.tar 1.html#比较归档文件与文件系统中的内容，-dtar -df txt.tar 1.txt 2.txt#从归档文件中删除文件，--deletetar -f txt.tar --delete 1.txt 2.txt#从归档文件中排除部分文件,--excludetar -cf all.tar ./* --exclude="*.html" #排除.html文件tar -cvf txt.tar *.txt --exclude="1.txt"#打印总字节数,--totalstar -cf all.txt ./* --totals#压缩tar归档文件，指定不同压缩格式#-z, .tar.gz#-j, .tar.bz2#--lzma, .tar.lzma,#.tar.lzotar -czvf txt.tar.gzip *.txttar -xzvf txt.tar -C /dir/path 用cpio归档cpio是类似于tar的另一种归档格式。它多用于RPM软件包、Linux内核和initramfs文件等。cpio通过stdin获取输入，并将归档写入stdout。 1234567touch file&#123;1..4&#125;echo file1 file2 file3 file4 | cpio -ov file.cpio#-o指定输出，-v打印归档文件列表#-i指定输入，-t列出归档中文件cpio -it &lt; file.cpio 用gunzip或gzip压缩gzip是GNU/Linux下常用压缩格式。gzip,gunzip都可处理gzip压缩文件类型。gzip只能够压缩单个文件，而无法对目录和多个文件进行归档。因此需要先交给tar，然后再用gzip压缩 1234567891011121314151617181920gzip file #file.gz，会覆盖原文件gunzip file.gz #file，也会删除原文件#列出压缩文件的属性信息，-lgzip -l file.gz#指定gzip的压缩级别，--fast或--best--fast 最低压缩比，最快速度完成--best 最高压缩比，最慢速度完成#将gzip与归档文件结合，-ztar -czvf txt.tar.gzip ./*.txt#-a指定从文件扩展名自动判断压缩格式tar -cavf txt.tar.gzip ./*.txt#tar只能从命令行中接收有限个文件，要解决这个问题，可以写一个循环并添加-r选项#解压缩，-xtar -xzvf txt.tar.gziptar -xavf txt.tar.gzip -C /dir/path 用bunzip或bzip压缩bzip2通常能够生成比gzip更小(压缩比更高)的文件。 12345678910111213141516171819bzip2 file #file.bz2,同理会覆盖原文件bzip2 file -k #保留原文件bunzip2 file.bz2 #解压缩bunzip file.bz2 -k#从stdin读入并写到stdoutcat file | bzip2 -c &gt; file.bz2#将bzip2与归档文件结合，-jtar -cvjf 1.tar.bz2 ./1.*tar -cavf 1.tar.bz2 ./1.* #-a根据文件扩展名自动判断压缩格式tar -xjvf 1.tar.bz2tar -xavf 1.tar.bz2 -C /tmp#压缩比#从1级(速度最快，压缩率最低)到9级bzip -9 -k file#对成千上万的文件进行归档，需要借助 循环和-r选项 lzma压缩lzma是一个较新的压缩工具，它提供了比gzip或bzip2更好的压缩率。xz, unxz, xzcat, lzma, unlzma, lzcat - Compress or decompress .xz and .lzma files 1234567891011121314151617lzma file #file.lzma,同样也会删除原文件lzma file -k #保留原文件unlzma file.lzma#从stdin读入并写入stdoutcat file | lzma -C &gt; file.lzma#与tar相结合,--lzmatar -cvf 1.tar.lzma ./1.* --lzmatar -cavf 1.tat.lzma ./1.* #自动判断tar -xvf 1.tar.lzma --lzmatar -xavf 1.tar.lzma -C /tmp#压缩率#从1级到9级(压缩级别最高，速度最慢)#对成千上万的文件，需要使用循环和-r选项 zip归档和压缩zip在Linux下不如gzip,bzip2那么广泛，但在Internet上的文件通常都采用这种格式。zip - package and compress (archive) files 123456789101112131415zip file.zip fileunzip file.zip#与lzma,gzip,bzip2相比，zip完成后不会删除原文件#对目录和文件进行递归操作,-rzip -r dir.zip /root/test ./file#向归档文件中增加内容，-uzip dir.zip -u newfile#从压缩文件中删除内容，-dzip -d dir.zip file#列出归档文件中内容unzip -l dir.zip 超高压缩率的squashfs文件系统squashfs是一种只读型的超高压缩率文件系统。这种文件系统能够将 2GB-3GB的数据压缩成一个700MB的文件。你有没有想过Linux Live CD是怎样运行的？当Live CD启动后，它会加载一个完整的Linux环境。这就是利用了一种被称为squashfs的只读型压缩文件系统。它将根文件系统保存在一个压缩过的文件系统文件中。这个文件可以使用环回的形式来挂载并对其中的文件进行访问。一次当进程需要某些文件，可以将它们解压，然后载入内存中使用。如果需要构建一个定制的Live OS，或是需要超高压缩率的文件并且无需解压就可以访问文件，那么squashfs的相关知识就能派上用场。要解压个头较大的压缩文件，需要花费不少时间。但如果将文件以环回形式挂载，速度就飞快，因为只有出现访问请求的时候，对应的那部分压缩文件才会被解压缩。而普通的解压缩方式是首先解压缩所有的数据。 环回文件系统就是指那些在文件中而非物理设备中创建的文件系统。比如我们可以创建一个文件，然后把这个文件格式化为我们常见ntfs、exfat或者ext4等文件系统格式，然后把它挂载在一个目录上使用。 如果你有一张Ubuntu CD，可以在CDRom Root/casper/filesystem.squashfs中找到文件.squashfs。squashfs在内部采用了gzip和lzma这类压缩算法。 mksquashfs - tool to create and append to squashfs filesystems 123456789101112131415161718yum install squashfs-tools -y#创建squashfs文件mksquashfs source compressfile.squashfsmksquashfs /etc etc.squashfs#/etc(67M) --&gt; etc.suqashfs(18M)#要挂载squashfs文件，利用环回形式进行挂载mkdir /mnt/squashmount -o loop etc.squashfs /mnt/squash#此处挂载使用etc.squashfs文件系统#如果直接查看etc.squashfs，就是一个普通文件，但是挂载以后所有文件都出现了umount /mnt/squash#在创建squashfs文件时排除指定文件，-emksquashfs /etc etc.squashfs -e /etc/passwd /etc/shadow /etc/*.txt#在挂载之后就没有相关文件了 加密工具与散列加密技术主要用于防止数据遭受未经授权的访问。Linux下某些工具用于执行加密和解密，使用加密算法散列值来验证数据完整性。 crypt, gpg, base64, md5sum, sha1sum, openssl的用法 ccyptccrypt是为了取代UNIX crypt而设计的，这个实用工具可用于文件和数据流加密及解密。ccrypt - encrypt and decrypt files and streams 12345678ccrypt 1.txt #会要求输入口令(encryption key)#之后会生成1.txt.cpt覆盖原文件#更改key,-xccrypt -x 1.txt.cpt #输入old key和new key#解密，-d(--decrypt)ccrypt -d 1.txt.cpt #输入key解密 gpggpg(GNU privacy guard,GNU隐私保护)，是一种应用广泛的加密方案。它采用签名密钥技术保护文件内容，只有经过认证的用户才能访问数据。我们对gpg签名早已耳熟能详。gpg - OpenPGP encryption and signing tool1234#加密，-c(--symmetric)对称加密gpg -c file #会要求输入口令(Passphrase)，生成file.gpg#解密gpg file.gpg base64base64是一组类似的编码方案(encoding scheme)，它通过将ASCII字符转换成以64为基数的形式(radix-64 representation)来用ASCII字符串描述二进制数据。base64可用来对 编码和解码 base64字符串。base64 - base64 encode/decode data and print to standard output123456#将文件编码为base64格式base64 file &gt; outputfilecat file | base64 &gt; outputfile#解码,-dbase64 -d outputfile &gt; file md5sum与sha1summd5sum 和 sha1sum 都是单向散列算法(unidirecrional hash algorithm)，均无法逆推出原始数据。它们通常用于验证数据完整性或为特定数据生成唯一的密钥，因为通过分析文件内容，它们可以为每个文件生成一个唯一的密钥。 这种类型的散列算法是存储密码的理想方案。密码使用其对应的散列值来存储。如果某个用户需要认证，读取该用户提供的密码并转换成散列值，然后将其与之前存储的散列值进行比对。将密码以明文的形式存储是非常危险的事情，它面临密码泄露的危险。而因为 md5sum和sha1sum 是单向散列算法，所以密码使用散列值存储是很安全的。 12345678910echo "1.txt" &gt; 1.txtmd5sum 1.txt #生成密钥到stdout#39061daa34ca3de20df03a88c52530ea 1.txtsha1sum file #生成密钥到stdout#659fcbc505db207c03b5c4c0b6981d63286abe21 1.txt#查看/etc/shadow中密码的散列值awk 'NR==1' /etc/shadow | awk -F: '&#123;print $2&#125;' #root密码散列#$6$BxpV48gPsjuq6.pF$wE7pUDwtOI.v64kd5folG68yUt2UAQDTUGgKa5Iz69GaupEoRAdCeerP8nRKXo48c4azutUCGhnDgzd1qe8YX0 shadowlike散列(salted散列)shadow密码通常都是salted密码，所谓SALT就是额外的一个字符串，用来起一个混淆的作用，使加密更加不同里被破解。salt由一些随机位组成，被用作密钥生成函数的输入之一，以生成密码的salted散列值。 1234567891011#/etc/passwd里面的密码散列类型就是salted散列#查看root密码对应的散列值head -1 /etc/shadowroot:$6$ZlHRCZG2iRwQUXAu$RAEDH97nPdZB2RK20npua6Qf6jB7osatoC99ow3LtPQ6aORdLISYC7/4iTYU162emkQLt4ZafdgjyAeoSB7IU0::0:99999:7:::#openssl - OpenSSL command line tool#shadow密码是使用openssl生成#将SALT_STRING替换为随机字符串，同时将pass替换成你想测试的密码openssl -1 -salt SALT_STRING passwd 用rsync备份系统rsync借助差异计算以及压缩技术来最小化数据传输量。相较于cp命令，它的优势在于使用了高效的差异算法(difference algorithm)。它还支持网络数据传输。在进行复制的同时，rsync会比较源端和目的端的文件，只有当文件有更新是才进行复制。默认情况下，rsync并不会在目的端删除源端已不存在的文件。 rsync - a fast, versatile, remote (and local) file-copying toolinotifywait - wait for changes to files using inotify 1234567891011121314151617181920212223242526272829303132333435363738394041#-a进行归档，-v详细信息rsync -av source destinationrsync -av /etc /tmp#异地cprsync -av source username@host:PATHrsync -av username@host:PATH destination#rsync借助于ssh，可以使用ssh无秘钥认证rsync -av /etc zhang@192.168.1.11:~#-z, --compress compress file data during the transferrsync -avz zhang@192.168.1.11:/etc /tmp#注意，路径格式rsync /etc /tmp #整个/etc目录rsync /etc/ /tmp #/etc目录下所有内容#显示进度，--progressrsync -avz --progress /etc /tmp#排除部分文件，--excludersync -avz /etc /tmp --exclude=/etc/nginx --exclude "*.txt"#更新rsync时，删除不存在的文件，--delete#默认情况下，rsync并不会在目的端删除源端已不存在的文件rsync -avz /etc zhang@192.168.1.1:~ --delete#定期调度crontab -e0 */10 * * * rsync -avz /etc user@host:PATH#实时同步，inotifywait+rsyncyum install inotify-tools -y#-m(monitor),-r(recursive),-q(--quiet)静默模式，-e(event)vi inotify_rsync.shinotifywait -mrq -e creat,delete,modify,move --exclude "^.*\.filepart$" /etc | while read filedorsync -az --exclude=".*" --exclude="*.swp" --exclude=".filepart" --delete /etc /tmp &gt; /dev/null 2&gt;$1done 用Git备份版本控制维护和恢复变更最好的方法是使用版本控制系统。由于代码变更频繁，版本控制系统多用于软件开发和代码维护。Git(GNU it)是有名气也是最高效的版本控制系统。我们可在非编程环境下用Git备份普通文件。 git - the stupid content tracker 1234567891011121314151617181920212223242526272829303132333435363738394041424344mkdir /home/zhang/gittestcd /home/zhang/gittest#在源主机中添加用户信息git config --global user.name "username" #设置用户名git config --global user.email "someone@example.com" #设置邮箱#创建一个空的Git版本库或初始化一个老版本git init#记录变更到版本库git commit#添加远程git目录并同步备份git remote add origin user@host:/home/zhang/gittest#为git跟踪(git tracking)添加或删除文件#add,添加内容至索引git add *#git add *.txt; git add *.ph #添加部分文件#删除不需要跟踪的文件和文件夹#rm,从工作去和索引删除文件git rm file#git rm *.txt#检查点或创建备份点(check point)git commit -m "Commit Message"#push,更新远程git push#用Git恢复数据#log,显示提交日志git log#返回之前某个版本或状态git checkout xxxxxxxx(Commit ID)#clone,克隆一个版本库到本地git clone URLgit clone user@host:PATH 用dd克隆磁盘dd命令能用于克隆任何类型的磁盘，如硬盘、闪存、CD、DVD及软盘。可能需要创建所有分区的副本而不仅仅是复制内容，包括硬盘分区、引导记录、分区表等信息。 使用dd的时候，要留意参数的顺序。错误的参数会损毁全部数据。dd基本上算是一个比特流复制器(bitstream duplicator),它可以将来自磁盘的比特流写入文件，也可以将来自文件的比特流写入硬盘。 dd - convert and copy a file 12345678910dd if=source of=target bs=block_size count=count#bs块大小，count块数dd if=/tmp/centos7.iso of=/dev/sdc#/dev/zero是一个字符设备，它总是返回字符'\0'dd if=/dev/zero of=./file bs=10m count=100#用环回(loop back)方法可将任何由dd生产的文件镜像进行挂载mount -o loop file /mnt 无网不利j简介网络是计算机系统中重要的部分。我们以Tcp/Ip为协议栈，所有操作都是基于它进行的。 一些使用网络的应用通过打开并连接到防火墙端口进行运作，而有的管理任务可以通过网络进行。 ##网络小知识 网络接口(Interface)用来连接网络。在每个系统中，默认都有一个称之为环回接口的lo，这个接口指向当前主机本身。操作系统维护者一个被称为路由表(routing table)的表格，它包含了分组如何转发以及通过网络中的哪些节点转发的消息。metric是路由算法用以确定到达目的地的最佳路径的计量标准，如路径长度。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#显示网络接口、子网掩码等详细信息ifconfig #/sbin/ifconfig#显示某个特定接口ifconfig eth0#提取IP地址ifconfig eth0 | egrep -o "inet [^ ]*" | grep -o "[0-9.]*"#设置网络接口的IP地址和子网掩码ifconfig eht0 192.168.1.11ifconfig eth0 192.168.1.11 netmask 255.255.255.0#远程的时候，千万别乱改IP，不然连不上你就要去机房了#MAC地址欺骗ifoconfig eth0 hw ether 11:22:33:44:55:66#域名服务器与DNScat /etc/resolv.conf#添加域名服务器echo "name 114.114.114.114" &gt;&gt; /etc/resolv.conf#nameserver 114.114.114.114#一个域名可以分配多个地址，DNS只会返回其中一个#要想获得域名所有IP地址，需要使用DNS查找工具#DNS查找工具host www.baidu.comnslookup www.baidu.com#自定义解析cat /etc/hostsecho "192.168.1.11 www.zhang.me" &gt;&gt; /etc/hosts#设置默认网关，显示路由表信息#路由表routeroute -n #以数字形式显示地址#设置默认网关route add default gw $ip $interfaceroute add default gw 192.168.1.1 eht0#显示分组途经的所有网关地址traceroute www.baidu.com pingping使用 网际控制报文协议(Internet Control Message Protocol,ICMP)的echo分组。如果分组能够送达且该主机为活动主机，那它就会发送一条回应。一旦主机不可到达，ping返回错误信息”Destination Host Unreachable”。 123456ping 192.168.1.1#往返时间(Round Trip Time,RTT)#发送分组数量ping $URL -c 6 列出网络上所有活动主机当涉及大型局域网时，可能需要检查网络上的其他主机的活动状态。一台非活动主机可能是：没有开机；网络连接有问题；主机禁ping；防火墙问题。 当我们要检测ip时，在一个脚本中，每一次ping都是依次执行。即使所有的ip地址都是彼此独立，由于编写的是顺式程序(sequential program)，ping命令也只能按顺序执行。每次执行一个ping命令。都要经历一段延迟——“发送echo分组，并接收或等待回应超时”。 要是处理几百个ip地址的话，这个延时就真不短了。我们可以使用并行方式来加速所有ping命令的执行。可以将ping命令中的循环体放入( )&amp; 中，( ) 使其中的命令可作为子shell来执行，&amp; 使之在后台继续运行。 1234567891011121314151617181920#编写G一个并行方式的ping脚本fo ip in 192.168.1.&#123;1..255&#125;do ( ping $ip -c2 &amp;&gt; /dev/null; if[ $? -eq 0 ] then echo "$ip is alive" fi )&amp;waitdone#wait命令是脚本只有在所有子进程或后台进程全部终止或完成后才能结束#使用fping,-a显示活动主机，-g生成目标列表,-u显示无法到达主机fping -a 192.168.0.0/24 -g 2&gt; /dev/nullfping -a 192.168.0.1 192.168.3.255 -g 2&gt; ./unreach.txt#将unreach主机找出cat unreach.txt | egrep -o "to [0-9.]+$" | grep -o "[0-9.]*" 传输文件有很多不同的方法可以在网络节点上传输文件，常见的协议有FTP, SFTP, RSYNC, SCP。 通过FTP传输文件可使用lftp命令；通过SSH传输文件可使用sftp；RSYNC使用SSH与rsync命令；scp通过SSH进行传输。 文件传输协议(File Transfer Protocol, FTP)，使用21端口。FTP是明文传输，So…需要远程主机上启用了FTP服务器才能使用FTP。1234567lftp user@ftp-host#输入密码后便可以操作如下命令cd -- lcd(本地)mkdirget filename #下载文件put filename #上传文件quit #退出 SFTP(Secure FTP,安全FTP)，运行在SSH连接之上。利用SSH连接模拟FTP接口。它不需要源端运行FTP服务器，不要运行OpenSSH。SFTP是一个交互式命令，提供了命令提示符。 rsync广泛用于网络文件与系统快照的备份。 SCP(Secure Copy,安全复制)，远程文件复制工具。通过SSH加密通过进行传输。123456789scp SOURCE DESTINATIONscp /path/file user@host:PATHscp usr@host:/dir/file /home/zhang#需要输入密码，可以用SSH无秘钥认证#-r递归复制,-p保持文件权限和模式scp -r /etc user@host:/tmpscp -rp user@host:/var/www /var SSH无秘钥认证特别是在定时任务传输备份文件时，无秘钥认证就很方便了。SSH服务默认在22端口，你可以在配置文件中修改。 具体步骤： 创建SSH密钥(公钥和私钥)； 将客户端公钥上传给需要连接的主机，并写入~/.ssh/authorized_keys文件； 修改相关目录和文件权限； 123456ssh-keygen -t rsa#后续操作默认即可#生成~/.ssh/id_rsa.pub和id_rsa#写入远程主机ssh user@host "cat &gt;&gt; ~/.ssh/authorized_keys" &lt; ~/.ssh/id_rsa.pub 用SSH在远程主机上运行命令1234567891011121314#连接远程主机ssh user@host#非默认端口ssh user@host -p 2211#在远程主机中运行命令ssh user@host 'command'ssh user@host 'cmd1'; 'com2'...ssh user@host 'whoami'#-C压缩功能，当带宽有限时ssh -C user@host 'cmd' 在本地挂载远程驱动器在执行读写数据操作时，通过本地挂载远程主机文件系统。利用SSH和sshfs来实现这一功能。sshfs是FUSE文件系统的一个扩展，FUSE允许其支持的操作系统像使用本地文件系统一样挂载各类数据。sshfs允许将远程文件系统挂载到本地挂载点上。 相当于便捷的NFS，但并不需要搭建NFS服务。 SSHFS - filesystem client based on ssh1234#挂载远程文件到本地ssh user@host:PATH /mnt/sshfsumout /mnt/sshfs 网络流量和端口分析应用程序在主机上打开端口，然后与远程主机中打开的端口实现通信。出于安全方面的考虑，必须留意系统中打开及关闭的端口。 恶意软件和rootkit可能会利用特定的端口及服务运行在系统之中，从而进行攻击。通过分析开放端口列表以及运行在端口上的服务，我们便可以分析并检查恶意软件，保证主机安全。 了解及使用各种端口分析工具。 lsof - list open fileslsof列出系统中开放端口以及运行在端口上的服务的详细信息; netstat查看开放端口与服务netstat - 显示网络连接，路由表，接口状态，伪装连接，网络链路信息和组播成员组; iftop - display bandwidth usage on an interface by hostiftop - 展示带宽使用情况； ifstat - handy utility to read network interface statisticsifstat - 展示某时刻网络状态； nload - displays the current network usagenload - 可查看系统总带宽； nethogs - Net top tool grouping bandwidth per processnethogs- 可查看每个进程流量情况； ethtool - query or control network driver and hardware settingsethtool - 检查网卡支持的带宽 1234567891011121314151617#lsof的每一项都对应着一个打开了特定端口的服务lsof -i#查看开放端口和服务netstat -nltp#查看网络实时状态iftop#查看当前网络状态ifstat#查看系统带宽nload#查看进程流量nethogs 当个好管家简介操作系统(Operation System,OS)，是由一系列用于不同目的、服务于不同任务的系统软件组成。日志记录(logging)和监视是很重要的，能帮助我们从大量数据中收集信息。 监视系统活动的各种命令，日志技术及其使用方法。 统计磁盘使用情况磁盘空间是一种有限资源，我们需要了解磁盘的可用空间。 df, du, fdisk是Linux中的磁盘管理三板斧df(disk free): 报告文件系统磁盘空间的使用情况;du(disk usage): 报告磁盘空间使用情况; 使用du时，要确保对其遍历的目录和文件拥有适合的读权限。fdisk: Linux分区表操作工具软件。 12345678910111213141516171819202122232425262728293031323334353637383940du file1 #默认以字节为单位#-a,显示目录下所有文件大小du -a /home/zhangdu /home/zhang #只显示目录大小#-h,以可读形式打印du -h /home/zhang#-c,显示使用总量du -c file1 /dir2du -c *.txt *.sh#-s，打印摘要du -s /dirdu -sh /home/zhang#-b,-k,-m,-B，用特定单位打印du -k file1du -m file2#--exclude,从磁盘统计中排除部分文件du --exclude="*.swap" -sh /home/zhang#--max-depth,指定最大遍历深度du -h --max-depth n /dirdu -h --max-depth=2 /home/zhang#-x,将/mnt中所有挂载点排除在磁盘统计之外du -xh /dir#找出目录中最大的文件du -ak /dir | sort -nrk 1 | head -n 5#此输出包含了目录大小，需要细化#利用find替du过滤文件find /dir -type f --exec du -ak &#123;&#125; \; | sort -nrk 1 | head#df,磁盘可用空间信息df -h 计算命令执行时间当测试一个应用程序或比较不同的算法时，程序的执行时间非常重要。所以需要计算命令执行时间。 所有的Unix-Like操作系统都包含time命令，可将time放在需要计算执行时间的命令前。 time命令有个可执行二进制文件位于/usr/bin/time，还有一个shell built-in命令也叫作time；当运行time时，默认调用的是shell built-in命令。內建time命令选项有限；因此，如果我们需要使用另外的功能，就应该使用/usr/bin/time命令。 1234567891011121314151617181920212223#计算命令执行时间time commandtime ls#real,挂钟时间(wall clock time),命令从开始执行到结束的时间；#user,指进程花费在用户模式(user-mode)中的CPU时间。这是唯一用于执行进程所花费的时间；#sys，指进程花费在内核模式(in the kernel)中的CPU时间。它代表在内核中执行系统调用所使用的时间。#-o,将命令执行时间写入文件/usr/bin/time -o exetime.txt ls /#-a,不影响原文件/usr/bin/time -a -o exetime.txt ls /home#-f,格式化时间输出#时间格式字符串#real %e#user %U#sys %S/usr/bin/time -f "FORMAT STRING" command/usr/bin/time -f "Rtme: %e" -a -o timing.log uname/usr/bin/time -f "Rtime: %e\nUtime: %U\nStime: %S" -ao timing.log uname 当前登录用户、启动日志、启动故障的相关信息收集与操作系统、当前登录用户、主机运行时间、启动故障等相关信息很有用处。 1234567891011121314151617181920#获取当前登录用户who #显示已经登录的用户w #显示已经登录的用户以及他们在做什么#会显示用户使用的伪终端(pseudo TTY)，对应设备文件出现在/dev/pts/n#列出登录主机的用户列表users#查看系统运行时间uptime#显示用户登录列表last#获取某个用户登录信息last zhang#获取重启会话信息last reboot#获取失败的用户登录信息lastb 打印10条最常使用的命令终端是用来访问shell的工具，在shell中我们可以输入并执行命令。我们可以找出在shell中运行最多的命令。 ~/.bash_history，默认保留1000个最近执行命令。或者history命令。 1cat .bash_history | sort -n | uniq -c | sorn -nr | head 列出占用CPU最多的进程CPU时间是一项重要资源，有时需要跟踪占用CPU周期最多的进程。对于需要处理大量请求的服务器来说，CPU是极其重要的资源。通过监视某个时期内CPU的使用情况，可以找出长期占用CPU的进程并对其进行优化，或是调试其他问题。 用ps命令收集系统中进程的详细信息。ps - report a snapshot of the current processes 1234567891011121314151617181920212223242526272829303132333435363738394041424344#-e,以标准语法显示每个进程ps -eps -ef#ax,以BSD语法显示每个进程ps axpa axu#获取安全信息#ps -eo euser,ruser,suser,fuser,f,comm,pcpu,label#comm显示命令，pcpu显示CPU使用率ps -eo comm,pcpu#监视并计算一小时内CPU使用情况的shell脚本secs=3600unit_time=60steps=$(($secs / $unit_time))echo "Whatching CPU usage..."for((i=0; i&lt;steps; i++))do ps -eo comm,pcpu | tail -n +2 &gt;&gt; /tmp/cpu_usage.$$ sleep $unit_timedoneecho "CPU eaters: "cat /tmp/cpu_usage.$$ | \awk '&#123;process[$1]+=$2&#125;END&#123; for (i in process) &#123; printf("%-20s %s",i,process[i]); &#125;&#125;' | sort -nrk 2 | head#tail -n +K，从第K行开始输出。上面输出第一行是 COMAND 和 %CPU#$1,command; $2,%CPU#process[$1]是一个关联函数，相当于arr[command]#arr[command]=arr[command]+ $2，计算同一命令的累积时间#i指命令，process[i]指命令运行时间 用watch监视命令输出可能需要在在某段时期内以固定的间隔时间不短监视某个命令的输出。可利用watch命令。 watch - execute a program periodically, showing output fullscreen 123456789101112#watch命令可以用来在终端以固定的间隔监视命令输出，默认2秒间隔watch commandwatch 'command'watch lswatch 'ls -l'#-n,指定时间间隔watch -n 5 'yum update -y'#-d，突出(highlighting)watch输出中的差异watch -d -n 1'dd if=/dev/zero of=/tmp/zero.test' 对文件及目录访问进行记录记录重要文件及目录访问，对于追踪文件和目录的变化很有帮助。inotifywait命令可以用来收集有关文件访问的信息。inotifywait和rsync用户实时同步哦！ inotifywait - wait for changes to files using inotify 1234567891011yum install -y inotify-tools#-q,减少冗余信息inotifywait -m -r -q -e create,move,delete /dirinotifywait -m -r -q -e create,move,modify,delete /home/zhang &gt;&gt; inotifywait.log#利用inotifywait检测，rsync同步inotifywait -mrq -e create,move,modify,delete /dir --exclude="*.swap" | while read filedorsync -av --exclude="*.swqp" --delete /dir user@host:PATH &gt; /dev/null 2&gt;&amp;1done 用logrotate管理日志文件日志文件是Linux系统维护中必不可少的组成部分。日志文件可以帮助跟踪系统中多种服务所发生的事件，这有助于排除系统问题。但随着时间推移，日志文件会变得越来越大。因而必须对日志文件进行管理。 我们可以利用一种称为“轮询(rotation)”的技术来限制日志文件的体积。一旦日志文件超过了限定大小，就要对它的内容进行抽取(strip)，同时将日志文件的旧条目归档到文件中。 logratate是每一位Linux系统管理员都应该了解的命令。它能够将日志文件大大小限制在给定的SIZE内。logrotate配置文件位于/etc/logrotate.d logrotate ‐ rotates, compresses, and mails system logs 123456789101112vim /etc/logrotated.d/custom/var/log/custom.log &#123; missingok #日志文件丢失，则忽略 notifempty #仅当源日志文件非空时才进行轮替 size 30k #限制实施轮替的日志文件大小 compress #压缩旧日志 weekly #轮询时间，daily,weekly,yearly rotate 7 #保留旧日志数量 create 0600 root root #创建的日志文件模式，用户和用户组#还有一些其他选项&#125; 用sys记录日志在Linux系统中，在/var/log中创建并写入日志信息的是由被称为syslog的协议处理的。它由守护进程syslogd负责执行。每一个标准应用进程都可以用syslog记录日志信息。 syslog处理/var/log下的多个日志文件。但是当logger发送消息时，它用标记字符串来确定应该纪录到哪一个日志文件中。syslogd使用与日志相关联的TAG来决定应该将其记录到哪一个文件中。可以从/etc/rsyslog.d/目录的配置文件中看到与日志文件相关联的标记字符串。 Linux中一些重要日志文件： /var/log/boot.log， 系统启动信息；/var/log/message， 内核启动信息；/var/log/auth.log， 用户认证日志；/var/log/dmesg， 系统启动信息；/var/log/mail.log， 邮件服务器日志。 logger - a shell command interface to the syslog 123456#logger命令，默认记录日志信息到/var/log/messageslogger "test log message to messages"tail -n 1 /var/log/message#-t，指定特定TAGlogger -t TAG "test log message to messages" 管理重任简介GNU/Linux的生态系统是由运行的程序、服务、连接的设备、文件系统、用户等组成。按照我们需要的方式对整个系统有一个微观并对操作系统进行整体上的管理，这就是系统管理的主要目的。 收集进程信息进程是程序运行实例(runing instance)。同一程序的多个实例可以同时运行，但他们的进程ID却互不相同。 进程管理相关的重要命令是： top, display Linux processes; ps, report a snapshot of the current processes; pgrep, look up or signal processes based on name and other attributes. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#ps命令#-f, 显示更多进程信息ps -f#-e,every; -a,allps -efps -ax#-o, 指定想要的列ps -e -o parameter1,parameter2...ps -eo comm,pcpu,pmem#pccpu CPU占用率#pid 进程ID#ppid 父进程ID#pmem 内存使用率#comm 命令名#cmd 简单命令#user 启动进程的用户#nice 优先级#time 累积的CPU时间#etime 进程启动后度过的时间#tty 所关联的TTY设备#euid 有效用户ID#stat 进程状态#--sort,根据参数对ps输出进行排序#+升序，-降序ps -eo comm,pcpu,pmem --sort -pcpups -eo comm,pcpu,pmem --sort -pcpu,+pmem#-C, 给定命令全名找出PIDps -C cmd -o comm,pid#-u, 指定有效用户列表#-U, 指定真实用户列表ps -u root -U zhang -o user,pcpu#-t, 用TTY过滤输出ps -t TTY1,TTY2...ps -t pts/0,pts/1 -ef#-L, 显示进程相关信息#LWP线程ID， NLWP线程数量ps -efL#pgrep命令, 获得一个特定命令的PID列表#它只需要命令的一部分即可pgrep cmdpgre inotifpgrep bas#-d, 指定定界符pgrep rsync -d ":"#-u, 指定进程的用户pgrep -u root,zhang rsync#-c, 返回匹配的进程数量pgrep -c rsync#top命令top 杀死进程以及发送响应信息在Unix-Like环境中与进程有关的一个重要概念就是信号。信号是一种进程间通信机制，它用来中断运行的进程以执行某些操作。终止程序也是通过使用信号技术来实现的。 像ctrl+C,ctrl+Z这种作业都属于信号。 kill 命令可用来向进程发送信号; trap 命令用来处理所接收的信号; killall 以名字方式来杀死进程. 1234567891011121314151617181920212223#列出所有可用信号kill -l#-s, 发送信号#信号名称和信号数都可以kill -信号数 PIDkill -s SIGNAL PID#常用信号#SIGHUP 1 --对控制进程或终端进行挂起检测(hangup detection);#SIGINT 2 --当按下ctrl+c时发送该信号;#SIGKILL 9 --强行杀死进程;#SIGTERM 15 --终止进程;#SIGTSTP 20 --当按下crtl+z时发送该信号.#killall, 通过命令名终止进程killall -s SIGNAL PNamekillall -信号数 PName#trap, 捕捉并响应信号trap 'signal-handler-func' SIGNAL LIST which, whereis, file, whatis与平均负载which hows the full path of (shell) commands。找出某个命令的位置;whereis locate the binary, source, and manual page files for a command。不仅返回命令路径，还能打印命令手册的位置以及命令源代码路径;file determine file type。用来确定文件类型;whatis display manual page descriptions。输出简短描述信息;平均负载(load average),是系统运行总负载量的一个重要参数。它指明了系统中可运行进程总量的平均值。平均负载由三个值来指定，第一个指明1分钟内的平均值，第二个指明5分钟内的平均值，第三个指明15分钟内的平均值。 单核CPU，类似于单车道，负载在 0.00-1.00 之间正常； 多核CPU，类似于多车道，负载在 核数*(0.00-1.00) 之间正常； 安全的系统负载，单核应该在 0.7 以下； 12345#查看平均负载uptimecat /proc/loadavg#0.00 0.01 0.05 1/355 44955#分母355表示系统进程总数, 分子表示正在运行的进程数, 最后一个数字表示最近运行进程ID 向用户终端发送消息系统管理员可能需要向网络中所有主机上的所有用户或特定用户的终端发送消息。`wallrsync -av –exclude=”*.s命令用来向所有当前登录用户的终端写入消息。 在Linux系统中，终端是作为设备存在的。因此那些打开的终端在dev/pts/中都会与对应的设备节点文件。向特定设备写入数据将会在对应的终端显示出消息。 12345echo "It's just a test" | wall#查看用户对应的/dev/pts/, 并向某一个用户终端发送信息ll /dev/pts | awk '&#123;print $3,$6&#125;'echo"Haha" &gt; /dev/pts/[1,2,3...] 收集系统信息包括主机名、内核版本、Linux发行版本、CPU信息、内存信息、磁盘分区信息等。 123456789101112131415161718192021222324252627#主机名hostnameuname -n#内核版本，架构uname -runame -muname -a#Linux发行版本cat /etc/redhat-release#CPU相关信息lscpucat /proc/cpuinfocat /proc/cpuinfo | grep 'model name'#内存详细信息free -hcat /proc/meminfo#分区信息cat /proc/partitionsfdisk -l#系统详细信息lshw 用/proc收集信息在GNU/Linux操作系统中，/proc是一个位于内存中的伪文件系统(in-memory pseudo filesystem)。它的引用是为了提供一个可以从用户空间(user space)读取系统参数的接口。 可以对/proc中的文件和子目录进行cat来获取信息，所有内容都是易读的格式化文本。 /proc/下的数字目录，包含了对应进程的相关信息；/proc/environ，包含于进程相关联的环境变量；/proc/cwd，是一个到进程工作目录的符号链接；/proc/fbcat，包含了由进程所使用的文件描述符。 用cron进行调度GNU/Linux系统包含了各种用于调度任务的工具。cron就是其中之一，它通过守护进程crond使得任务能够以固定的时间间隔在系统后台自动运行。cron利用的是一个被称为“cron表(cron table)”的文件，这个文件中存储了需要执行的脚本或命令的调度列表以及执行时间。 123456789101112131415161718192021222324crontab -e#* * * * * cmd#分钟(0-59)#小时(0-23)#天(1-31)#月(1-12)#工作日(0-7)，0和7都代表周天#命令#*号,所有值#,号,范围。1,3,5,7,9#-号,连续范文。1-10#/号,*/10;0-8/20#栗子* 0-6 * * * /home/zhang/test.sh1,3,5,7,9 * * * * /home/zhang/test.sh*/5 * * * * /home/zhang/test.sh#-l,查看cron表crontab -l#-r,移除cron表crontab -r 用户管理常用命令12345678910111213141516171819202122232425262728#添加用户userall#删除用户userdel--remove-all-file删除与用户相关的所有文件#修改shellchsh#修改用户属性usermod#修改密码过期时间chage#修改密码passwd#登录到一个新组newgrp#添加、删除组groupaddgroupdel#指纹finger]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[歌词摘抄]]></title>
    <url>%2F2017%2F09%2F13%2Fsong%2F</url>
    <content type="text"><![CDATA[就像蝴蝶飞不过沧海，没有人会忍心责怪 这个城市一下雨就进入了秋天，]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>Songs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作与人生]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%B7%A5%E4%BD%9C%E4%B8%8E%E4%BA%BA%E7%94%9F%2F</url>
    <content type="text"><![CDATA[我现在已经活到了人生的中途，拿一日来比喻人的一生，现在正是中午。人在童年时从朦胧中醒来．需要一些时间来克服清晨的软弱，然后就要投入工作；在正午时分，他的精力最为充沛，但已隐隐感到疲惫；到了黄昏时节，就要总结一日的工作，准备沉入永恒的休息。按我这种说法，工作是人一生的主题。这个想法不是人人都能同意的。我知道在中国，农村的人把生儿育女看作是一生的主题。把儿女养大，自己就死掉，给他们空出地方来——这是很流行的想法。在城市里则另有一种想法，但不知是不是很流行：它把取得社会地位看作一生的主题。站在北京八宝山的骨灰墙前，可以体会到这种想法。我在那里看到一位已故的大叔墓上写着：副系主任、支部副书记、副教授、某某教研室副主任，等等。假如能把这些“副”字去掉个把，对这位大叔当然更好一些，但这些“副”字最能证明有这样一种想法。顺便说一句，我到美国的公墓里看过，发现他们的墓碑上只写两件事：一是生卒年月。二是某年至某年服兵役；这就是说，他们以为人的一生只有这两件事值得记述：这位上帝的子民曾经来到尘世，以及这位公民曾去为国尽忠，写别的都是多余的，我觉得这种想法比较质朴……恐怕在一份青年刊物上写这些墓前的景物是太过伤感，还是及早回到正题上来罢。 我想要把自己对人生的看法推荐给青年朋友们：人从工作中可以得到乐趣，这是一种巨大的好处。相比之下，从金钱、权力、生育子女方面可以得到的快乐，总要受到制约。举例来说，现在把生育作为生活的主题，首先是不合时宜；其次，人在生育力方面比兔子大为不如，更不要说和黄花鱼相比较；在这方面很难取得无穷无尽的成就。我对权力没有兴趣，对钱有一些兴趣，但也不愿为它去受罪——做我想做的事(这件事对我来说，就是写小说)，并且把它做好，这就是我的目标。我想，和我志趣相投的人总不会是一个都没有。 根据我的经验，人在年轻时，最头疼的一件事就是决定自己这一生要做什么。在这方面，我倒没有什么具体的建议：干什么都可以，但最好不要写小说，这是和我抢饭碗。当然，假如你执意要写，我也没理由反对。总而言之，干什么都是好的；但要干出个样子来，这才是人的价值和尊严所在。人在工作时，不单要用到手、腿和腰，还要用脑子和自己的心胸。我总觉得国人对这后一方面不够重视，这样就会把工作看成是受罪。失掉了快乐最主要的源泉，对生活的态度也会因之变得灰暗…… 人活在世上，不但有身体，还有头脑和心胸——对此请勿从解剖学上理解。人脑是怎样的一种东西，科学还不能说清楚。心胸是怎么回事就更难说清。对我自己来说，心胸是我在生活中想要达到的最低目标。某件事有悖于我的心胸，我就认为它不值得一做；某个人有悖于我的心胸，我就觉得他不值得一交；某种生活有悖于我的心胸，我就会以为它不值得一过。罗素先生曾言，对人来说，不加检点的生活，确实不值得一过。我同意他的意见：不加检点的生活，属于不能接受的生活之一种。人必须过他可以接受的生活，这恰恰是他改变一切的动力。人有了心胸，就可以用它来改变自己的生活。 中国人喜欢接受这样的想法：只要能活着就是好的，活成什么样子无所谓。从一些电影的名字就可以看出来：《活着》、《找乐》……我对这种想法是断然地不赞成。因为抱有这种想法的人就可能活成任何一种糟糕的样子，从而使生活本身失去意义。高尚、清洁、充满乐趣的生活是好的，人们很容易得到共识。卑下、肮脏、贫乏的生活是不好的，这也能得到共识。但只有这两条远远不够。我以写作为生，我知道某种文章好，也知道某种文章坏。仅知道这两条尚不足以开始写作。还有更加重要的一条，那就是：某种样子的文章对我来说不可取，绝不能让它从我笔下写出来，冠以我的名字登在报刊上。以小喻大，这也是我对生活的态度。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>王小波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[告别信]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%91%8A%E5%88%AB%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[1999年，72岁的马尔克斯患上淋巴癌后，写了一封信向读者告别。如果有一刹那，上帝忘记我是一只布偶并赋予我片刻生命，我可能不会说出我心中的一切所想，但我必定会思考我所说的一切。 我会评价事物，按其意义大小而非价值多少。 我会少睡觉，多思考。因为我知道，每当我们闭上一分钟眼睛，我们也就同时失去了60秒。当他人停滞时我会前行，当他人入梦时我会清醒，当他人讲话时我会倾听，就像享受一支美味的巧克力冰激凌！ 如果上帝赏我一段生命，我会简单装束，伏在阳光下，袒露的不仅是身体，还有我的魂灵。 上帝呀，如果我有一颗心，我会将仇恨写在冰上，然后期待太阳的升起；我会用凡高的梦在星星上画一首贝内德第的诗，而塞莱特的歌会是将是我献给月亮的小夜曲。我会用泪水浇灌玫瑰，以此体味花刺的痛苦和花瓣的亲吻…… 上帝呀，如果我有一段生命……我不会放过哪怕是一天，而不对我所爱的人说我爱他们。我会使每个男人和女人都了解他们皆我所爱，我要怀着爱而生活。 对于大人，我会向他们证明，那种认为因衰老而失去爱的想法是多么错误，我们是因为失去爱而衰老而不是与之相反。对于孩子，我会给他们插上翅膀而让他们自己学会飞翔；对于老人，我会教给他们死亡的来临不是因为衰老而是因为遗忘。 人呀，我从你们身上学会了太多的东西… …我知道，人们都想伫立在颠峰上，殊不知，真正的幸福恰恰就在于攀登险阻的过程。我懂得，当婴儿用小拳头第一次抓住爸爸的手指时，他也就永远地抓住了它。 我明白，一个人只有在帮助他人站起时才有权利俯视他。我能够从你们身上学到的东西是如此之多，可事实上已经意义寥寥，因为当人们将我敛入棺木时，我正在死去。 永远说你感到的，做你想到的吧！如果我知道今天是我最后一次看你入睡，我会热烈地拥抱你，祈求上帝守护你的灵魂。如果我知道这是最后一次看你离开家门，我会给你一个拥抱一个吻，然后重新叫住你，再度拥抱亲吻。如果我知道这是最后一次听到你的声音，我会录下你的每个字句，以便可以一遍又一遍永无穷尽地倾听。如果我知道这是看到你的最后几分钟，我会说”我爱你”，而不是傻傻地以为你早已知道。 永远有一个明天，生活给我们另一个机会将事情做好，可是如果我搞错了，今天就是我们所剩的全部，我会对你说我多么爱你，我永远不会忘记你。 明天从不向任何人作保证，无论青年或老人，今天可能就是你最后一次看到你所爱的人。因此，别再等待了，今天就开始！因为如果明天永远不来，你也许会遗憾今天没来得及微笑，拥抱，亲吻，会遗憾自己忙碌得只能把它们归为一个最后的愿望。保护周围你爱的人吧，告诉他们你多么需要他们。爱他们，善待他们，用些时间对他们说：”对不起”，”原谅我”，”劳驾”，”谢谢”，以及你知道的所有爱的话语。 没有人会因为你秘而不宣的思想而记住你。向上帝祈求力量和智慧来表达它们吧，向你的朋友证明，他们对你来说是多么的重要。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>马尔克斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2017%2F09%2F01%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[关于MarkdownMarkdown 是一种轻量级标记语言。它允许人们“使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档”。 参考文件Markdown-wikiMarkdown官网Markdown中文文档Markdown语法 Markdown语法首行缩进12345678#一个空格&amp;ensp;#两个空格&amp;emsp;#不断行空白格&amp;nbsp; 栗子： &ensp;一个空格； &emsp;两个空格； &nbsp;不断行空白格； 段落与换行 段落的前后必须是空行空行是指行内什么都没有，或者只有空白符（空格或制表符）相邻两行文本，如果中间没有空行，会显示在一行中（换行符被转换为空格） 如果需要在段内加入换行 可以在前一行的末尾加入至少两个空格，然后换行写其它的文字 Markdown中的多数区块都需要在两个空行之间 粗体和斜体语法：1234*斜体*, _斜体_**粗体*****粗斜体***~~删除线~~ 显示效果： 斜体, 斜体 粗体 粗斜体 删除线 分级标题Setext形式大标题：12345一级大标题========二级大标题-------- atx形式普通标题：12345# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题 超链接MarkDown支持两种形式的链接语法：行内式和参考式。 行内式语法说明：[ ] 里面写链接文字，( ) 里面写链接地址，()中的” “可以指定title属性。 代码：欢迎来到 [简书](www.jianshu.com &quot;Jianshu&quot;) 效果：欢迎来到 简书 参考式参考式超链接一般用在学术论文上面，或某一个链接在文章中多处使用，那么引用的方式创建链接将非常好，它可以让你对链接进行统一的管理。语法说明：123参考式链接分为两部分，文中的写法[链接文字][链接标记]，在文本任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格如果链接文字本身可以作为链接标记，也可以写成[链接文字][][链接文字]：链接地址的形式 代码：123456简书里面有 [简书早报][1]、[简书晚报][2]以及 [简黛玉][3][简黛玉 美人][3] 是一个[才女][][1]:http://www.jianshu.com &quot;Jianshu&quot;[2]:http://www.jianshu.com &quot;EveningPaper&quot;[3]:http://www.jianshu.com[才女]:http://www.jianshu.com 效果：简书里面有 简书早报、简书晚报以及简黛玉简黛玉 美人 是一个才女 自动链接MarkDown支持以比较简短的自动链接形式来处理网址和电子邮件，只要用&lt;&gt;包起来，MarkDown就会自动把它转成链接。 代码：12&lt;http://example.com&gt;&lt;address@example.com&gt; 锚点MarkDown Extra只支持在标题后插入锚点，其他地方无效Leanote编辑器右侧显示效果区域暂时不支持锚点跳转，所以点来点去发现没有跳转不必惊慌，你发布后的文章是支持的在你准备跳转到的指定标题后插入锚点 { # 标记 }，然后在其他地方写上连接到锚点的链接 代码：12目录 \&#123; \# index \&#125;跳转到 \[ 目录\ ]( \# index) 效果：目录 { #index }跳转到 [目录 ](#index) 列表无序列表使用 * ，+ ，- 表示无序列表 代码：123- 无序列表1- 无序列表2- 无序列表3 效果： 无序列表1 无序列表2 无序列表3 有序列表有序列表使用数字接着英文点 代码：1231. 有序列表12. 有序列表23. 有序列表3 效果： 有序列表1 有序列表2 有序列表3 定义型列表定义型列表由名词和解释组成。一行写上定义，紧跟一行写上解释。解释的写法：紧跟一个缩进（Tab） 列表缩进列表项目标记通常是放在最左边，但是其实也可以缩进，最多3个空格，项目标记后则一定要接着至少一个空格或制表符。 代码：123* 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。* 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 效果： 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 引用引用需要在被引用的文本前加上 &gt; 符号 代码：12&gt; 引用1&gt; 引用2 效果： 引用1引用2 引用的多层嵌套区块引用可以嵌套（如引用的引用），只要根据层次加上不同数量的 &gt;符号 代码：123&gt;&gt;&gt; 请问MarkDown怎么用？&gt;&gt; 自己看教程！&gt; 教程在哪里？ 效果： 请问MarkDown怎么用？ 自己看教程！ 教程在哪里？ 插入图像图片的创建方式与超链接类似。 代码：![](http://zhangxx5678.lofter.com/post/39b969_df4f526#) 内容目录在段落中填写 [TOC] 以显示全文内容结构目录 注脚在需要添加注脚的文字后加上注脚名字 [^注脚名字]，称为加注。然后在文中的任意位置（一般最后）添加脚注，脚注前必须有对应的脚注名字。注脚与注脚间必须空一行！注脚自动被搬运到最后面，请到文章末尾查看，并且脚注后的链接可以直接跳转会到加注的地方 代码：123使用 MarkDown[^1]可以提高书写效率，直接转换成 HTML[^2][^1]:MarkDown是一种纯文本标记语言[^2]:HTML超文本标记语言 效果： 使用 MarkDown^1可以提高书写效率，直接转换成 HTML^2 分割线可以在一行中用 三个以上的 *,-,_ 建立一个分割线，行内不能有其他东西。 代码：12345671. * * *2.3. ***4.5. - - -6.7. --- 效果： 扩展语法Markdown标准 本身所包含的功能有限，所以产生了许多第三方扩展语法，如 GFW, GitHub Flavored Markdown Tasklist代码：12345- [ ] Monday- [ ] Tuesday- [ ] Wednesday- [ ] Tuesday- [ ] Friday 效果： Monday Tuesday Wednesday Tuesday Friday 表格 不管是哪种方式，第一行为表头，第二行为分割表头和主体部分，第三行开始每一行为一个表格行； 列与列之间用管道符号 | 隔开； 第二行还可以为不同的列指定对其方向，默认左对齐，在 - 右边加上 : 就右对齐 代码：12345学号 | 姓名 | 分数- | - | -001 | 张三 | 78002 | 李四 | 67003 | 王五 | 99 学号 姓名 分数 001 张三 78 002 李四 67 003 王五 99 代码块和高亮代码块插入代码的方式有两种，一种是利用缩进(Tab)，另一种是利用反引号 `` 和 ``` ``` 代码：1Python语言的输出函数 `Print()` 怎么使用？ 效果：Python语言的输出函数 Print() 怎么使用？ 123import osfrom flask import Flaskapp = Flask(app) 高亮在 ``` 之后添加代码的语言 代码：```pythonimport osfrom flask import Flaskapp = Flask(app)``` 效果：123import osfrom flask import Flaskapp = Flask(app) 流程图 流程图语法参考 LaTeX公式关于LaTEX： 是一种跨平台的基于TEX的排版系统，对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学、化学类文档。 关于MathJax： MathJax是一种跨浏览器JavaScript库，它使用MathML，LaTeX和ASCIIMathML 标记在Web浏览器中显示数学符号。MathJax作为Apache License下的开源软件。 MathJax语法 语法$表示行内公式质能守恒公式 $E=mc^2$ 方程式 效果：$E=mc^2$ $$表示整行公式 代码：12345$$\sum_&#123;i=1&#125;^n a_i=0$$$$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$$$\sum^&#123;j-1&#125;_&#123;k=0&#125;&#123;\widehat&#123;\gamma&#125;_&#123;kj&#125; z_k&#125;$$ 效果：$$\sum_{i=1}^n a_i=0$$ $$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$ $$\sum^{j-1}{k=0}{\widehat{\gamma}{kj} z_k}$$ Markdown编辑器介绍一些常用的书写、编辑Markdown的工具。 MarkdownPad Windows (windows); Texts (Windows, osX); MarkPad (Windows); Haroopad (Windows, osX, Linux); ReText (Linux); 等等 格式转换Markdown文档可以方便地转换为 HTML, Word, PDF 等文件格式。可以利用 软件 或者 命令 转换文件。 转换为 HTML转换为 PDF转换为 Word]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Nginx]]></title>
    <url>%2F2017%2F09%2F01%2F%E5%85%B3%E4%BA%8ENginx%2F</url>
    <content type="text"><![CDATA[关于Nginx Nginx官网 Nginx-Wikipedia Nginx（发音同engine x）是一个 Web服务器，也可以用作反向代理，负载平衡器和 HTTP缓存。它能反向代理 HTTP, HTTPS, SMTP, POP3, IMAP 的协议连接。基于BSD-like协议发行，支持多种操作系统。作为HTTP服务软件的后起之秀，Nginx有很多优点： 在性能上，Nginx占用的系统资源更少，支持更多的并发连接（特别是小静态文件场景下），达到更高的访问效率； 在功能上，Nginx不仅是一个优秀的Web服务软件，还可以作为反向代理 负载均衡及缓存使用。它类似于LVS负载均衡及HAProxy等专业代理软件，又类似于Squid等专业缓存服务软件； 在安装配置上，Nginx方便、简单、灵活。 Nginx功能丰富，可作为HTTP服务器、反向代理服务器、邮件服务器。支持FastCGI, SSL, Virtual Host, URL Rewrite, Gzip等功能，并支持很多第三方模块扩展。 与PHP的集成自PHP-5.3.3起，PHP-FPM加入到了PHP核心，编译时加上–enable-fpm即可提供支持。PHP-FPM以守护进程在后台运行，Nginx响应请求后，自行处理静态请求，PHP请求则经过fastcgi_pass交由PHP-FPM处理，处理完毕后返回。Nginx和PHP-FPM的组合，是一种稳定、高效的PHP运行方式，效率要比传统的Apache和mod_php高出不少。 Nginx的重要特性 可针对静态资源高速高并发访问及缓存；可使用反向代理加速，并且可进行数据缓存；具有简单负载均衡、节点健康检查和容错功能；支持远程FastCGI、Uwsgi、SCGI、Memcached Servers的加速和缓存；支持SSL、TLS、SNI；具有模块化的架构：过滤器包括gzip压缩、ranges支持、chunked响应、XSLT、SSI及图像缩放等功能。在SSI过滤器中，一个包含多个SSI的页面，如果FastCGI或反向代理处理，可被并行处理；它具备的其他WWW服务特性：支持基于名字、端口及IP的多虚拟主机站点；支持Keep-alived和pipelined连接；可进行修改Nginx配置，并且在代码上线时，可平滑重启，不中断业务访问；可自定义访问日志格式，临时缓冲些日志操作，快速日志轮询及通过rsyslog处理日志；可利用信号控制Nginx进程；支持 3xx-5xx HTTP状态码重定向；支持rewrite模块，支持URI重写及正则表达式匹配；支持基于客户端IP地址和HTTP基本认证的访问控制；支持PUT、DELETE、MKCOL、COPY及MOVE等较特殊的HTTP请求方法；支持FLV流和MP4流技术产品应用；支持HTTP响应速率限制；支持同一IP地址的并发连接或请求数连接；支持邮件服务器代理； Nginx安装 添加RPM源安装: 1234yum install -y gcc gcc-c++ make libtool zlib zlib-devel openssl openssl-devel pcre pcre-devel 安装依赖rpm -ivm http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm 安装RPM源yum install -y nginx 安装Nginxrpm -q nginx 查询安装 添加Nginx yum repository休息安装： 123456vim /etc/yum.repos.d/nginx.repo[nginx] 唯一name=nginx repobaseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/gpgcheck=0enabled=1 源码安装: 1234567cd /usr/local/src 建议解压于此目录wget http://xxx.xx.com/nginx.tar.gztar -zxvf nginx.tar.gzcd ./nginx./configure --prefix=/usr/localmakemake install]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test-My-Site]]></title>
    <url>%2F2017%2F08%2F30%2FTest-My-Site%2F</url>
    <content type="text"><![CDATA[Monday Tuesday Wednesday Thursday Friday Saturday Sunday $$\sideset{^1_2}{^3_4}A$$ $E=mc^2$ $$\sum_{i=1}^n a_i=0$$ $$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$ $$\sum^{j-1}{k=0}{\widehat{\gamma}{kj} z_k}$$]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>Testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
