<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Windows常用命令]]></title>
    <url>%2F2018%2F02%2F11%2FWindows%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Windows快捷操作 打开Windows系统应用 运行 Win+R 远程桌面 mstsc (Microsoft terminal services client) 记事本 notepad 计算器 calc 屏幕键盘 osk 设备管理器 devmgmt.msc 事件查看器 eventvwr.msc 任务管理器 taskmgr 服务列表 services.msc 注册表 regedit 共享文件夹 fsmgmt.msc 系统信息 msinfo32 系统配置 msconfig DirecX dxdiag 清屏 cls Windows快捷键 Windows批处理命令Windows下并不区分大小写。 batch or batch file or DOS batch.是由DOS或Windows内嵌命令解释器(cmd.exe)解释执行。类似于Unix中的shell script。 在批处理中，不仅可以使用系统內建的命令，还可使用已安装的第三方提供的软件。 一般情况下，每条命令占一行，当然也可以将多条命令用特定符号&amp; &amp;&amp; | ||分隔后写到同一行。 系统在解释运行批处理程序时，首先扫描整个批处理程序，然后从第一行代码开始详细逐句执行所有命令，直至程序结尾或遇见 exit 命令或出错意外退出。 基础批处理命令123456789101112131415161718192021222324252627282930#查看命令帮助cmd /?#打开或关闭回显echo#注释命令rem#暂停命令pause#从一个批处理程序调用另一个批处理程序，并且不终止父批处理程序call#调用外部程序，所有的dos命令都可以由start命令来调用start#跳转命令goto#显示、设置或删除变量set 批处理常用符号12345678910111213141516171819202122232425262728293031#回显屏蔽@#输出重定向&gt; &gt;&gt;#输入重定向&lt;#管道符号|#转义符^#逻辑命令符&amp; &amp;&amp; ||#无条件执行&amp;后面的命令&amp;#当&amp;&amp;前面的命令成功后才执行&amp;&amp;后面的命令，否则不执行&amp;&amp;#当 || 前面的命令失败时，才执行 || 后面的命令，否则不执行|| 基础dos命令文件操作系统123vollable 文件夹管理12345678910111213cdmd/mkdirrd/rmdirdirtreepathxcopy 文件管理1234567891011121314151617typecopydel/erasemoveren/renamereplaceattribfindfc 网络相关12345678910111213141516171819pingftpnetnetstatroutetracerttelnetipconfigmsgarp 系统管理123456789101112131415atshutdowntskilltaskkilltasklistscregpowercfg 其它命令12345clsassocftype 批处理命令使用 回显控制123456789101112131415161718192021222324252627#关闭单行回显echo#从下一行开始关闭回显echo off#从本行开始关闭回显，一般批处理第一行都是这个@echo off#从下一行开始打开回显echo on#显示当前是 echo on还是 echo offecho#输出一个“回车换行”echo.#输出 hello worldecho hello world#关闭回显是指运行批处理文件时，不显示文件里的每条命令，只显示运行结果批处理开始和结束时，系统都会自动打开 errorlevel程序返回码类似于Unix下的$? 12345#查看返回码的值echo %errorlevel%#执行成功默认为0#一般命令执行出错为1 显示目录和子目录1234567891011121314151617181920212223242526272829303132dir#列出所有文件和目录，包括隐藏文件dir /a#列出D盘dir d:#列出D盘中的目录dir d: /A:d#/A：属性/A 显示具有指定属性的文件#属性D 目录 R 只读文件H 隐藏文件 A 准备存档的文件S 系统文件 I 无内容索引文件L 解析点 - 表示“否”的前缀#显示文件内容type d:\hello.txt 参数%0 %1 %2 … %*, 命令行参数类似于shell %0: 批处理文件本身 %1: 第一个参数 %*: 所有参数 if和for123456789if &quot;%1&quot; == &quot;/a&quot; echo 第一个参数/aif /i &quot;%1&quot; equ &quot;/a&quot;if exist d:\hell.txt ( echo 存在)else ( echo 不存在) 12for %%i in (c: d: e: f:) doecho %%i set设置变量引用变量可在变量前加%，即 %变量名% 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#显示目前所有可用的变量，包括系统变量和自定义的变量set#显示系统盘盘符。系统变量可以直接引用echo %SystemDrive%#显示所有以p开头的变量，要是一个也没有就设errorlevel=1set p#设置变量p，并赋值为 = 后面的字符串，即aa1bb1aa2bb2set p=aa1bb1aa2bb2#显示变量p代表的字符串，即aa1bb1aa2bb2echo %p%#显示变量p中第6个字符以后的所有字符，即aa2bb2echo %p:~6%#显示第6个字符以后的3个字符，即aa2echo %p:~6,3%#显示前3个字符，即aa1echo %p:~0,3%#显示最后面的2个字符，即b2echo %p:~-2%#显示除了最后2个字符以外的其它字符，即aa1bb1aa2becho %p:~0,-2% #用c替换变量p中所有的aa，即显示c1bb1c2bb2echo %p:aa=c%#将变量p中的所有aa字符串置换为空，即显示1bb12bb2echo %p:aa=% #第一个bb及其之前的所有字符被替换为c，即显示c1aa2bb2echo %p:*bb=c%#设置变量p，赋值为 %p:*bb=c% ，即c1aa2bb2set p=%p:*bb=c%#设置p为数值型变量，值为39set /a p=39#支持运算符，有小数时用去尾法，39/10=3.9，去尾得3，p=3set /a p=39/10#用 /a 参数时，在 = 后面的变量可以不加%直接引用set /a p=p/10#”与”运算，要加引号。其它支持的运算符参见set/?set /a p=”1&amp;0″#取消p变量set p=set /p p=请输入 一些动态变量1234567891011121314151617181920212223242526272829#系统盘符%SystemDrive%#代表当前目录的字符串%CD%#当前日期%DATE%#当前时间%TIME%#随机整数，介于0~32767%RANDOM%#当前ERRORLEVEL值%ERRORLEVEL%#当前命令处理器扩展名版本号%CMDEXTVERSION%#调用命令处理器的原始命令行%CMDCMDLINE% 其它命令12345678910#批处理中调用外部程序的命令，否则等外部程序完成后才继续执行剩下的指令start#批处理中调用另一个批处理的命令，否则剩下的批处理指令将不会被执行call#让用户输入一个字符，从而选择运行不同的命令，返回码errorlevel为1234...choice]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式]]></title>
    <url>%2F2018%2F02%2F08%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[参考： 《鸟哥的Linux私房菜》 正则表达式： https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F 正则表达式介绍正则表达式，又称正规表示式、正规表示法、正规表达式、规则表达式、常规表示法(Regular Expression, 在代码中常简写为regex、regexp或RE）。是计算机科学的一个概念。正则表达式使用单个字符串来描述、匹配一系列匹配某个句法规则的字符串。在很多文本编辑器里，正则表达式通常被用来检索、替换那些匹配某个模式的文本。 正则表达式的POSIX规范，分为两大流派： 基本型正则表达式（Basic Regular Expression，BRE） grep、vi、sed都属于BRE，是历史最早的正则表达式，因此元字符必须转译之后才具有特殊含义 扩展型正则表达式（Extended Regular Express，ERE） egrep、awk则属于ERE，元字符不用转译 正则表达式基本语法一个正则表达式通常被称为一个模式（pattern），用来描述或者匹配一系列匹配某个句法规则的字符串。 大部分正则表达式有如下结构： 选择 |竖线符代表选择(或)，具有最低优先级 数量限定 字符后的数量限定符用来限定前面这个字符允许出现的个数 不加数量限定则代表仅出现一次 常见的数量限定符包括 +、?、* +加号代表前面的字符必须至少出现一次 ( $$$&gt;=1$$$ ) ?问号代表前面的字符最多只可出现一次 ( $$$1&gt;=?&gt;=0$$$ ) *星号代表前面的字符可不出现，也可出现一次或多次 ($$$&gt;=0$$$) 匹配 ()圆括号可以定义操作符的范围和优先度 PCRE表达式全集正则表达式有多种不同的风格。PCRE（Perl兼容正则表达式，Perl Compatible Regular Expression）。适用于Perl或者Python编程语言（grep或者egrep的正则表达式文法是PCRE的子集） 基础正则表达式 字符 描述 \ 转义字符 zhang 匹配文本字符串值zhang . 匹配除\r,\n之外的任何单个字符 竖线l 匹配竖线两边某一个 ^ 匹配输入字符串的开始位置 $ 匹配输入字符串的结束位置 * 匹配前面的子表达式零次或多次 + 匹配前面的子表达式一次或多次 ? 匹配前面的子表达式零次或一次 {n} n是一个非负整数。匹配n次 {n,} n是一个非负整数。至少匹配n次 {n,m} m和n均为非负整数，匹配n-m次 [xyz] 字符集合（character class）。匹配所包含的任意一个字符 [^xyz] 排除型字符集合（negated character classes）。匹配未列出的任意字符 [a-z] 字符范围。匹配指定范围内的任意字符 [^a-z] 排除型的字符范围。匹配任何不在指定范围内的任意字符 [:name:] 增加命名字符类（named character class） [=elt=] 增加当前locale下排序（collate）等价于字符“elt”的元素 [.elt.] 增加排序元素（collation element）elt到表达式中。这是因为某些排序元素由多个字符组成 元字符元字符(meta character)，是一种Perl风格的正则表达式，只有一部分文本处理工具支持它。 字符 描述 \b 匹配一个单词边界，也就是指单词和空格间的位置 \B 匹配非单词边界。“er\B”能匹配“verb”中的“er”，但不能匹配“never”中的“er” \cx 匹配由x指明的控制字符 \d 匹配一个数字字符。等价于[0-9]。注意Unicode正则表达式会匹配全角数字字符 \D 匹配一个非数字字符。等价于[^0-9] \f 匹配一个换页符。等价于\x0c和\cL \n 匹配一个换行符。等价于\x0a和\cJ \r 匹配一个回车符。等价于\x0d和\cM \s 匹配任何空白字符，包括空格、制表符、换页符等等 \S 匹配任何非空白字符。等价于[^ \f\n\r\t\v] \t 匹配一个制表符。等价于\x09和\cI \v 匹配一个垂直制表符。等价于\x0b和\cK \w 匹配包括下划线的任何单词字符。等价于“[A-Za-z0-9_]”。注意Unicode正则表达式会匹配中文字符 \W 匹配任何非单词字符。等价于“[^A-Za-z0-9_]” \ck 匹配控制转义字符。k代表一个字符。等价于“Ctrl-k”。用于ECMA语法 \xnn 十六进制转义字符序列。匹配两个十六进制数字nn表示的字符 \num 向后引用（back-reference）一个子字符串（substring），该子字符串与正则表达式的第num个用括号围起来的捕捉群（capture group）子表达式（subexpression）匹配。其中num是从1开始的十进制正整数，其上限可能是9[注 2]、31、[注 3]99甚至无限。[注 4]例如：“(.)\1”匹配两个连续的相同字符 \n 标识一个八进制转义值或一个向后引用。如果\n之前至少n个获取的子表达式，则n为向后引用。否则，如果n为八进制数字（0-7），则n为一个八进制转义值 \nm 3位八进制数字，标识一个八进制转义值或一个向后引用。如果\nm之前至少有nm个获得子表达式，则nm为向后引用。如果\nm之前至少有n个获取，则n为一个后跟文字m的向后引用。如果前面的条件都不满足，若n和m均为八进制数字（0-7），则\nm将匹配八进制转义值nm \nml 如果n为八进制数字（0-3），且m和l均为八进制数字（0-7），则匹配八进制转义值nml \un Unicode转义字符序列。其中n是一个用四个十六进制数字表示的Unicode字符 扩展正则表达式 字符 描述 ? 非贪心量化（Non-greedy quantifiers）：当该字符紧跟在任何一个其他重复修饰符（*,+,?，{n}，{n,}，{n,m}）后面时，匹配模式是非贪婪的。非贪婪模式尽可能少的匹配所搜索的字符串，而默认的贪婪模式则尽可能多的匹配所搜索的字符串 (pattern) 匹配pattern并获取这一匹配的子字符串。该子字符串用于向后引用。所获取的匹配可以从产生的Matches集合得到，在VBScript中使用SubMatches集合，在JScript中则使用$0…$9属性。要匹配圆括号字符，请使用“(”或“)” (?:pattern) 匹配pattern但不获取匹配的子字符串（shy groups)，也就是说这是一个非获取匹配，不存储匹配的子字符串用于向后引用 (?=pattern) 正向肯定预查（look ahead positive assert），在任何匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用 (?!pattern) 正向否定预查(negative assert)，在任何不匹配pattern的字符串开始处匹配查找字符串。这是一个非获取匹配，也就是说，该匹配不需要获取供以后使用 (?&lt;=pattern) 反向(look behind)肯定预查，与正向肯定预查类似，只是方向相反 (?&lt;!pattern) 反向否定预查，与正向否定预查类似，只是方向相反 POSIX字符组POSIX字符类(POSIX character class),是一个形如[:…:]的特殊元序列，它用于匹配特定的字符范围。 POSIX字符组 说明 ASCII环境 Unicode环境 [:alnum:] 字母字符和数字字符 [a-zA-Z0-9] [\p{L&amp;}\p{Nd}] [:alpha:] 字母 [a-zA-Z] \p{L&amp;} [:ascii:] ASCII字符 [\x00-\x7F] \p{InBasicLatin} [:blank:] 空格字符和制表符 [ \t] [\p{Zs}\t] [:cntrl:] 控制字符 [\x00-\x1F\x7F] \p{Cc} [:digit:] 数字字符 [0-9] \p{Nd} [:graph:] 空白字符之外的字符 [\x21-\x7E] [^\p{Z}\p{C}] [:lower:] 小写字母字符 [a-z] \p{Ll} [:print:] 类似[:graph:]，但包括空白字符 [\x20-\x7E] \P{C} [:punct:] 标点符号 }~-] [\p{P}\p{S}] [:space:] 空白字符 [ \t\r\n\v\f] [\p{Z}\t\r\n\v\f] [:upper:] 大写字母字符 [A-Z] \p{Lu} [:word:] 字母字符 [A-Za-z0-9_] [\p{L}\p{N}\p{Pc}] [:xdigit:] 十六进制字符 [A-Fa-f0-9] [A-Fa-f0-9] 优先级 优先权 符号 最高 \ 高 ( )、(?: )、(?= )、[ ] 中 *、+、?、{n}、{n,}、{m,n} 低 ^、$、中介字符 次最低 串接，即相邻字符连接在一起 最低 l]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>RegularExpression</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis]]></title>
    <url>%2F2018%2F02%2F05%2FRedis%2F</url>
    <content type="text"><![CDATA[参考: 《Redis官方文档》: http://www.redis.cn/documentation.html 《Redis命令大全》: http://www.redis.cn/commands.html 环境: CentOS7x86_64 Redis 3.2 Redis简介 Redis是什么Redis是一个使用ANSI C编写的开源、支持网络、基于内存、可选持久性的、非关系型,键值对存储数据库。Redis是一个开源(BSD许可)的,内存中的数据结构存储系统,它可以用作数据库、缓存和消息中间件。 毫无疑问,Redis开创了一种新的数据存储思路,使用Redis,我们不用在面对功能单调的数据库时,把精力放在如何把大象放进冰箱这样的问题上,而是利用Redis灵活多变的数据结构和数据操作,为不同的大象构建不同的冰箱。希望你喜欢这个比喻。 Remote Dictionary Server(Redis)是由一个Salvatore Sanfilippo写的key-value储存系统。Redis提供了一些丰富的数据结构,包括lists,sets,ordered sets,hashes,当然还有和Memcached一样的string结构,所以常被称为是一款数据结构服务器(data structure server)。Redis当然还包括了对这些数据结构的丰富操作。 你可以在这些类型上面运行原子操作,例如,追加字符串,增加哈希中的值,加入一个元素到列表,计算集合的交集、并集和差集,或者是从有序集合中获取最高排名的元素。 Redis的优点为了满足性能,Redis采用内存(in-memory)数据集(dataset)。根据你的使用场景,你可以通过每隔一段时间转储数据集到磁盘,或者追加每条命令到日志来持久化。持久化也可以被禁用,如果你只是需要一个功能丰富,网络化的内存缓存。 性能极高,Redis能支持超过100K+每秒的读写频率 丰富的数据类型,Redis支持二进制案例的Strings,Lists,Hashes,Sets及Ordered Sets数据类型操作 原子,Redis的所有操作都是原子性的,同时Redis还支持对几个操作全并后的原子性执行 丰富的特性,Redis还支持publish/sucscribe,通知,key过期等特性 Redis还支持主从异步复制,非常快的非阻塞初次同步、网络断开时自动重连局部重同步 Redis安装直接通过yum安装: 1yum install -y redis 启动redis-server的两种方式: redis-server: standalone模式 systemctl redis start: daemon模式 需要在配置文件中开启daemonize 启动redis-cli: 12redis-cliredis-cli -a passwd Redis配置redis配置文件(/etc/redis.conf)常用参数: 参数 说明 daemonize 以守护进程启动,放置于后台 bind 监听地址,建议只对本地127.0.0.1开放 protect-mode redis的保护模式 requirepass 设置密码 timeout 超时 tcp-keepalive 在Linux上,指定值(秒)用于发送ACKs的时间,关闭连接需要双倍的时间,默认为0 loglevle 指定日志记录的级别。有四个级别:debug(记录很多信息,用于开发测试)、notice(常用于生产环境)、warning(严重的信息)、verbose(有用的信息) logfile 日志文件,默认为stdout databases 可用数据库,范围在0-(database-1) save 保存数据到磁盘(.rdb) stop-writes-on-bgsave-error 后台储存错误停止写 rdbcompression 储存到本地数据库时(持久化到rdb文件)是否压缩 dbfilename 本地持久化数据库文件名,默认dump.rdb dir 数据库文件路径,是目录 salveof 设置从库 masterauth 设置主库认证的密码 slave-read-only 设置slave是否只读 slave-serve-stale-data 从库同主库失去连接或复制正在进行时,从库是否继续响应客户端请求 repl-disable-tcp-nodelay tcp-nodelay slave-priority slave优先级,master不能工作后,从众多slave中选出优先值最小的slave提升为master,优先值为0表示不能为master appendonly 是否开启AOF数据备份,redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件,当此文件很大 appendsync AOF文件同步策略,后台会进行大量I/O no-appendfsync-on-rewrite - auto-aof-rewrite-percentage aof自动重写 auto-aof-rewrite-min-size 指定最小大小用于aof重写 slowlog-log-slower-than 慢日志,记录超过特定执行时间的命令,不包括I/o slowlog-max-len 慢日志记录的长度,超过大小,最先进入队列的记录会被踢出 hash-max-zipmap-entries hash将以一种特殊的编码方式(大大减少内存使用)来储存,这是其中一个临界值 hash-max-zipmap-value 另一个临界值 list-max-ziplist-entries 多个list以特定的方式编码来节省空间 activerehashing Redis将在每100ms时使用1ms的CPU时间来对redis的hash表进行重新hash,可降低内存的使用 hz 不是所有任务都以相同的频率执行,但redis按照指定的“hz”值执行检查任务 aof-rewrite-incremental-fsync 当一个子节点重写AOF文件时,则文件每生产32m数据进行同步 官方文档对VM的使用建议: 当KEY很小而VALUE很大时,使用VM的效果会比较好,因为这样节约内存比较大 当key不小时,可以考虑使用一些非常方法将很大的key变成value,比如将key,value组合成一个新的value Redis数据类型Redis不仅仅是简单的key-value存储器,同时也是一种data structure server。传统的key-value是指支持使用一个key字符串来索引value字符串的储存。而Redis中,value不仅仅支持字符串,还支持更多的复杂结构,包括列表、集合、哈希表等。Redis采用二进制安全,这就意味着你可以使用任何二进制序列作为重点。 字符串(strings)字符串 是一种最基本的Redis值类型。Redis字符串是二进制安全的,这意味着一个Redis字符串能包含任意类型的数据。 只关心二进制化的字符串,不关心具体格式。只会严格的按照二进制的数据存取。不会妄图已某种特殊格式解析数据。 列表(lists)Redis列表是简单的字符串列表,按照插入顺序序列,你可以添加一个或多个元素到列表的头部或者尾部。 散列(hash)Redis Hashes是字符串字段和字符串值之间的映射,因此他们是展现对象的完美数据类型。如一个有姓、名、年龄等属性的用户。一个带有一些字段的hash仅仅需要一块很小的空间储存,因此你可以储存数以百万计的对象在一个小的Redis实例中。 哈希主要用来表现对象,他们有能力储存很多对象,因此你可以将哈希用于许多其他的任务。 无序集合(unorder set)Redis集合(Set)是一个无序的字符串集合。可以用O(1)的时间复杂度(无论集合中有多少元素时间复杂度都是常量)完成添加、删除、测试元素是否存在。 Redis集合拥有令人满意的不允许包含相同成员的属性。多次添加相同的元素,最终在集合里只会有一个元素。实际上就是添加元素时无序检测元素是否存在。 一个Redis集合有趣的事情是它支持一些服务端的命令从现有的集合出发去进行集合运算,因此你可以在非常短的时间内进行合并(unions)、交集(intersections)、找出不同的元素(difference of sets)。 有序集合(order set)Redis有序集合与普通集合非常相似,也是一个没有重复项的字符串集合。不同之处是有序集合的每一个成员都关联了一个评分,这个评分被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的,但是评分可以是重复了。 使用有序集合可以以非常快的速度(O(log(N)))添加,删除和更新元素。可以很快根据评分(score)或者次序(position)来获取一个范围的元素。访问有序集合的中间元素也是很快的,因此能够使用有序集合作为一个没有重复成员的智能列表。在有序集合中,你可以很快捷的访问一切你需要的东西。 简而言之,使用有序的集合你可以做完许多对性能有极端要求的任务,而那些任务使用其他类型的数据库真的是很难完成。 Redis命令 常用命令123456789101112131415161718192021exists key #判断一个key是否存在del key #删除某个或一系列keytype key #返回某个key元素的数据类型,key不存在返回空keys key-pattern #返回匹配的key列表randomkey #随机获取一个已经存在的keyrename oldname newname #改key的名字,如果存在将会覆盖dbsize #返回当前数据库的key的总和expire key time #设置某个key的过期时间(秒),到期后自动删除ttl #查询key剩余存活时间flushdb #清空当前数据库中的所有键flushall #清空所有数据库中的键 设置相关12345config get #用来读取Redis服务器的配置参数config set #用于更改运行Redis服务器的配置参数config resetstat #重置数据统计报告,通常返回OK 连接操作12345quit #关闭连接auth #密码认证help command #帮助 持久化1234567save #将数据同步保存到磁盘bgsave #将数据异步保存到磁盘lastsave #返回上次成功将数据保存到磁盘的Unix时戳 远程服务1234567891011121314151617181920212223242526272829info #服务器信息统计,基本所有信息monitor #实时转储收到的请求slaveof #改变复制策略shutdown #将数据同步保存到磁盘,然后关闭服务server #Redis server的常规信息clients #Client的连接选项memory #存储占用相关信息persistence #RDB and AOF 相关信息stats #常规统计replication #Master/slave请求信息cpu #CPU占用信息统计cluster #Redis 集群信息keyspace #数据库信息统计all #返回所有信息default #返回常规设置信息 值(value)操作12345678910111213141516171819202122232425exists key #判断一个key是否存在del key #删除一个keytype key #返回值的类型keys pattern #返回满足给定模式的所有keyrandomkey #随机返回key空间的一个rename oldname newname #改key的名字,如果存在将会覆盖dbsize #返回当前数据库中key的数目expire #设定一个key的活动时间(s)ttl #获得一个key的活动时间select index #按索引查询move key dbindex #移动当前数据库中的key到dbindex数据库flushdb #删除当前选择的数据库中的所有keyflushall #删除所有数据库中的所有key 字符串(string)操作123456789101112131415161718192021222324252627set key value #给数据库中名称为key的string赋值valueget key #返回数据库中名为key的string的valuegetset key value #给名称为key的string赋予上一次的valuemget key1 key2 ... key N #返回库中多个string的valuesetnx key value #添加string 名称为key 值为valuesetex key time value #向库中添加string 设定过期时间timemset key 1 value 1 ... key N value N #批量设置多个string的值msetnx key 1 value 1 ... key N value N #如果所有名称为 key N的string都不存在 则向库中添加string 名称为 key N赋值value Nincr key #名称为key的string加 1 操作incrby key integer #名称为key的string增减integerdecr key #名称为key的string减1操作decrby key integer #名称为key的string的值附加valueappend key value #名称为key的值附加valuesubstr key start end #返回名称为key的string的value的子串 列表(list)操作12345678910111213141516171819rpush key value #在名称为key的list尾部添加一个值为value的元素lpush key value #在名称为key的list首部添加一个值为value的元素llen key #返回名称为key的list的长度lrange key start end #返回名称为key的list中start至end之间的元素 下表从0开始ltrim key start end #截取名称为key的list 保留start至end之间的元素lindex key index #返回名称为key的list中index位置的元素lset key index value #给名称为key的list中index位置的元素赋值valuelrem key count value #删除count个名称为key的list中值为value的元素brpop key1 key2 ... keyN #rpop的block版本rpoplpush srckey dstkey #返回并删除名为srckey的list尾元素 并将该元素添加到名为dstkey的list的头部 集合(set)操作123456789101112131415161718192021222324252627sadd key member #向名为key的set中添加元素membersrem key member #删除名为key的set中元素的memberspop key #随机返回并删除名为key的set中的一个元素smove srckey dstkey member #将member元素从名为srckey的集合移动到名为dstkey的集合scard key #返回名为key的set的基数sismember key member #测试member是否是名称为key的set的集合sinter key1 key2 ... key N #求交集sinterstore dstkey key1 ... key N #求交集并将交集保存到dstkey的集合sunion key1 ... key N #求并集sunionstore dstkey key 1 ... key N #求并集并将并集保存到dstkey的集合sdiff key1 ... key N #求差集sdiffstore dstkey key 1 ... key N #求差集并将差集保存到dstkey的集合smembers key #返回名为key的set的所有元素srandmember key #随机返回名为key的set的一个元素 有序集合(sorted set)操作12345678910111213141516zadd key score member #向名为key的zset中添加元素member score用于排序 如果该元素已经存在 则根据score更新该元素的顺序zrem key member #删除名为key的zset中的元素memberzincrby key increment member #如果在名为key的zset中已经存在元素member 则该元素的score增加increment 否则向集合中添加该元素 其score的值为incrementzrank key member #返回名为key的zset 顺序zrevrank key member #返回名为key的zset 倒序zrange key start end #返回名为key的zset score顺序按index从start到end返回所有元素zrevrange key start end #返回名为key的zset score倒序按index从start到end返回所有元素zrangebyscore key min max #返回名为key的zset中score大于等于min 小于等于max的所有元 hash操作123456789101112131415161718192021hset key field value #向名为key的hash中添加元素filed----valuehget key field #返回名为key的hash中field对应的valuehmset key field1 value1 ... field N value N #向名为key的hash中添加元素field----valuehmget key field1 ... field N #返回名为key的hash中filed对应的valuehincrby key field integer #将名为key的hash中field的value增加integerhexists key field #名为key的hash中是否存在键为field的域hdel key field #删除名为key的hash中键为field的域hlen key #返回名为key的hash中元素个数hkeys key #返回名为key的hash中所有键hvals key #返回名为key的hash中所有键对应的valuehgetall key #返回名为key的hash中所有的键 field 及其对应的value Redis高级应用Redis高级应用包括安全性设置、主从复制、事务处理、持久化机制和虚拟内存的使用。 安全性由于redis速度相当快，一秒钟可以150K次密码尝试，所以需要设置一个密码强度很强大的密码。 设置密码的两种方法： configg set requirepass &quot;passwd&quot;，通过命令设置密码 直接在配置文件中requirepass属性后加上密码 认证登录的两种方式： redis-cli -a passwd redi-cli –&gt; auth passwd 主从复制Redis的主从复制的配置和使用都比较简单。 master server slave server Redis主从复制特点： 一主多从 当master宕机后，优先级值小的那台slave server自动转变为master 主从复制不同阻塞master，在同步数据时master可以继续处理client的请求 提高了系统的可伸缩性 Redis主从复制过程： slave与master建立连接，发送sync同步命令 master会启动一个后台进程，将数据库快照保存到文件中，同时master主进程会开始收集新的写命令并缓存 后台完成保存后，就将此文件发送给slave slave将文件保存在磁盘上 主从复制栗子Redis主从配置，一主多从。注意：由于redis吃内存，可能会由于内存过小而无法正常启动redis，可查看/var/log/message。 配置master： 123456789101112131415161718vim /etc/redis_master.confdaemon yesbind 127.0.0.1 ip1port 6379requirepass fuza_mimaprotect-mode yesdatebases 100logfile /var/log/redis/redis_master.logdir /var/lib/redis_mastermkdir /var/lib/redis_masterchown redis:redis /var/lib/redis_mastersystemctl start redis 配置slave： 123456789101112131415161718192021222324252627282930vim /etc/redis_slave.confdaemon yesbind 127.0.0.1port 6379protect-mode yeslogfile /var/log/redis/redis_slave.logdir /var/lib/redis_slaveslaveof &lt;master-ip&gt; &lt;master-port&gt;masterauth &lt;master-passwd&gt;slave-read-only yesslave-priority 100#master挂掉后，从slave中选出优先级最小的作为master······#其他具体主从参数自己配置mkdir /var/lib/redis_slavechown redis:redis /var/lib/redis_slavesystemctl start redis 测试master： 1234redis-cli -a xxxset name zhangget zhang 测试slave： 123456redis-cliauth(&apos;passwd&apos;)key *get zhang 注意： 由于Redis只是主从，并不像MongoDB的主从和集群功能。当Redis主挂掉以后，是不可能进行故障转移的。所以也就无法写入，即从库无法更新数据。这点也可以从Redis从的配置文件中看出，连接到Redis主的IP：PORT，并通过主的密码来认证。 事务处理Redis的事务处理比较简单。只能保证client发起的事务中的命令可以连续的执行，而且不会插入其他的client命令。 当一个client在连接中发出multi命令时，这个连接就进入一个事务的上下文，该连接后续的命令不会执行，而是存放在一个队列中，当执行exec命令时，redis会顺序的执行队列中的所有命令。如果其中执行出现错误，执行正确的不会回滚，不同于关系型数据库的事务。 持久化机制持久化就是把数据从内存保存到硬盘。 Redis是一个支持持久化的内存数据库，Redis需要经常将内存中的数据同步到磁盘来保证持久化。 Redis支持两种持久化方式： snapshotting(快照) 将数据存放到文件里，默认方式。默认写入dump.rdb二进制文件中 可配置redis在n秒内超过m个key被修改就自动做快照 save 500 10 –&gt; 500s内超过10个key被修改，则保存快照 由于快照方式在一定间隔时间做一次保存， 如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。AOF比快照方式有更好的持久化性，是由于使用aof时，redis会将每一个收到的写命令都通过write函数写入到文件中当redis启动时会通过重新执行文件中保存的写命令在内存中重新建立整个数据库的内容。 appendonly file(AOF) aof方式redis会将每一次的函数都追加到文件中，当redis重启时会重新执行文件中保存的命令 配置文件参数： 1234567891011#启用aof持久化方式appendonly yes#每秒写入磁盘一次，在性能和持久化方面做了很好的折中appendonly everysc#将数据写入磁盘save 900 1save 300 10save 60 10000 虚拟内存Redis的虚拟内存是暂时把不经常访问的数据从内存交换到磁盘中，从而腾出内存空间用于其它的访问数据。对于redis这样的内存数据库，内存总是不够用的。 在配置文件(/etc/redis.conf)中配置VM: 123456789101112131415161718#开启vm功能vm-enableyes#交换出来的value保存的文件路径vm-swap-file /tmp/redis.swap#redis使用的最大内存上线vm-max-memory 10000000#每个页面的大小32字节vm-page-size 32#最多使用多少个页面vm-pages 123217729#用于执行value对象换入的工作线程数量vm-max-threads 4 注意 Redis监听地址bind： x.x.x.x，强烈建议只对本地127.0.0.1开放。不建议对外网开放，有安全隐患 防火墙，最简单就是关闭防火墙，另一个就是开放redis的监听端口 开启守护进程，让redis可以在后台运行而不必通过redis-server的方式来启动，将配置文件里的deamonize no改为yes 关闭redis的保护模式(protect-mode)，这里的保护模式是指是否允许其他IP的设备访问redis。如果开启的话就只能允许本机访问。如果是生产开发的实际运行环境，请一定开启保护模式 设置redis数据库密码！不仅仅是redis，任何数据库都应该设置密码，否则对外网开放的数据库就成了活靶子。]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机集群]]></title>
    <url>%2F2018%2F02%2F03%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[参考： 《老男孩Linux运维》 《服务器集群系统各概念》: https://segmentfault.com/a/1190000009923581 《WEB的负载均衡、集群、高可用解决方案》： https://zhuanlan.zhihu.com/p/23826048 《计算机集群》： https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%9B%86%E7%BE%A4 计算机集群计算机集群简称集群(Clusters)，是一种计算机系统。它通过一组散列集成的软件或硬件 连接起来高度紧密地协作完成计算工作。在某种意义上，他们可以被看做是一台计算机。 集群就是指一组（若干）相互独立的计算机，利用高速通信网络组成的一个较大的计算机服务系统，每个集群结点都是运行各自服务的独立服务器。这些服务器之间可以彼此通信，协同向用户提供应用程序、系统资源和数据，并以单一系统的模式加以管理。 当客户机请求集群系统时，集群给用户的感觉就是一个单一独立的服务器，而实际上用户请求的是一组集群服务器。 集群系统中的单个计算机通常称为节点，通常通过内网连接，但也有其它的可能连接方式。集群计算机通常用来改进单个计算机的计算速度和可靠性。 服务器集群概念集群、冗余、负载均衡、主从复制、读写分离、分布式、分布式计算、分布式计算平台、并行计算…… 实际生产环境中常有的问题： 当数据库性能遇到问题时，是否能够横向扩展，通过添加服务器的方式达到更高的吞吐量，从而充分利用现有的硬件实现更好的投资回报率; 是否拥有实时同步的副本，当数据库面临灾难时，可以短时间内通过故障转移的方式保证数据库的可用性。此外，当数据丢失或损坏时，能否通过所谓的实时副本（热备）实现数据的零损失; 数据库的横向扩展是都对应用程序透明，如果数据库的横向扩展需要应用程序端进行大量修改，则所带来的后果不仅仅是高昂的开发成本，同时也会带来很多潜在和非潜在的风险. 集群和冗余集群和冗余并不对立，多台服务器做集群（不是主从），本身就有冗余和负载均衡的效果。狭义上来说，集群就是把多台服务器虚拟成一台服务器，而冗余的每台服务器都是独立的。 集群的侧重点在于协同，多台服务器系统分担工作，提升效率； 冗余的侧重点在于防止单点故障，一主多备的架构，也就是主从复制； 数据冗余==高可用性==主从 主从一定程度上起到了负载均衡的作用，但主要目的还是为了保证数据冗余和高可用性 主从只提供一种成本较低的数据备份方案加上不完美的灾难和负载均衡，由于复制存在时间差，不能同步读，所以只是不完善的负载均衡和有损灾备 主从显然达不到集群的严格度，不论是 HA 还是 AA（多活并行集群），主从都达不到数据一致性的集群要求 为什么要使用集群 高性能（Performance） 大型网站谷歌、淘宝、百度等，都不是几台大型机可以构建的，都是上万台服务器组成的高性能集群，分布于不同的地点。 只有当并发或总请求数量超过单台服务器的承受能力时，服务器集群的优势才会体现出来。 价格有效性（Cost-effectiveness） 在达到同样性能的需求下，采用计算机集群架构比采用同等运算能力的大型计算机具有更高的性价比。 可伸缩性（Scalability） 当服务负载、压力增长时，针对集群系统进行较简单的扩展即可满足需求，且不会降低服务质量。 高可用（Availability） 单一计算机发生故障时，就无法正常提供服务；而集群架构技术可以是得系统在若干硬件设备发生故障时仍可以继续工作。 集群系统在提高系统可靠性的同时，也大大减小了系统故障带来的业务损失，目前几乎100%的网站都要求7x24h提供服务。 透明性（Transparency） 多个独立计算机组成的耦合集群系统构成一个虚拟服务器。用户访问集群系统时，就像访问一台高性能、高可用的服务器一样，集群中一部分服务器的上线、下线不会中断整个系统服务，这对用户也是透明的。 可管理性（Manageability） 这个系统可能在物理上很大，但其实很容易管理，就像管理一个单一映像系统一样。 可编程性（Programmability） 在集群系统上，容易开发及修改各类应用程序。 集群分类集群分为同构和异构，他们区别在于 “组成集群系统的计算机之间的体系结构是否相同”。 集群计算机按功能和结构可以分为以下几类： 均衡集群（Load balancing clusters） 用性集群（High-availability clusters） 能计算集群（High-performance cluster） 计算集群（Grid computing） 负载均衡集群（LB）和高可用性集群（HA）是互联网行业常用的集群架构模式 负载均衡集群负载均衡集群用于抗并发。 负载均衡集群典型的开源软件包括：LVS、Nginx、Haproxy 等。 负载均衡集群可以把很多客户集中的访问请求负载压力尽可能平均分摊在计算机集群中处理。集群中每个节点都可以一定的访问请求负载压力，并且可以实现访问请求在各节点之间动态分配，以实现负载均衡。负载均衡集群运行时，一般是通过一个或多个前端负载均衡器（Director）将客户访问请求分发到后端的一组服务器上，从而达到整个系统的高性能和高可用性。一般高可用性集群和负载均衡集群会使用类似的技术，或同时具有高可用性与负载均衡的特点。 Linux虚拟服务器（LVS）项目 在Linux操作系统上提供最常用的负载均衡软件。 负载均衡的作用： 用户访问请求及数据流量（负载均衡） 业务连续性，即7x24h服务（高可用） 于Web业务及数据库从库等服务器的业务 高可用性集群高可用性集群用于避免单点故障。 高可用性集群常用开源软件包括：Keepalived、Heartbeat 等。 一般是指集群中任意一个节点失效的情况下，该节点上的所有任务会自动转移到其他正常的节点上。此过程不会影响整个集群的运行。 当集群中的一个节点系统发生故障时，运行着的集群服务器会迅速做出反应，将该系统的服务分配到集群中其他正在工作的系统上运行。考虑到计算机硬件和软件的容错性，高可用性集群的主要目的是使局群的整体服务尽可能可用。如果高可用集群中的主节点发生了故障，那么这段时间内将由备节点代替它。备节点通常是主节点的镜像。当它代替主节点时，它可以完全接管主节点（包括Ip和其他资源）提供服务，因此，使集群系统环境对系统环境来说是一致的，既不会影响用户的访问。 高可用性集群使服务器系统的运行速度和响应速度会尽可能的快。它们经常利用在多台机器上运行的冗余节点和服务来相互跟踪。如果某个节点失败，它的替补者将在几秒钟或更多时间内接管它的职责。因此，对于用户来说，集群里的任意一台机器宕机，业务都不会受影响。 高可用性集群的作用： 当一台机器宕机后，另外一台机器接管宕机的机器的Ip资源和服务资源，提供服务； 常用于不易实现负载均衡的应用，如负载均衡器、主数据库、主存储对之间； 高性能计算集群高性能计算集群也称并行计算。通常，高性能计算集群涉及为集群开发的并行应用程序，以解决复杂的科学问题。 高性能计算集群对外就好像一个超级计算机，这种超级计算机内部由数万个独立服务器组成，并且在公共消息传递层上进行通信以运行并行应用程序。 高可用与负载均衡有什么区别 HA偏重于备用资源，切机时会有业务的断开的，保证了数据的安全，但造成资源的浪费； LB侧重于资源的充分应用，没有主备的概念，只有资源的最大限度的加权平均应用，基本不会业务的中断； HA的目的是不中断服务，LB的目的是为了提高接入能力。虽然经常放一起用，但确实是两个不同的领域； HA在一条路不通的时候提供另一条路可走，而 LB 就类似于是春运时的多个窗口； 集群软硬件 企业运维中常见集群产品： 开源集群软件：+ Nginx, LVS, Haproxy, Keepalived, Heartbear... 商业集群硬件：+ F5， Netscaler,Radware, A10... 如何选择开源集群软件： 网站在并发访问和总访问量不是很大的情况下，建议首选Nginx负载均衡，Nginx配置简单使用方便安全稳定。 另一个实现负载均衡的产品为Haproxy 如果要考虑Nginx负载均衡的高可用功能，建议首选Keepalived软件，因为安装配置简单方便稳定。类似高可用软件还有Heartbeat，但比较复杂 如果是大型企业，负载均衡可以使用 LVS+Keepalived 在前端做四层转发，后端使用Nginx或Haproxy做七层转发，再后面是应用服务器。如果是数据库与存储的负载均衡和高可用，可选用LVS+Heartbeat 负载均衡所谓负载均衡，就是把大访问量分发给不同的服务器，也就是分流请求。 HTTP重定向协议实现负载均衡HTTP 重定向就是应用层的请求转发，用户的请求其实已经到了HTTP重定向负载均衡服务器，服务器根据算法要求用户重定向，用户收到重定向请求后，再次请求真正的集群. 优点：简单 缺点：性能较差 DNS域名解析负载均衡DNS域名解析负载均衡就是在用户请求DNS服务器，获取域名对应的IP地址时，DNS服务器直接给出负载均衡后的服务器IP。 优点：交给DNS，不用我们去维护负载均衡服务器 缺点：当一个应用服务器挂了，不能及时通知DNS，而且DNS负载均衡的控制权在域名服务商那里，网站无法做更多的改善和更强大的管理 反向代理负载均衡在用户的请求到达方向代理服务器时（已到达网站机房），由于反向代理服务器根据算法转发到具体的服务器，常用的Apache，Nginx都可以充当反向代理服务器。 优点：部署简单 缺点：代理服务器可能成为性能的瓶颈，特别是一次上传大文件 IP负载均衡(LVS-NAT)LVS集群中实现的三种IP负载均衡技术。 在请求到达负载均衡器后，负载均衡器通过修改请求的目的IP地址，从而实现请求的转发，做到负载均衡。 优点：性能更好 缺点：负载均衡器的带宽称为瓶颈 直接路由负载均衡(LVS-DR)数据链路层负载均衡，在请求到达负载均衡器后，负载均衡器通过修改请求的Mac地址，从而做到负载均衡，与IP负载均衡不一样的是，当请求访问完服务器之后，直接返回客户，而无需在经过负载均衡器。 IP隧道负载均衡(LVS-TUN) 主从复制主从是一种用于数据容错和灾备的高可用解决方案，而不是一种处理高并发压力的解决方案（负载均衡是用来抗并发的）。 如MySQL主从复制，MongoDB主从复制(副本集) 主机负责查询，从机负责增删改 可以在从机上执行备份，以避免备份期间影响主机的服务 主从复制后，也可以在从机上查询，以降低主机的访问压力。但是，只有更新不频繁的数据或者对实时性要求不高的数据可以通过从服务器查询，实时性要求高的数据仍需在主服务器查询（因为主从复制有同步延迟，所以不能保证强数据一致性） 主从复制和读写分离 主从复制是实现读写分离的技术之一，也是实现读写分离的前提条件 做读写分离时最重要的就是确保 读库 和 写库 的数据统一，而主从复制是实现数据统一最简单的方法（并不能够保证强数据的一致性） 读写分离，顾名思义，就是一个表只负责向前台页面展示数据，而后台管理人员对表的增删改在另一个表中，把两个表分开，就是读写分离 主从复制则是一个表数据 增删改 之后会及时更新到另一个表中，保证两个表的数据一致 主从类型 双机热备=主机+备机 主要应用运行在主机，备机即备用机器。备机不工作，主机出现故障时备机接管主机的所有工作 双机互备=主机（备机） + 备机（主机） 互为主备，部分应用运行于主机，部分应用运行于备机，主机备机同时工作 双机双工=主机+主机 两台主机同时运行应用，主机备机同时工作 分布式 广义上的分布式是指，将不同的服务分布在不同的服务器上 集群是指，将几台服务器集中在一起，实现同一业务 分布式中的每一个节点都可以做集群，而集群并不一定是分布式的]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bash特殊符号]]></title>
    <url>%2F2018%2F01%2F25%2FBash%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[在编写shellscripts的时候，特殊符号也有其重要的功能。 具体描述如下： 符号 描述 栗子 #! shellban，申明脚本所使用的shell #!/bin/bash \ 转义字符 \n l 管道 stdout l grep &gt;,&gt;&gt; 输出重定向 &gt;1.txt &lt;,&lt;&lt; 输入重定向 &lt;1.txt 2&gt; 错误输出 2&gt;error.txt ; 连续命令分隔符 cmd1; cmd2 &amp;&amp; 与，只有当前命令完成后才执行后一个命令 cmd1 &amp;&amp; cmd2 ll 或，或此或彼 cmd1 ll cmd2 ~ 用户家目录 cd ~ # 注释符 #it’s a comments $ 取用变量前导符 \$PATH或${PATH} &amp; 工作控制，将命令放入后台(bg) command&amp; *, ?, [], [-], [^] 通配符 .sh，?.sh，[a-z].txt ! 逻辑运算’非’not != =,两边无空格 赋值符号 name=zhang =,两边有空格 比较符号 if [ $name = zhang ] \$0 代表脚本自身 zhang.sh \$1，\$2 第1个/2个…变量 ./zhang.sh start… $# 命令后面参数个数 if [ $# -ne 2 ]；then echo “Usage: \$0 arg1 arg2” $@ 代表\$1,\$2,\$3…之意，每个变量是独立的 xx $* 代\$1 \$2 \$3 …”之意 xxx \$? 命令状态码，成功为0 \$? \$\$ 当前shell的PID echo \$\$ ‘单引号’ 单引号内特殊字符仅为一般字符 echo ‘\$host’–$host “双引号” 双引号内特殊符号，可保有原本特性 echo “\$host” –localhost `反引号` 运行命令，也可用()代替 `date`或 \$(date) () 以子shell方式执行 ($(date)) {} 命令区块的组合 xxx PS1 命令提示符 $PS1 PS2 第二行以后的提示字符 $PS2 shift 移动参数 当使用shift后，\$1=\$2;再次shift后，\$1=\$3 set 查看所有变量 set unset 取消变量 unset name，没有$符号 export 使某变量成为环境变量 export name，没有$符号 source source命令通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录 source file]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不成熟的小想法]]></title>
    <url>%2F2018%2F01%2F21%2F%E4%B8%8D%E6%88%90%E7%86%9F%E7%9A%84%E5%B0%8F%E6%83%B3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[你在大学奋斗四年和你进入工作岗位后奋斗四年，这两者的质量是完全不同的。前者是一个人一生的黄金时代，他有绝对的选择权来决定自己要成为什么样子；而后者则不同，吃饱饭才是这些已经步入社会的人需要考虑的第一要务。 我想不明白，为什么非要把成都建设成为2/3个四川？(或许西部省份都是这样，省会便是这个省)2017年成都市GDP约为13800亿人民币，而第二名的绵阳，却连2000亿都不到。 我喜欢苹果，你却给了我一车梨，然后告诉全世界你花光了所有的钱给我买了一车梨。可是我却没有一点点感动，你说我是一个铁石心肠的人，可是我只是喜欢苹果而已。 任何人不是要你来教他如何做人的！ 难道真如马尔克斯所说——“上了年纪的人不是按照已经活了多少岁来衡量年龄的，而是通过距离死亡还有多远来衡量！” 我刚开上本田的时候，也是宋总这心情，把骑自行车的人贬个贼死，而且心里就会骂出口，大屁股晃什么晃，开个玛萨拉蒂得瑟呀，我那时就像我买得起玛萨拉蒂一样。哈哈，就是个工具，先上手再说。原来人都有这种心态，哈哈哈。]]></content>
      <categories>
        <category>Zhang</category>
      </categories>
      <tags>
        <tag>Zhang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL]]></title>
    <url>%2F2018%2F01%2F16%2FMySQL%2F</url>
    <content type="text"><![CDATA[参考： MySQL5.7参考文档： https://dev.mysql.com/doc/refman/5.7/en/ 环境： CentOS7.x86_64 MySQL5.7 序言MySQL官网： https://www.mysql.com/ 由于MySQL5.7和以前版本之间的许多功能和其他差异，因此此手册不太适用于之前的老版本。之前的版本请参考MySQL相关版本的手册。 综述General information MySQL™ software提供了一个快速、多线程、多任务和健壮的SQL(结构化查询语言)的数据库服务器。MySQL server是为关键服务(mission-critical)、重负荷(heavy-load)生产系统以及嵌入式(embedding)大规模部署的软件而设计。MySQL是Oracle Corporation的商标(trademark)。 MySQL software是双重许可的(dual license)： Open Source product of the GNU General Public License A Standard commercial License from Oracle 关于此手册 该手册作为一个参考，它不提供关于SQL或关系型数据库概念的一般指令； MySQL Database Software正在不断发展，所以参考手册也经常更新。可在此 &lt; http://dev.mysql.com/doc/&gt; 获取最新版的手册； 参考手册(Reference Manual)的源文件使用DocBook XML格式书写的，其他版本(如HTML)等是自动生成的； 如果在使用过程中有任何问题或建议，请发邮件给我们； 手册由MySQL Documentation Team维护。 MySQL数据库管理系统MySQL Database Management System MySQL介绍MySQL是最流行的开源的SQL数据库管理系统，由Oracle Corporation开发、分发和支持。 MySQL is a database management system数据库是一个结构化的数据集合。它可能是从简单的购物清单到图片库，或是公司网络中的大量信息。若要添加、访问和处理存储在计算机数据库中的数据，你需要一个像MySQL Server这样的数据库管理系统。由于计算机非常擅长处理大量的数据，数据库管理系统在计算机中扮演这一个重要的角色。 MySQL databases are relational关系型数据库将数据存储在单独的表(table)中，而不是将所有数据放入一个大的库房中。数据库结构被组织成针对速度优化的物理文件。具有数据库(database)，表(table)，视图(view)，行(row)，列(column)等物理对象的逻辑模型提供了灵活的编程环境。你设置了管理不同数据字段之间关系的规则，如一对一，一对多，唯一，必须和可选关系，以及不同表之间的指针(pointer)。数据库强制执行这些规则，这样在设计良好的数据库中，应用程序就不会看到不一致、重复、孤立、过时或丢失的数据。 MySQL也是代表SQL(Structure Query Language)的一部分。SQL是访问数据库最常用的标准化语言。你可以直接使用SQL语句，或者将SQL语法隐藏到语言特定的API中。 -MySQL software is Open SourceMySQL software使用GPL(GNU General Public License)，开源意味着任何人都可以下载、转发、使用和修改软件，而不需要支付任何费用。 MySQL database server is very fast,reliable,scalabe and easy to use MySQL server works in Client/Server or embedded systemMySQL Database Server是一个由多线程(multi-threaded)SQL Server组成的客户/服务器系统。它支持不同的后端，多个不同的客户程序和库、管理工具和广泛的APIs。还提供MySQL Server作为一个嵌入式多线程库以便链接到你的产品，以获得一个更小，更快，更容易管理的独立产品。 A large amount of contributed MySQL software is available MySQL主要特点Internals and Portability 由C和C++写成 适用于许多不同的平台 为了可移植性，使用CMake 采用独立(independent)模块的多层(layer)服务器设计 设计为使用内核线程的完全多线程，如果有多核CPU，能够轻松使用它们 提供了事务性(transactional)和非事务性(notransactional)存储引擎 使用非常快速的带有索引压缩的B-tree磁盘表 添加其他存储引擎相对容易 使用非常快速的基于线程的内存分配系统 使用优化的嵌套循环(nested-loop)连接执行非常快的联结 实现内存中的hash table，这些表用作临时表 使用高度优化的类库实现SQL函数 数据类型 1,2,3,4和8byte的有无符号(signed/unsigned)的整数(integers) FLOAT DOUBLE CHAR, VARCHAR BINARY, VARBINARY TEXT BLOB DATE, TIME, DATETIME TIMESTAMP YEAR SET ENUM OpenGIS 状态和功能statement and function SELECT和WHERT中包含了所有支持的操作符和函数 SQL中的GROUP BY和ORDER BY也全部支持 GROUP functions(COUNT(), AVG(), STD(), SUM(), MAX(), MIN(), GROUP_CONCAT()) 支持LEFT OUTER JOIN和ROGHT OUTER JOIN 按照SQL标准支持table和columns的别名 支持DELETE,INSERT,REPLACE,UPDATE，以返回受影响的行数 支持MySQL特定的SHOW显示语句 一个EXPLAIN语句显示优化器如何解析查询 安全security 权限(privilege)和密码系统，非常灵活和安全，并且支持基于主机的验证 当连接到Server时，通过加密(encryption)所有密码通信量来确保密码安全 扩展性和限制Scalability and Limits 支持大型数据库。包含五千万条记录，二十万个表，五十亿行 每个表最多支持64个索引，每个索引可以由1到16个列组成 #### 连通性 Conectivity 客户端使用如下几种协议连接到MySQL Server TCP/IP sockets –enable-named-pipe on Windows Unix domain socket files on UNIX MySQL客户端可用多种语言编写 APIs对于多数语言是可用的 本地化Localization Server可以向多种语言的客户端提供错误信息 完全支持几个不同的字符集(character sets) 所有数据都被保存在选取的字符集(chracter set) 排序和比较是根据默认的字符集和排序规则完成 服务器时区(time zone)可动态更改，个客户端也可修改自己的时区 客户端和工具Clients and Tools MySQL包含几个客户机和使用程序 command-line： mysqldump, mysqladmin graphical: MySQL Workbench MySQL Server内置了对SQL语句的支持来检查、优化和修复表 MySQL程序可使用--help或-?来获取帮助 MySQL历史History of MySQL MySQL is named after co-founder Monty Widenius’s daughter, My. The name of the MySQL Dolphin (our logo) is “Sakila,” which was chosen from a huge list of names suggested by users in our “Name the Dolphin” contest. MySQL5.7新特色What Is New in MySQL 5.7 MySQL5.7新功能Features Added in MySQL 5.7 MySQL5.7中过期的功能Features Deprecated in MySQL 5.7 MySQL5.7中移除的功能Features Removed in MySQL 5.7 Server and Status Variables and Options Added, Deprecated, or Removed in MySQL 5.7 MySQL信息源MySQL Information Sources]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018小计划]]></title>
    <url>%2F2018%2F01%2F15%2F2018%E5%B0%8F%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[工作 《MongoDB官方文档》： https://docs.mongodb.com 《SatlStack官方文档》： https://docs.saltstack.com 《MySQL官方文档》： https://dev.mysql.com/doc/ 《TCP/IP协议族》： https://book.douban.com/subject/5386194/ 《Linux性能调优指南》： https://lihz1990.gitbooks.io/transoflptg/content/ 个人 《资本论》： https://book.douban.com/subject/1150503/ 《灵飞经小楷》： https://book.douban.com/subject/1115916/ 《经济学原理》： https://book.douban.com/subject/26435630/ 生活 沉得住气；]]></content>
      <categories>
        <category>Zhang</category>
      </categories>
      <tags>
        <tag>2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海明威的《老人与海》]]></title>
    <url>%2F2018%2F01%2F13%2F%E6%B5%B7%E6%98%8E%E5%A8%81%E7%9A%84%E3%80%8A%E8%80%81%E4%BA%BA%E4%B8%8E%E6%B5%B7%E3%80%8B%2F</url>
    <content type="text"><![CDATA[我不相信人会有所谓的“命运”，但是我相信对于任何人来说，“限度”总是存在的。再聪明再强悍的人，能够做到的事情也总是有限度的。老人桑地亚哥不是无能之辈，然而，尽管他是最好的渔夫，也不能让那些鱼来上他的钩。他遇到他的限度了，就象最好的农民遇上了大旱，最好的猎手久久碰不到猎物一般。每一个人都会遇到这样的限度，仿佛是命运在向你发出停止前行的命令。 可是老人没有沮丧，没有倦怠，他继续出海，向限度挑战。他终于钓到了一条鱼。如同那老人是人中的英雄一样，这条鱼也是鱼中的英雄。鱼把他拖到海上去，把他拖到远离陆地的地方，在海上与老人决战。在这场鱼与人的恶战中，鱼也有获胜的机会。鱼在水下坚持了几天几夜，使老人不能休息，穷于应付，它用酷刑来折磨老人，把他弄得血肉模糊。这时，只要老人割断钓绳，就能使自己摆脱困境，得到解放，但这也就意味着宣告自己是失败者。老人没有作这样得选择，甚至没有产生过放弃战斗的念头。他把那条鲨鱼当作一个可与之交战的敌手，一次又一次地做着限度之外的战斗，他战胜了。 老人载着他的鱼回家去，鲨鱼在路上抢劫他的猎物。他杀死了一条来袭的鲨鱼，但是折断了他的鱼叉。于是他用刀子绑在棍子上做武器。到刀子又折断的时候，似乎这场战斗已经结束了。他失去了继续战斗的武器，他又遇到了他的限度。这是，他又进行了限度之外的战斗：当夜幕降临，更多的鲨鱼包围了他的小船，他用木棍、用桨、甚至用舵和鲨鱼搏斗，直到他要保卫的东西失去了保卫的价值，直到这场搏斗已经变得毫无意义的时候他才住手。 老人回到岸边，只带回了一条白骨，只带回了残破不堪的小船和耗尽了精力的躯体。人们怎样看待这场斗争呢？ 有人说老人桑地亚哥是一个失败了得英雄。尽管他是条硬汉，但还是失败了。 什么叫失败？也许可以说，人去做一件事情，没有达到预期得目的，这就是失败。 但是，那些与命运斗争的人，那些做接近自己限度的斗争的人，却天生地接近这种失败。老人到海上去，不能期望天天有鱼来咬他的钩，于是他常常失败。一个常常在进行着接近自己限度的斗争的人总是会常常失败的，一个想探索自然奥秘的人也常常会失败，一个想改革社会的人更是会常常失败。只有那些安于自己限度之内的生活的人才总是“胜利”，这种“胜利者”之所以常胜不败，只是因为他的对手是早已降伏的，或者说，他根本没有投入斗争。 在人生的道路上，“失败“这个词还有另外的含义，即是指人失去了继续斗争的信心，放下了手中的武器。人类向限度屈服，这才是真正的失败。而没有放下手中武器，还在继续斗争，继续向限度挑战的人并没有失败。如此看来，老人没有失败，老人从未放下武器，只不过是丧失了武器。老人没有失去信心，因此不应当说他是“失败了的英雄”。 那么，什么也没有得到的老人竟是胜利的么？我确是这样看的。我认为，胜利就是战斗到最后的时刻。老人总怀着无比的勇气走向莫测的大海，他的信心是不可战胜的。 他和其他许多人一样，是强悍的人类的一员。我喜欢这样的人，也喜欢这样的人性。我发现，人们常常把这样的事情当作人性最可贵的表露：七尺男子汉坐在厨房里和三姑六婆磨嘴皮子，或者衣装笔挺的男女们坐在海滨，谈论着高尚的、别人不能理解的感情。我不喜欢人们像这样沉溺在人性软弱的部分之中，更不喜欢人们总是这样描写人性。 正像老人每天走向大海一样，很多人每天也走向与他们的限度斗争的战场，仿佛他们要与命运一比高低似的。他们是人中的强者。 人类本身也有自己的限度，但是当人们一再把手伸到限度之外，这个限度就一天一天地扩大了。人类在与限度的斗争中成长。他们把飞船送上太空，他们也用简陋的渔具在加勒比海捕捉巨大的马林鱼。这些事情是同样伟大的。做这样不可思议的事情的人都是英雄。而那些永远不肯或不能越出自己限度的人是平庸的人。 在人类前进的道路上，强者与弱者的命运是不同的。弱者不羡慕强者的命运，强者也讨厌弱者的命运。强者带有人性中强悍的一面，弱者带有人性中软弱的一面。强者为弱者开辟道路，但是强者往往为弱者所奴役，就像老人是为大腹便便的游客打鱼一样。 《老人与海》讲了一个老渔夫的故事，但是在这个故事里却揭示了人类共同的命运。我佩服老人的勇气，佩服他不屈不饶的斗争精神，也佩服海明威。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>王小波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sysctl,ulimit以及/proc]]></title>
    <url>%2F2018%2F01%2F09%2Fsysctl%E3%80%81ulimit%E5%92%8Cproc%2F</url>
    <content type="text"><![CDATA[参考： sysctl命令 ulimit命令 ulimit、limits.conf、sysctl和proc文件系统 sysctl.conf学习和调优 sysctlsysctl 命令被用于在内核运行时动态地修改内核的运行参数，可用的内核参数在目录 /proc/sys 中。它包含一些Tcp/Ip堆栈和虚拟内存系统的高级选项，可以通过修改某些值来提高系统性能。 sysctl可以读取和设置超过五百个系统变量。sysctl变量的设置通常是字符串、数字或布尔型（布尔型用1表示yes，0表示no）。 sysctl - configure kernel parameters at runtime. 语法： 123#sysctl [options] [variable[=value]] [...]sysctl -w net.ipv4.tcp_syncookies=1 可以通过sysctl命令修改系统变量，也可以通过编辑sysctl.conf配置文件来修改系统变量。 sysctl.conf - sysctl preload/configuration file. 举个栗子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124vim /etc/sysct.conf# Controls source route verification# Default should work for all interfaces net.ipv4.conf.default.rp_filter = 1# net.ipv4.conf.all.rp_filter = 1# net.ipv4.conf.lo.rp_filter = 1# net.ipv4.conf.eth0.rp_filter = 1# Disables IP source routing# Default should work for all interfaces net.ipv4.conf.default.accept_source_route = 0# net.ipv4.conf.all.accept_source_route = 0# net.ipv4.conf.lo.accept_source_route = 0# net.ipv4.conf.eth0.accept_source_route = 0# Controls the System Request debugging functionality of the kernelkernel.sysrq = 0# Controls whether core dumps will append the PID to the core filename# Useful for debugging multi-threaded applicationskernel.core_uses_pid = 1# Increase maximum amount of memory allocated to shm# Only uncomment if needed# kernel.shmmax = 67108864# Disable ICMP Redirect Acceptance# Default should work for all interfacesnet.ipv4.conf.default.accept_redirects = 0# net.ipv4.conf.all.accept_redirects = 0# net.ipv4.conf.lo.accept_redirects = 0# net.ipv4.conf.eth0.accept_redirects = 0# enable Log Spoofed Packets, Source Routed Packets, Redirect Packets# Default should work for all interfacesnet.ipv4.conf.default.log_martians = 1#net.ipv4.conf.all.log_martians = 1# net.ipv4.conf.lo.log_martians = 1# net.ipv4.conf.eth0.log_martians = 1# Decrease the time default value for tcp_fin_timeout connectionnet.ipv4.tcp_fin_timeout = 25# Decrease the time default value for tcp_keepalive_time connectionnet.ipv4.tcp_keepalive_time = 1200# Turn on the tcp_window_scalingnet.ipv4.tcp_window_scaling = 1# Turn on the tcp_sacknet.ipv4.tcp_sack = 1# tcp_fack should be on because of sacknet.ipv4.tcp_fack = 1# Turn on the tcp_timestampsnet.ipv4.tcp_timestamps = 1# Enable TCP SYN Cookie Protectionnet.ipv4.tcp_syncookies = 1# Enable ignoring broadcasts requestnet.ipv4.icmp_echo_ignore_broadcasts = 1# Disable ping requestsnet.ipv4.icmp_echo_ignore_all = 1# Enable bad error message Protectionnet.ipv4.icmp_ignore_bogus_error_responses = 1# make more local ports available# net.ipv4.ip_local_port_range = 1024 65000# set TCP Re-Ordering value in kernel to 5net.ipv4.tcp_reordering = 5# Lower syn retry ratesnet.ipv4.tcp_synack_retries = 2net.ipv4.tcp_syn_retries = 3# Set Max SYN Backlog to 2048net.ipv4.tcp_max_syn_backlog = 2048# Various Settingsnet.core.netdev_max_backlog = 1024# Increase the maximum number of skb-heads to be cachednet.core.hot_list_length = 256# Increase the tcp-time-wait buckets pool sizenet.ipv4.tcp_max_tw_buckets = 360000# This will increase the amount of memory available for socket input/output queuesnet.core.rmem_default = 65535net.core.rmem_max = 8388608net.ipv4.tcp_rmem = 4096 87380 8388608 net.core.wmem_default = 65535net.core.wmem_max = 8388608net.ipv4.tcp_wmem = 4096 65535 8388608net.ipv4.tcp_mem = 8388608 8388608 8388608net.core.optmem_max = 40960 重新加载内核参数： 12345#-p, read values from filesysctl -p#-a, display all variablessysctl -a ulimit大多Unix-Like系统，都提供了限制每个进程和每个基本用户使用线程，文件和网络连接等系统资源的一些方法。 假设有这样一种情况，当一台Linux主机上同时登陆了10人，在资源无限制的情况下，这10个用户同时打开了500个文件。假设每个文件的大小有10M，这是系统的内存资源就会收到巨大挑战。但是任何一台主机的资源都不可能是无限的。所以，资源的合理配置和分配，不仅仅是保证系统可用性的必要条件，也与系统上软件运行的性能有着密不可分的联系。 ulimit是指每个user使用各种资源的限制值。ulimit 命令用来限制系统用户对shell资源的访问，它是一种简单并且有效的实现资源限制的方式。 ulimit的设置值是 per-process的，也就是说，每个进程都有自己的limits值； 使用ulimit进行修改，是立即生效的； ulimit只影响shell进程及其子进程，用户登出后失效； 修改ulimit设置之后，要重启程序修改值才会有效。可通过/proc文件系统查看运行进程当前的限制值; 使用ulimit对系统限制的改变在系统重启后都会恢复到默认值; 可以在profile中加入ulimit的设置，便能做到永久生效。 ulimit 用于限制 shell 启动进程所占用的资源，支持以下各种类型的限制： 所创建的内核文件的大小； 进程数据块的大小； Shell进程创建文件的大小； 内存锁住的大小； 常驻内存集的大小； 打开文件描述符的数量； 分配堆栈的最大大小； CPU时间； 单个用户的最大线程数； Shell进程所能使用的最大虚拟内存； 它支持硬资源(hard)和软资源(soft)的限制。 sort和hard hard：是指用户在任何时候都可以活动的进程的最大数量，这是上限。没有任何non-root进程能够增加hard ulimit； soft：是对会话或进程实际执行的限制，但任何进程都可以将其增加到hard ulimit的最大值。 设置ulimit可以在以下位置进行ulimit的设置： /etc/profile，所有用户有效，永久生效； ~/.bash_profile,当前用户有效，永久生效； 直接在控制台修改，当前用户有效，临时生效； 永久生效： 123vim /etc/profilevim ~/.bash_profile 临时生效： 12345678910111213141516171819202122232425ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 7170max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 7170virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited#修改限定值ulimit -n 201400ulimit -t ulimited limits.conflimits.conf - configuration file for the pam_limits module limits.conf是pam_limits.so的配置文件，Linux PAM(Pluggable Authentication Modules，插入式认证模块)。突破系统默认限制，对系统资源有一定保护作用。 pam_limits模块对用户的会话进行资源限制，然后/etc/pam.d/下的应用程序调用pam_***.so模块。 limits.conf是针对用户，而sysctl.conf是针对整个系统参数配置。 一个shell的初始limits就是由pam_limits设定的，用户登录后，pam_limits会给用户的shell设定在limits.conf定义的值； pam_limits的设定值也是per-process； pam_limits的设置是 永久生效的。 配置limits.conf： 1vim /etc/security/limits.conf 举个栗子： 123456789#&lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt;#* soft core 0#* hard rss 10000#@student hard nproc 20#@faculty soft nproc 20#@faculty hard nproc 50#ftp hard nproc 0#@student - maxlogins 4 domain： username @groupname type： soft hard - item： core，限制内核文件的大小 date，最大数据大小 fsize，最大文件大小 memlock，最大锁定内存地址空间 nofile，打开文件的最大数目 rss，最大持久设置大小 stack，最大栈大小 cpu，以分钟为单位的最多CPU时间 nproc，进程的最大数目 as，地址空间限制 maxlogins，此用户允许登录的最大数目 value： item值的大小 /proc什么是/proc文件系统Linux内核提供了一种通过/proc文件系统，在运行时访问内核内部数据结构，改变内核设置的机制。 proc文件系统是一个伪文件系统，它只存在内存当中，不占用外部空间。它以文件系统的方式为访问系统内核数据的操作提供接口。 对/proc中内核文件的修改，针对的是整个系统的内核参数，修改后立即生效，但修改是 临时的，重启后失效。 /proc与sysctl.conf的对应关系修改/proc文件系统中的参数是临时的，但修改sysctl.conf的参数确是永久有效的。 配置文件sysctl.conf变量在/proc/sys下，其对应关系如下： 123456789#将文件名的 . 变为 /#/proc/sys/net/ipv4/icmp_echo_ignore_all#net.ipv4.icmp_echo_ignore_allecho 0 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_allvim /etc/sysctl.confnet.ipv4.icmp_echo_ignore_all = 0 /proc文件系统几个常用的内核文件 /proc/meminfo #内存信息 /proc/cpuinfo #CPU信息 /proc/sys/fs/file-max #文件打开数 /proc/sys/fs/file-nr #整个系统目前使用的文件句柄数量 /proc文件系统中文件的权限proc中的每个文件都有一组分配给它的非常特殊的文件许可权，并且每个文件属于特定的用户标识。 只读：任何用户都不能更改该文件，它用于表示系统信息 root写 root读 对/proc进行读写123456cat /proc/sys/net/ipv4/icmp_echo_ignore_all#0echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all#当然,也可是用sysctl来配置 /proc内核文件详解 /proc/buddyinfo 每个内存区中的每个order有多少块可用，和内存碎片问题有关 /proc/cmdline 启动时传递给kernel的参数信息 /proc/cpuinfo cpu的信息 /proc/crypto 内核使用的所有已安装的加密密码及细节 /proc/devices 已经加载的设备并分类 /proc/dma 已注册使用的ISA DMA频道列表 /proc/execdomains Linux内核当前支持的execution domains /proc/fb 帧缓冲设备列表，包括数量和控制它的驱动 /proc/filesystems 内核当前支持的文件系统类型 /proc/interrupts x86架构中的每个IRQ中断数 /proc/iomem 每个物理设备当前在系统内存中的映射 /proc/ioports 一个设备的输入输出所使用的注册端口范围 /proc/kcore 代表系统的物理内存，存储为核心文件格式，里边显示的是字节数，等于RAM大小加上4kb /proc/kmsg 记录内核生成的信息，可以通过/sbin/klogd或/bin/dmesg来处理 /proc/loadavg 根据过去一段时间内CPU和IO的状态得出的负载状态，与uptime命令有关 /proc/locks 内核锁住的文件列表 /proc/mdstat 多硬盘，RAID配置信息(md=multiple disks) /proc/meminfo RAM使用的相关信息 /proc/misc 其他的主要设备(设备号为10)上注册的驱动 /proc/modules 所有加载到内核的模块列表 /proc/mounts 系统中使用的所有挂载 /proc/mtrr 系统使用的Memory Type Range Registers (MTRRs) /proc/partitions 分区中的块分配信息 /proc/pci 系统中的PCI设备列表 /proc/slabinfo 系统中所有活动的 slab 缓存信息 /proc/stat 所有的CPU活动信息 /proc/sysrq-trigger 使用echo命令来写这个文件的时候，远程root用户可以执行大多数的系统请求关键命令，就好- 像在本地终端执行一样。要写入这个文件，需要把/proc/sys/kernel/sysrq不能设置为0。这个文件对root也是不可- 读的 /proc/uptime 系统已经运行了多久 /proc/swaps 交换空间的使用情况 /proc/version Linux内核版本和gcc版本 /proc/bus 系统总线(Bus)信息，例如pci/usb等 /proc/driver 驱动信息 /proc/fs 文件系统信息 /proc/ide ide设备信息 /proc/irq 中断请求设备信息 /proc/net 网卡设备信息 /proc/scsi scsi设备信息 /proc/tty tty设备信息 /proc/net/dev 显示网络适配器及统计信息 /proc/vmstat 虚拟内存统计信息 /proc/vmcore 内核panic时的内存映像 /proc/diskstats 取得磁盘信息 /proc/schedstat kernel调度器的统计信息 /proc/zoneinfo 显示内存空间的统计信息，对分析虚拟内存行为很有用 以下是/proc目录中进程N的信息： /proc/N pid为N的进程信息 /proc/N/cmdline 进程启动命令 /proc/N/cwd 链接到进程当前工作目录 /proc/N/environ 进程环境变量列表 /proc/N/exe 链接到进程的执行命令文件 /proc/N/fd 包含进程相关的所有的文件描述符 /proc/N/maps 与进程相关的内存映射信息 /proc/N/mem 指代进程持有的内存，不可读 /proc/N/root 链接到进程的根目录 /proc/N/stat 进程的状态 /proc/N/statm 进程使用的内存的状态 /proc/N/status 进程状态信息，比stat/statm更具可读性 /proc/self 链接到当前正在运行的进程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>系统优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源许可协议]]></title>
    <url>%2F2018%2F01%2F09%2F%E5%BC%80%E6%BA%90%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[当你为你的产品签发许可，你就是在出让自己的权利。不过，你仍拥有版权和专利（如果申请了专利）。许可的目的，是向使用你产品的人提供一定的权利。 不管产品是免费分发，还是出售，指定一份许可协议都非常有用。否则，对于免费，你相当于放弃了自己的所有权利，任何人都没有义务表明你的原始作者身份。对于出售，你将不得不花费比开发更多的精力用来处理授权问题。 而开源许可协议是这些事情变得简单，开发者很容易向一个项目贡献自己的代码，它还可以保护你原始作者的身份，使你至少获得认可。开源许可协议还可以阻止其它人将某个产品据为己有。 几大开源许可协议 GNU Project GNU是“GNU’s Not Unix”的递归缩写，发音为 /‘gnu:’/； GNU Project，是一个由自由软件集体协作项目，它的目标是创建一套完全自由的操作系统，称为GNU； GNU是一个自由操作系统，其内容软件完全以 GPL 方式发布，它的设计类似于Unix，但它不包含具有著作权的Unix代码。 GPLGNU(General Public Licence)，GNU通用许可协议(简称GPL)是广泛使用的免费软件许可证，也称为 copyleft，与copyright相对应。GPL保证了所有开发者的权利，同时为使用者提供了足够的复制、分发、修改的权利。 需要注意的是，分发的时候，需要明确提供源代码和二进制文件。 可自由复制： 你可以将软件复制到你的电脑或任何地方，复制份数没有限制； 可自由分发： 可下载后拷贝分发； 可以用来盈利： 你可以在分发软件的时候收费，但必须在收费前向你的客户提供该软件的 GNU GPL许可协议，以便让他们知道，他们可以从别的渠道免费得到这份软件以及你收费的理由； 可自由修改： 你过你想添加或删除某个功能，没问题。如果你想在别的项目中使用部分代码，也没问题，唯一要求是使用了这段代码的项目也必须使用 GPL协议。 LGPLGNU还有另外一种协议，叫做LGPL（Lesser General Public License），它对产品所保留的权利比GPL少。总的来说，LGPL适合那些用于非GPL或非开源产品的开源类库或框架。因为GPL要求，使用了GPL代码的产品也必须使用GPL协议，开发者不允许将GPL代码用于商业产品。LGPL绕过了这一限制。 GPL和LGPL都属于GNU计划里面的许可证。 BSD伯克利软件套件（Berkeley Software Distribution，缩写BSD），也被称为伯克利Unix，是一个操作系统的名称，衍生自Unix，也被用来代表一整套软件发行版。 BSD许可证（Berkeley Software Distribution License），是自由软件中使用广泛的许可证。BSD软件就是遵照这个许可证来发布，该许可证也因此而得名。 BSD在软件分发方面的限制比别的开源协议要少，且和GPL兼容，并为开源组织所认可。 MITMIT（Massachusetts Institute of Technology），麻省理工学院。MIT许可协议（The MIT License）是许多软件授权条款中，被广泛使用的其中一种。与其他常见的软件许可协议相比，MIT是相对宽松的软件许可协议，除了必须包含许可声明外，再无任何限制。 MIT许可协议核心条款： 该软件及其相关文档对所有人免费，可以任意处置，包括使用、复制、修改、合并、发表、分发、再授权或销售； 唯一的限制，软件中必须包含上述版权和许可证。 ApacheApache许可证（Apache License），是一个由Apache软件基金会发布的自由软件许可证。Apache许可证要求被授权者保留版权和放弃权利的声明，但它不是一个反版权的许可证。兼容与GPL。 除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。 永久权利：一旦被授权，永久拥有； 全球范围的权利：在一个国家获得授权，适用于所有国家； 授权免费，且无版税：前后期均无任何费用； 授权不可撤销：一旦获得授权，没有任何人可以取消。 分发代码方面，要在声明中对参与开发的人给予认可并包含一份许可协议原文。 MPLMPL是The Mozilla[mɔzilə] Public License的简写，是1998年初Netscape的 Mozilla小组为其开源软件项目设计的软件许可证。MPL许可证出现的最重要原因就是，Netscape公司认为GPL许可证没有很好地平衡开发者对源代码的需求和他们利用源代码获得的利益。 同著名的GPL许可证和BSD许可证相比，MPL在许多权利与义务的约定方面与它们相同（因为都是符合OSIA 认定的开源软件许可证）。 MPL几个特点： MPL虽然要求对于经MPL许可证发布的源代码的修改也要以MPL许可证的方式再许可出来，以保证其他人可以在MPL的条款下共享源代码。但是，在MPL 许可证中对“发布”的定义是“以源代码方式发布的文件”，这就意味着MPL允许一个企业在自己已有的源代码库上加一个接口，除了接口程序的源代码以MPL 许可证的形式对外许可外，源代码库中的源代码就可以不用MPL许可证的方式强制对外许可。这些，就为借鉴别人的源代码用做自己商业软件开发的行为留了一个豁口； MPL许可证第三条第7款中允许被许可人将经过MPL许可证获得的源代码同自己其他类型的代码混合得到自己的软件程序； 对软件专利的态度，MPL许可证不像GPL许可证那样明确表示反对软件专利，但是却明确要求源代码的提供者不能提供已经受专利保护的源代码（除非他本人是专利权人，并书面向公众免费许可这些源代码），也不能在将这些源代码以开放源代码许可证形式许可后再去申请与这些源代码有关的专利； 对源代码的定义，MPL许可证第3条有专门的一款是关于对源代码修改进行描述的规定，就是要求所有再发布者都得有一个专门的文件就对源代码程序修改的时间和修改的方式有描述。 CC知识共享许可协议(Creative Commons License，简称CC)，并非严格意义上的开源许可，是一种公共版权许可协议。它主要用于设计，其允许分发受版权保护的作品。 CC协议主要包含4种基本形式： 署名权：必须为原始作业署名，然后才可以修改、分发、复制； 保持一致：作品同样可以在CC协议的基础上修改、分发、复制； 非商业：不能用于商业用途； 不能衍生新作品：你可以复制、分发、但不能修改，也不能以此为基础创作自己的作品。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>开源许可协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yum源]]></title>
    <url>%2F2018%2F01%2F09%2FYum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[参考： CentOS 7下配置本地yum源及yum客户端 Centos7 配置本地源+阿里yum源/epel-yum+修改优先级 调整CentOS 7中yum仓库的优先级 国内开源站点 国内开源镜像站点 网易开源镜像站：http://mirrors.163.com/ 阿里云开源镜像站：http://mirrors.aliyun.com 清华大学开源镜像站：https://mirrors.tuna.tsinghua.edu.cn/ 浙江大学开源镜像站： http://mirrors.zju.edu.cn/ 中国科技大学开源镜像站：http://mirrors.ustc.edu.cn/ CentOS自带源rpm包管理方式，对于安装、升级、卸载却难以处理包之间的依赖关系。而yum作为一个rpm包前端管理工具，可以自动处理依赖性，并支持在线现在、安装、升级、卸载rpm软件包。 CentOS默认自带CentOS-Base.repo源，但官方源在国外，连接速度令人心痛。并且有很多软件在默认源里面是找不到的。 配置网络yun源配置aliyun.repo： 12345678910#先备份默认源mv CentOS-Base.repo&#123;,.bak&#125;#下载阿里云源替换默认源wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repoyum clean allyum makecache #重构yum缓存yum repolist #查看yum仓库 配置本地yum源配置本地yum源，考虑到优先使用本地安装包，所以会涉及到一个优先级的概念。 安装完毕后，就可以在yum源中添加一个优先级priority。 安装yum优先级插件： 1234567yum install -y yum-plugin-priorities#检查安装完成后配置vim /etc/yum/pluginconf.d/priorities.confenable=1#enable=0 创建本地yum源： 123456789101112131415161718192021222324mv /etc/yum.repos.d/CentOS-Base.repo&#123;,.bak&#125;vim /etc/yum.repos.d/CentOS-Local.repo[base-Local]name=Centos- Localbaseurl=file:///mnt/xxxgpgcheck=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7priority=1 #优先级为1[updates-Local]name=CentOS- Localgpgcheck=0baseurl=file:///dir/path/gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7priority=1······#具体可参考CentOS-Base.repo#可将aliyun源优先级写成2yum clean allyum makecache 配置ftp方式源1234567891011vim /etc/yum.repos.d/ftp.repo[ftp-media]name=name=CentOS-$releasever - mediabaseurl=ftp://ipgpgcheck=0enable=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7yum clean allyum makecache 其他常见YUM源官方的默认yum源提供的软件包往往是很滞后的，(可能为了服务器版本的稳定性和安全性)。并且官方默认源提供的RPM包也不够丰富。 EPEL源EPEL的全称叫 Extra Packages for Enterprise Linux 。EPEL是由 Fedora 社区打造，为 RHEL 及衍生发行版如 CentOS、Scientific Linux 等提供高质量软件包的项目。装上了 EPEL之后，就相当于添加了一个第三方源。 EPEL源为服务器提供了大量的rpm包(这些包可能有很多在默认源中没有)，并且绝大多数rpm包比官方默认源版本要新。 添加epel源：epel下载地址：http://download.fedora.redhat.com/pub/epel/123rpm -vih http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-9.noarch.rpm#yum install -y epel-release remi源Remi源大家或许很少听说，不过Remi源GoFace强烈推荐，尤其对于不想编译最新版的linux使用者，因为Remi源中的软件几乎都是最新稳定版。或许您会怀疑稳定不？放心，这些都是Linux骨灰级的玩家编译好放进源里的，他们对于系统环境和软件编译参数的熟悉程度毋庸置疑。 添加remi源：Remi下载地址：http://rpms.famillecollet.com123rpm -ivh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm#yum install -y http://rpms.famillecollet.com/enterprise/remi-release-7.rpm RPMForge源RPMForge是CentOS系统下的软件仓库，拥有4000多种的软件包, 被CentOS社区认为是最安全也是最稳定的一个软件仓库。 添加RPMForge源：RPMForge下载地址：http://repository.it4i.cz/mirrors/repoforge/redhat/el7/en/x86_64/rpmforge/RPMS/GitHub:https://github.com/repoforge 123rpm -ivh http://repository.it4i.cz/mirrors/repoforge/redhat/el7/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el7.rf.x86_64.rpm#yum localinstall --nogpgcheckhttp://repository.it4i.cz/mirrors/repoforge/redhat/el7/en/x86_64/rpmforge/RPMS/rpmforge-release-0.5.3-1.el7.rf.x86_64.rpm]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写给2017]]></title>
    <url>%2F2018%2F01%2F01%2F%E5%86%99%E7%BB%992017%2F</url>
    <content type="text"><![CDATA[2017大事件 Feb.27.2017，开始工作； May.27.2017，工作转正； Jun.25.2017，大学毕业； Aug.27.2017，工作半年； Aug.30.2017，搭建博客。 记得大四下期的时候，在学校实在是想出来工作，以为这样就可以有工资有点钱可以实现财务自由。Naive x3! 记得刚去面试的时候，公司的HR问我期待的薪资是多少。我这个老实人也不敢往高了说，以为能解决温饱就不错。这就导致后面日子过得紧巴巴，生活基本上处于没钱不敢消费不敢出去的惨状。Naive x3! 记得那天是星期四，从学校收拾了一箱子的衣物加上自己的笔记本就来了成都。还要感谢强对我的帮助，在他那借宿了两天。这两天跑到公司附近找房子，这打个电话，那去看看…结果看了看自己兜里不到三千，再看看房租，还是算了吧！后来由于周一就要开始去公司报道了，实在没办法，硬着头皮找了一个750的单间。我现在觉得，人在生活面前真的没有办法。借用张主任的一句话——“是生活，生活强奸了所有人。”一个套三的屋子，硬是被房主改造成了七间房，住了11人。我想各位住户都迫于无奈吧，有谁想和别人挤在一起了，况且还要忍受上厕所的艰难时光。还是说说我住的那间房屋吧。是客厅使用木板隔得房间，手指随便敲几下都铛铛作响，况且隔壁还住了一对情侣。一张床，一个烂小衣柜，一张摆放电脑的桌子。客厅的后面有一个阳台，所以就会有一个大窗户间隔阳台。然而这个阳台是和隔壁公用的，也就是说，这个大窗户是可以互通的。哎！阳台风大，还好我只住到了夏天。五月二十号，和我强一起搬到了中和。也是没有做好十足打算，说搬就搬了。虽说住宿条件好了一些，可这面修路是真的堵，公家车是真的挤，真的是挤得怀疑人生的那种，还是走路吧。由于搬过来中和，房租和其他开支多了一些，刚开始没什么钱的时候，自己带饭去公司。我都佩服自己，那么挤得车上还能坚持带饭几个月，看来还是穷吧。冬天到了，太冷了就没有带饭到公司了。但公司楼下吃饭真的好贵，吃不起吃不起。由于住宿条件好了一些，晚上回来可以自己做饭，还是不错。并且晚上基本有两个小时干干工作之外的事情，看看小说、看看电视、发发呆什么也挺好的！ 记得才开始工作的时候，好像每天都很闲，没有事做。心里担忧的不行，这样怎么有提升呢？每天都急躁不安，这不会那不会，又腼腆害羞…后来慢慢理解，任何事情都不能操之过急，不能带着情绪上班。不喜欢某个同事也不能表现出来，更不能带到工作上。工作是工作，生活是生活，一定要分开。被骂也没有办法，骂吧，你骂高兴。任何人都不会听你的理由，理由对于别人来说只是你没有完成的狡辩。踏踏实实上班，做好自己的事情才是正解！千万不要好为人师，人都是有嫉妒心得，不要太招摇！也不要羡慕别人工作轻松之类，那是人家的工作，和你没有半毛钱关系。上班好好上班，不要搞东搞西。我个人比较看重效率，怀着为了加班而加班的目的真的没有必要。那时由于住单间，条件差，愿意在公司多呆一会，多学习知识。越到后面，觉得自己越得努力学习新知识，需要了解的知识就在那，就等我把它们一个个打上勾，撸起袖子加油干。 自我批评: 心胸不够宽广，容易嫉妒别人； 害羞爱面子，拉不下面子做事情； 由于自卑心理导致的不敢说不敢争。 以前我以为自己是讨厌某种方式、讨厌别人炫耀、讨厌别人秀恩爱，现在才知道是由于自己没有，就用自己的不屑和厌恶来突出自己，让自己心安理得。说白了还不是自己嫉妒人家，嫉妒人家比我好，有女票。这点是真的要改，一定要改。千万别用别人的缺点来突出自己，这很SB，切记切记。如同小波所说：“人的一切痛苦，本质都是对于自己无能的愤怒。” 在学校总幻想自己能成为英雄，总想做一番事业，像历史上的英雄那般。不屑考个老师公务员职位，以为一辈子就那样，混吃等死的咸鱼。其实其它工作不也同样是这样吗。混吃等死的不是任何工作岗位，而是人！任何岗位都可以有所成就。自己也不过只是凡人一个。不过凡人却也可以有自己的一片天地。就像小波说的：“井底之蛙也能拥有自己的一片天地。” 工作没有高低贵贱之分，不要带着要面子的心情而不愿意做些打杂的活。没有基础的积累，哪来平地起的万丈高楼。不要看不起自己也不要看不起他人。三十年河东三十年河西，此一时彼一时。 ”中国的君子独善其身，这样就没有了尊严。这是因为尊严是属于个人的、不可压缩的空间，这块空间要靠自己来捍卫——捍卫的意思是指敢争、敢打官司、敢动手（勇斗歹徒）。我觉得人还是有点尊严的好，假如个人连个待的地方都没有，就无法为人做事，更不要说做别人的典范。“这句话同样适合我，该说该争该做的时候就应该大胆站出来，有一个男子汉的样子。要敢于亮剑！ 我现在还不太明白我的人生目标是什么，名利？我不知道。我只想做一个懂点道理的人。上班以后感觉也变得世俗化了，不经意间都会主动被动地谈及任何关于钱的话题。我对钱有一些兴趣，但不愿为之受罪。我不想把自己的下半生绑在房子上，虽然我也买不起。找不到人同我谈谈诗歌、文学、历史、足球，谈谈理想和爱情。但正如小波所言：”和我志趣相投的人总不会一个人都没有吧。“我也不顾影自怜了。 展望2018 最重要的事情当然是涨工资啦，哈哈。加油加油，为了涨工资可得好好奋斗； 如果能找一个离家近一点的工作当然是最好； 一个人总是孤独寂寞的，能找到一个能相互扶持的人当然最好； 改善自己的不同，提升自己的优点。扬长避短，向优秀的人多学习。 不成熟的想法 成都这地方什么都要争，连坐个公交地铁也得积极地抢位置，哎； 人太多，太拥挤，随便去哪都是拥挤的要命。但另一方面却是人越多机会越多； 原来工作才是一生的主题。但到底图个啥——名？利？ 上班以后认识的人也变少了，圈子也基本没什么了。曾经的同学们也各奔东西了； 我发觉任何一件事都是矛盾的。正面想是这样，反面想却又是那样，但都有道理。让我想起了一个故事，“一个农夫和一个老板在海边的对话。问：这么努力工作干嘛？答：为了以后能过更轻松的日子。那你看我现在不是挺轻松自在的吗？” 现在的自媒体为了流量真的是无所不用其极。各种大噱头的标题，完全不负责任的报道，只为吸引流量。到最后都不知道该相信谁，会不会被带节奏…； 一天24小时。8小时睡觉，8小时上班，3小时上下班，2小时吃饭及其他。It means that I only have 3 hours a day without Rest Day; 一个人只拥有此生此世是不够的，他还应该拥有诗意的世界。]]></content>
      <categories>
        <category>Zhang</category>
      </categories>
      <tags>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[君子的尊严]]></title>
    <url>%2F2017%2F12%2F28%2F%E5%90%9B%E5%AD%90%E7%9A%84%E5%B0%8A%E4%B8%A5%2F</url>
    <content type="text"><![CDATA[笔者是个学究，待人也算谦和有礼，自以为算个君子——当然，实际上是不是，还要别人来评判。总的来说，君子是有文化有道德的人，是士人或称知识分子。按照中国的传统，君子是做人的典范。君子不言利。君子忍让不争。君子动口不动手。君子独善其身。这都是老辈子传下来的规矩，时至今日，以君子自居的人还是如此行事。我是宁做君子不做小人的，但我还是以为，君子身上有些缺点，不配作为人的典范；因为他太文弱、太窝囊、太受人欺。 君子既不肯与人争利，就要安于清贫。但有时不是钱的问题，是尊严的问题。前些时候在电视上看到北京的一位人大代表发言，说儿童医院的挂号费是一毛钱，公厕的收费是两毛钱。很显然，这样的收费标准有损医务工作的尊严。当然，发言的结尾是呼吁有关领导注意这个问题，有关领导也点点头说：是呀是呀，这个问题要重视。我总觉得这位代表太君子，没把话讲清楚——直截了当的说法是：我们要收两块钱。别人要是觉得太贵，那你就还个价来——这样三下五除二就切入了正题。这样说话比较能解决问题。 君子不与人争，就要受气。举例来说，我乘地铁时排队购票，总有些不三不四的人到前面加塞。说实在的，我有很多话要说：我排队，你为什么不排队？你忙，难道我就没有事？但是碍于君子的规范，讲不出口来。话憋在肚子里，难免要生气。有时气不过，就嚷嚷几句：排队，排队啊。这种表达方式不够清晰，人家也不知是在说他。正确的方式是：指住加塞者的鼻子，口齿清楚地说道：先生，大家都在排队，请你也排队。但这样一来，就陷入与人争论的境地，肯定不是君子了。 常在报纸上看到这样的消息：流氓横行不法，围观者如堵，无人上前制止。我敢断定，围观的都是君子，也很想制止，但怎么制止呢？难道上前和他打架吗？须知君子动口不动手啊。我知道英国有句俗话：绅士动拳头，小人动刀子。假如在场的是英国绅士，就可以上前用拳头打流氓了。 既然扯到了绅士，就可以多说几句。从前有个英国人到澳大利亚去旅行，过海关时，当地官员问他是干什么的。他答道：我是一个绅士。因为历史的原因，澳大利亚人不喜欢听到这句话，尤其不喜欢听到这句话从一个英国人嘴里说出来。那官员又问：我问你的职业是什么？英国人答道：职业就是绅士。难道你们这里没有绅士吗？这下澳大利亚人可火了，差点揍他，幸亏有人拉开了。在英美，说某人不是绅士，就是句骂人话。当然，在我们这里说谁不是君子，等于说他是小人，也是句骂人话。但君子和绅士不是一个概念。从字面上看，绅士（gentleman）是指温文有礼之人，其实远不止此。绅士要保持个人的荣誉和尊严，甚至可以说是这方面的专业户。坦白地说，他们有点狂傲自大。但也有一种好处：真正的绅士决不在危险面前止步。大战期间，英国绅士大批开赴前线为国捐躯，甚至死在了一般人前面。君子的标准里就不包括这一条。 中国的君子独善其身，这样就没有了尊严。这是因为尊严是属于个人的、不可压缩的空间，这块空间要靠自己来捍卫——捍卫的意思是指敢争、敢打官司、敢动手（勇斗歹徒）。我觉得人还是有点尊严的好，假如个人连个待的地方都没有，就无法为人做事，更不要说做别人的典范。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>王小波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人尊严]]></title>
    <url>%2F2017%2F12%2F28%2F%E4%B8%AA%E4%BA%BA%E5%B0%8A%E4%B8%A5%2F</url>
    <content type="text"><![CDATA[在国外时看到，人们对时事做出价值评判时，总是从两个独立的方面来进行：一个方面是国家或者社会的尊严，这像是时事的经线；另一个方面是个人的尊严，这像是时事的纬线。回到国内，一条纬线就像是没有，连尊严这个字眼也感到陌生了。提到尊严这个概念，我首先想到的英文词＂dignity＂，然后才想到相应的中文词。在英文中，这个词不仅有尊严之义，还有体面、身份的意思。尊严不但指人受到尊重，它还是人价值之所在。从上古到现代，数以亿万计的中国人里，没有几个人有过属于个人的尊严。举个大点的例子，中国历史上有过皇上对大臣施廷杖的事，无论是多大的官，一言不和，就可能受到如此当众羞辱，高官尚且如此，遑论百姓。除了皇上一人，没有一个人能有尊严。有一件最怪的事是，按照传统道德，挨皇帝的板子倒是一种光荣，文死谏嘛。说白了就是：无尊严就是有尊严。此话如有任何古怪之处，罪不在我。到了现代以后，人与人的关系、个人与集体的关系，仍有这种遗风──我们就不必细说文革中、文革前都发生过什么样的事情。到了现在，已经不用见官下跪，也不会在屁股上挨板子，但还是缺少个人的尊严。环境就是这样，公共场所的秩序就是这样，人对人的态度就是这样，不容你有任何自尊。 举个小点的例子，每到春运高潮，大家就会在传媒上看到一辆硬座车厢里挤了三四百人，厕所里也挤了十几人。谈到这件事，大家会说国家的铁路需要建设，说到铁路工人的工作难做，提到安全问题，提到所有的方面，就是不提这些民工这样挤在一起，好像一个团，完全没有了个人的尊严──仿佛这件事很不重要似的。当然，只要民工都在过年时回家，火车总是要挤的；谁也想不出好办法。但个人的尊严毕竟大受损害；这件事总该有人提一提才对。另一件事现在已是老生常谈，人走在街上感到内急，就不得不上公共厕所。一进去就觉得自己的尊严一点都没了。现在北京的公厕正在改观，这是因为外国人到了中国也会内急，所以北京的公厕已经臭名远扬。假如外国人不来，厕所就要臭下去；而且大街上改了，小胡同里还没有改。我认识的一位美国留学生说，有一次他在小胡同里内急，走进公厕撒了一泡尿，出来以后，猛然想到自己刚才满眼都对黄白之物，居然能站住了不倒，觉得自己很了不起，就急忙来告诉我。北京的某些街道很脏很乱，总要到某个国际会议时才能改观，这叫借某某会的东风。不光老百姓这样讲，领导上也这样讲。这话听起来很有点不对味。不雅的景象外人看了丢脸，没有外人时，自己住在里面也不体面──这后一点总是被人忘掉。 作为一个知识分子，我发现自己曾有一种特别的虚伪之处，虽然一句话说不清，但可以举些例子来说明。假如我看到火车上特别挤，就感慨一声道：这种事居然可以发生在中华人民共和国的土地上！假如我看到厕所特脏，又长叹一声：唉！北京市这是怎么搞的嘛！这其中有点幽默的成份，也有点当真。我的确觉得国家和政府的尊严受到了损失，并为此焦虑着。当然，我自己也想要点个人尊严，但以个人名义提出就过于直露，不够体面──言必称天下，不以个人面目出现，是知识分子的尊严所在。当然，现在我把这做为虚伪提出，已经自外于知识分子。但也有种好处，我找到了自己的个人面目。有关尊严问题，不必引经据典，我个人就是这么看。但中国忽视个人尊严，却不是我的新发现。从大智者到通俗作家，有不少人注意到一个有中国特色的现象：罗素说，中国文化里只重家族内的私德，不重社会的公德公益，这一点造成了很要命的景象；费孝通说，中国社会里有所谓＂差序格局＂，与己关系近的就关心，关系远的就不关心或少关心；结果有些事从来就没人关心。龙应台为这类事而愤怒过，三毛也大发过一通感慨。读者可能注意到了，所有指出这个现象的人，或则是外国人，或则曾在国外生活过，又回到了国内。没有这层关系的中国人，对此浑然不觉。笔者自己曾在外国居住四年，假如没有这种经历，恐怕也发不出这种议论──但这一点并不让我感到开心。环境脏乱的问题，火车拥挤的问题，社会秩序的问题，人们倒是看到了。但总从总体方面提出问题，讲国家的尊严、民族的尊严。其实这些事就发生在我们身边，削我们每个人的面子──对此能够浑然无觉，倒是咄咄怪事。 人有无尊严，有一个简单的判据，是看他被当作一个人还是一个东西来对待。这件事有点两重性，其一是别人把你当做人还是东西，是你尊严之所在。其二是你把自己看成人还是东西，也是你的尊严所在。挤火车和上公共厕所时，人只被当身体来看待。这里既有其一的成份，也有其二的成份；而且归根结蒂，和我们的文化传统有关。 说来也奇怪，中华礼仪之邦，一切尊严，都从整体和人与人的关系上定义，就是没有个人的位置。一个人不在单位里、不在家里，不代表国家、民族，单独存在时，居然不算一个人，就算是一块肉。这种算法当然是有问题。我的算法是：一个人独处荒岛而且谁也不代表，就像鲁滨孙那样，也有尊严，可以很好的活着。这就是说，个人是尊严的基本单位。知道了这一点，火车上太挤了之后，我就不会再挤进去而且浑然无觉。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>王小波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SaltStack]]></title>
    <url>%2F2017%2F12%2F25%2FSaltStack%2F</url>
    <content type="text"><![CDATA[参考： SaltStack官网：https://saltstack.com ; SaltStack官方文档：https://docs.saltstack.com ; SaltStack-GitHub：https://github.com/saltstack ; Salt-repo：https://repo.saltstack.com/ . 环境： CentOS7_x64; Salt-2017.7.2 InstructionSaltStack是一种革命性的用速度(speed)取代复杂性(complexity)的基础设施(infrastucture)管理方法。 简单(Simple)，可以在几分钟内运行； 可伸缩性(Scalable)，足以管理数以万计的Server； 快速(Fast)，能在几秒内与各系统间进行通信。 You’ll learn how to: 安装和配置SaltStack； 在所有托管系统上远程执行命令(Remotely execute commands)； 设计、开发和部署系统配置； 使用Salt Reactor是基础设施自动化(automate)； 使用Salt Orchestration协调复杂管理操作。 Salt是建立在动态通信总线(dynamic communication bus)上的基础设施管理的一种新方法。Salt可以用于数据驱动(data-driven)业务，远程执行(remote execution)任何基础设施，配置管理(configuration management)任意应用堆栈… REMOTE EXECUTION; CONFIGURATION MANAGEMENT; EVENT-DRIVEN INFRASTRUCTURE; SALT ESSENTIALS. Installation如果是第一次设置环境，你应该在专用的管理服务器上安装Salt master，然后在每个使用Salt管理的系统上安装Salt minion。现在不需要担心你的系统架构(architecture)，你可以在以后轻易添加组件(componet)和修改配置(configuration)而不需要重新安装任何东西。 The general installation process is as follows: 安装Salt master，通过各平台说明安装或通过Salt bootstrap.sh脚本来安装； 确保你的Salt minion能够找到Salt master； 在想要管理的每个系统上安装Salt minion； 在Salt minion连接后接受Salt minion key。 在此之后，就可以运行一个简单命令，并从所有的Salt minion接收返回。 1234#salt &lt;minion-id&gt; &lt;cmd&gt;salt minion1 test.pingsalt * test.ping Quick Install在绝大多数发行版本上，可以使用Salt Bootstrap脚本进行快速安装。 参考：Salt Bootstrap 1234567#wgetwget https://bootstrap.saltstack.com -O bootstrap-salt.shsh bootstrap-salt.sh#curlcurl -o bootstrap-salt.sh -L https://bootstrap.saltstack.comsh bootstrap-salt.sh Platform-Specific Installation选择发行版本安装 在CentOS7上安装Saltrepo: https://repo.saltstack.com/#rhel 1. 下载SaltStack-Repository进行安装： systemd和systemd-python是Salt必须的，在安装Salt前需装好。 1234567891011121314#安装salt-repoyum install -y https://repo.saltstack.com/yum/redhat/salt-repo-latest-2.el7.noarch.rpmyum clean expire-cache#安装salt组件yum install -y salt-master salt-master salt-ssh salt-syndic salt-cloud salt-api#开启systemctl start salt-master 2. 自建salt-repo： 123456789vim /etc/yum.repos.d/saltstack.repo[saltstack-repo]name=SaltStack repo for Cent0S7baseurl=https://repo.saltstack.com/yum/redhat/$releasever/$basearch/latestenalbed=1gpgcheck=1gpgkey=https://repo.saltstack.com/yum/redhat/$releasever/$basearch/latest/SALTSTACK-GPG-KEY.pub Initial ConfigurationConfiguration Saltsalt-master 的默认配置会为安装而工作，唯一要求是在salt-minion的配置文件中设置salt-master的位置。 salt-master configuration默认的，salt-master配置文件位于/etc/salt/master，在all interfaces(0.0.0.0)上监听4505和4506端口。 1234vim /etc/salt/masterinterface: 0.0.0.0 salt-minion configuration默认，一个salt-minion会尝试连接到DNS名称为salt。如果salt-minion能够正确解析(resolve)这个名称，则可以不需要配置文件。如果DNS名称salt未能解析为salt-master的正确位置，那么可在/etc/salt/minion配置文件下重新定义salt。 12345678vim /etc/salt/minion#master: salt#如果是默认的salt,需要在本地hosts下解析salt#此处我们修改为salt-master的IP地址master: 192.168.1.9 修改配置文件后，请重启服务。 Proxy Minion ConfigurationA Proxy Minion 模仿一个规律的行为和继承(inherit)他们的选项。类似地，它的配置文件存放于/etc/salt/proxy，proxy也将尝试连接DNS名为salt的主机。 除了salt-minion有规律的选型，proxy还有一些特定的选项。参考:Proxy minion Running Salt以salt命令运行: 123456789101112salt-master#开启守护进程dalt-master -dsalt-minionsalt-minion -d#日志信息salt-master --log-level=debug 以non-root user运行salt 确保此用户有相应的权限； 可能需要手动创建salt运行需要的目录/etc/salt, /var/cache/salt, /var/log/salt, /var/run/salt。 Key Identity在initial key交换之前，Salt会提供命令来验证(validate)salt-master和salt-minion的身份。验证身份有助于避免疏忽地连接到错误的salt-master，并且在建立初始化连接的阻止MiTM攻击。 Master Key Fingerprint复制master.pub的值，并将其作为salt-minion配置文件/etc/salt/minion中master_finger的值。 12345678910#salt-key is used to manage Salt authentication keys#显示master的keysalt-key -F master#查看minion的finger#salt-key --finger &lt;minion_id&gt;dalt-key --finger '192.168.1.7' Minion Key Fingerprint 123456salt-call is used to execute module functions locally on a Salt Minion#查看minion key fingerprint#可在master上查看，比对两者是否相同salt-call --local key.finger Key ManagementSalt使用AES Encryption加密salt-master与salt-minion间的所有通信。这确保了发送到Minion的命令不会被篡改(tamper)，并保证了master与minion间是认证的和受信任的。 当命令发送到salt-minion之前，salt-minion的key必须要被salt-master所接受。 12#列出salt-master上已知的keyssalt-key -L 其中包含四项: Accepted Keys: Denied Keys: Unaccepted keys: Rejected keys: 让salt-master接收key，并允许salt-minion被salt-master控制 1234#-a 192.168.1.7, --accept=192.168.1.7#-A, --accept-allsalt-key -A Sending Commandssalt-master和salt-minion之间通过运行test.ping命令来证实(verified)。 123salt 192.168.1.7 test.pingsalt * test.ping Additional Installation GuidesSalt BootstrapSalt Bootstrap脚本允许用户在各种系统和版本上安装salt-minion和salt-master。shell脚本为bootstrap-salt.sh，运行一系列的检查来确定操作系统的类型和版本，然后通过适当的方法安装salt二进制文件。salt-bootstrap脚本安装运行salt的最小化安装包，如Git便不会安装。 Salt Bootstrap’s GitHub: https://github.com/saltstack/salt-bootstrap Example UsageSatl Bootstrap脚本有多种可以传递的选项，以及获取引导脚本本身的方法。 1. Using curl 12curl -o bootstrap-salt.sh -L https://bootstrap.saltstack.comsh bootstrap-salt.sh git develop 2. Using wget 12wget -O bootstrap-salt.sh https://bootstrap.saltstack.comsh bootstrap-salt.sh 3. An Insecure one-liner 12345curl -L https://bootstrap.saltstack.com | shwget -O - https://bootstrap.saltstack.com | shcurl -L https://bootstrap.saltstack.com | sh -s -- git develop 4. cmd line options 12#查看帮助sh bootstrap-salt.sh -h Openging the firewall up for saltsalt-master和salt-minion间的通信使用AES加密的ZeroMQ，它使用TCP的4505和4506端口，仅需要在salt-master上可访问就行。 下面概述了关于salt-master的防火墙规则： RHEL7/CENTOS7 12firewall-cmd --permanent --zone=&lt;zone&gt; --add-port=4505-4506/tcpfirewall-cmd --reload Preseed minion with accepted key某些情况下，在salt-master上接受minion-key之前等待salt-minion启动是不方便的。比如，你可能希望minion一上线(online)就引导。 有多种方式生成minion-key，下面是一般生成minion-key的四个步骤： 在salt-master上生成key： 12#请给key取个名字salt-key --gen-keys=[key_name] 把公钥(publick key)添加到已接受的salt-minion文件夹中: 公钥文件和 minion_id 有相同的名字是很有必要的，这就是Salt如何通过key与minions匹配。还有，由于不同操作系统或特定的master配置文件，pki 文件夹可能位于不同的位置。 1cp &lt;key_name&gt;.pub /etc/salt/pkimaster/minions/&lt;minion_id&gt; 分配minion-key： 对于minion来说，没有单一方法去得到密钥对，难点是找到一种安全的分配方法。 由于master已经接受了minion-key，因此分发私钥(private key)会有潜在的安全风险。 配置带key的minion： 你可能希望在启动salt-miniont daemon之前取得minion-key的位置。 12/etc/salt/pki/minion/minion.pem/etc/salt/pki/minion/minion.pub Running salt as normal user tutorial以普通用户(non-root)运行salt function 如果你不想使用root用户安装或运行salt，你可以在你的工作目录中创建一个虚拟根目录(virtual root dir)来配置它。salt system使用salt.syspathmodule来查找变量。 如果你运行salt-build，它会生成在: ./build/lib.linux-x86_64-2.7/salt/_syspaths.py； 运行python setup.py build命令来生成它； 复制生成的module到你的salt dir，cp ./build/lib.linux-x86_64-2.7/salt/_syspaths.py ./salt/_syspaths.py 修改它，并加入需要的变量和新路径： 1234567891011121314151617# you need to edit thisROOT_DIR = *your current dir* + '/salt/root'# you need to edit thisINSTALL_DIR = *location of source code*CONFIG_DIR = ROOT_DIR + '/etc/salt'CACHE_DIR = ROOT_DIR + '/var/cache/salt'SOCK_DIR = ROOT_DIR + '/var/run/salt'SRV_ROOT_DIR= ROOT_DIR + '/srv'BASE_FILE_ROOTS_DIR = ROOT_DIR + '/srv/salt'BASE_PILLAR_ROOTS_DIR = ROOT_DIR + '/srv/pillar'BASE_MASTER_ROOTS_DIR = ROOT_DIR + '/srv/salt-master'LOGS_DIR = ROOT_DIR + '/var/log/salt'PIDFILE_DIR = ROOT_DIR + '/var/run'CLOUD_DIR = INSTALL_DIR + '/cloud'BOOTSTRAP = CLOUD_DIR + '/deploy/bootstrap-salt.sh' 创建目录结构： 12mkdir -p root/etc/salt root/var/cache/run root/run/salt root/srvroot/srv/salt root/srv/pillar root/srv/salt-master root/var/log/salt root/var/run 填充配置文件： 123456cp -r conf/* /etc/salt/vi /etc/salt/masteruser: *your user name* 运行： 1PYTHONPATH=`pwd` scripts/salt-cloud Standalone minion因为salt-minion包含了如此广泛的功能，它可以独立运行。一个standalone minion可以用来做很多事情: 在没有连接到master的系统上使用salt-call命令； 无主状态(masterless states)。 当以无主模式运行salt时，不要运行salt-minion daemon。否则，它将尝试连接到master并失败。salt-call命令是独立的，不需要salt-minion daemon。 minion configuration有几个参考方法来设置不同的选项来配置masterless minion，salt-minion很容易通过配置文件(默认位于:/etc/salt/minion)进行配置。 告诉salt运行masterless salt-call命令用于在salt-minion本地运行模块功能，而不是在salt-master执行他们。通常，salt-call命令检查主机检索文件服务器和支柱数据，当时当运行standalone salt-call时，需要指示不要检查master的这些数据。为了指示minion不要查找master，需要在运行salt-call时设置file_client配置选项。默认情况下，file_client被设置为remote让minion知道将从master中收集文件服务器和支柱数据。当设置file_client为local时，minion将不会从master收集这些数据。 1234file_client: local#这样，salt-call命令将不会查找master#并认为本地系统拥有所有的文件文支柱资源 masterless运行状态 the state system在所有需要的文件都在minion本地，轻易地在没有salt-master的情况下运行。为了达到此效果，需要配置minion配置文件，以了解如何像master一样返回file_roots信息。 123file_roots: base: - /srv/salt 现在设置salt state tree, top file和SLS modules，就像在master上设置它们一样。将file_client设置为local，并且一个可用的state tree会调用state module中的function，将使用minion上的file_roots中的信息而不是master。 当在一个minion上创建一个state tree时，不需要语法或路径的更改。master上的SLS modules不需要进行任何修改就可以与minion一起工作。这就使得salt scrit不需要设置一个master就能轻易部署，并允许这些SLS modules随着部署发展而容易转移到master。 123456#以声明的状态可以执行salt-call state.apply#无需修改配置文件salt-call state.apply --local Salt masterless quickstart]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SaltStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2017%2F12%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[数据结构在计算机科学中，数据结构(data structure)是计算机中存储、组织数据的方式。大多数数据结构都有数列、记录、可辨识联合、引用等基本类型构成。 数据结构意味着结构和封装，一个数据结构可被视为两个函数之间的接口，或是由数据类型联合组成的存储内容的访问方法和封装。数据结构可通过程序语言所提供的数据类型、引用及其它操作加以实现。不同种类的数据结构适合不同种类的应用，部分数据结构甚至是为了解决特定问题而设计。一个涉及良好的数据结构，应该尽可能使用较少的时间与空间资源的前提下，支持各种程序运行。 正确选择数据结构可以提高算法的效率，在计算机程序设计里，选择适当的数据结构是一项重要工作。 常见数据结构 数组(Array); 栈(Stack): 后进先出，线性表； 队列(Queue): 先进先出，线性表； 链表(Linked List): 每个节点包括两部分，一个存储数据元素的数据域，另一个存储下一个节点地址的指针域； 树(Tree)； 图(Graph)； 堆(Heap): 一种动态树形结构； 散列表(Hash)； 数组(Array)数组数据结构，是由相同类型的元素的集合所组成，分配一块连续的内存来存储。利用数组元素的索引(index)可计算出元素对应存储地址。 数组有 一维数组、二维数组、多维数组、可变长数组…。 栈(Stack)堆栈又称为栈，是计算机科学中一种特殊的串列形式的抽象资料类别。其特殊之处在于只能允许在链接串列或阵列的一端(栈顶指标:top)，进行加入数据(push)和取出数据(pop)。 由于栈数据结构只允许在一端进行操作，因为按照后进先出(LIFO, last-in-first-out)的原理运行。 队列(Queue)队列，是先进先出(FIFO, first-in-first-out)的线性表。在具体应用中通常用链表或数组来实现。队列只允许在后端(Rear)进行插入操作，在前端(Front)进行删除操作。 链表(Linked List)链表是一种线性表，但并不按线性的顺序存储数据，而是在每一个节点里存到下一个节点的指针(Pointer)。由于不必须按顺序存储，链表再插入的时候可以达到 O(1)的时间复杂度，比另一种线性表顺序表快得多。但查找一个节点或访问特定节点则需要 O(n)的时间，而顺序表相应的时间复杂度分别是 O(logn)和O(1)。 是用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了节点的指针域，空间开销比较大。 链表有单向链表、双向链表、循环链表…。链表用来构建许多其它数据结构，如栈，队列和他们的派生。 树(Tree)树是一种抽象数据类型，用来模拟具有树状结构性质的数据集合。 树有有序树、无序树（二叉树，B树，霍夫曼树）。 图(Graph)在数学上，一个图是表示物体与物体之间的关系的方法，是图论的基本研究对象。 图有：有向图、无向图、简单图、多重图。 堆(Heap)堆是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵树的数组对象。在队列中，调度程序反复提取队列中的第一个作业并运行，因为实际情况中某些时间较短的任务将等待很长时间才能结束，或者某些不短小，但具有重要性的作业，同样应当具有优先权。堆即为解决此类问题设计的一种数据结构。 堆常用于排序，这种算法称作堆排序。 散列表(Hash)散列表也叫哈希表，是根据键(key)而直接访问在内存存储位置的数据结构。它通过计算一个关于键值的函数，将所需查询的数据映射到表中的一个位置来访问记录，这加快了查找速度。这种映射函数称为散列函数，存放记录的数组称为散列表。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB]]></title>
    <url>%2F2017%2F12%2F11%2FMongoDB%2F</url>
    <content type="text"><![CDATA[参考： MongoDB官方文档; MongoDB中文文档; https://zh.wikipedia.org/wiki/MongoDB; http://www.ywnds.com/?p=5635; https://www.centos.bz/2017/08/mongodb-secure-intro-user-auth/; http://www.03sec.com/3176.shtml; http://www.ywnds.com/?p=6502; http://wiki.jikexueyuan.com/project/the-little-mongodb-book/; 环境： CentOS7_x64； MongoDB3.4； NoSQLNoSQL(Not Only SQL)是对不同于传统的关系型数据库的数据库管理系统(DBMS)的统称。NoSQL不使用SQL作为查询语言，其数据结构可以不需要固定的表格模式，有横向可扩展性的特征。NoSQL用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无序多余操作就可以横向扩展。 关系型数据库的典型实现主要被调整用于执行规模小而读写频繁，或大批量极少写访问的事务。当代典型的关系型数据库在一些数据敏感的应用中表现了糟糕的性能。例如： 为巨量文档创建索引； 高流量网站的网页服务； 发送流媒体。 NoSQL数据库分类： 类型 栗子 特点 文档存储 MongoDB 用类似json的格式存储，存储的内容是文档型的。这样就有机会对某些字段建立索引，实现关系数据库的某些功能 图形关系存储 Neo4j 图形关系的最佳存储 键-值(ker-value)存储 最终一致性的键-值存储 架构性键-值存储 xxx 主机式服务 key-value硬盘存储 key-value RAM存储 MemcacheDB Redis 多数据库 OpenQM xxx 时序型数据库 Graphite xxx 对象数据库 ObjecStore 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据 列存储 HBase 顾名思义，按列存储数据。方便存储结构化和半结构化数据，方便做数据压缩，针对某一列或某几列的查询有很大的IO优势。 MongoDB简介 MongoDB(https://www.mongodb.com/)，是一种文档导向的数据库管理系统，由C++撰写而成，以此来解决应用程序开发社区中的大量现实问题。它是一种NoSQL。MongoDB支持的数据结构非常松散，是类似于json的bson格式，因此可以存储比较复杂的数据类型。MongoDB是一个开源文档数据库，提供高性能，高可用性和自动扩展。 预备知识： MongoDB中的database有和数据库一样的概念。一个MongoDB实例中，可以有零个或多个数据库，每个都作为一个高等容器，用于存储数据； MongoDB数据库中有零个或多个collections(集合)。集合类似于传统意义上的table(表)； MongoDB的集合是由零个或多个documents(文档)组成。文档类似于row(行)； MongoDB的文档由零个或多个fields(字段)组成。字段类似于columns(列)； MongoDB中Indexes(索引)扮演的角色与RDMS中一样； MongoDB中的Cursors(游标)很重要，当你向MongoDB取数据的时候，它会给你返回一个结果集的指针而不是真正的数据，这个指针我们叫它游标。我们可以用游标做任何事情，比如计数或跨行之类。 MongoDB特点不如这样认为，MongoDB是关系型数据库的一个代替案。比如用Lucene作为关系型数据库的全文检索索引的加强，或者是Redis作为持久性key-value存储。 无模式(Flexible Schema)：它不需要一个固定的模式，这使得他们比传统的数据库表要灵活更多。 写操作(Writes)：MongoDB可以胜任的一个特殊角色是在日志领域。有两点使得MongoDB的写操作非常快： 可以选择发送了写操作之后立刻返回，而无需等到操作完成； 可以控制数据持久性的写行为。 高性能(High Performance)：MongoDB提供了高性能的数据持久性。尤其是： 对嵌入式数据模型的支持减少了数据库系统上的I/O活动； 索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。 高可用(High Availability)：MongoDB的复制工具，称为副本集。提供：自动故障转移和数据冗余。 持久性(Durability)：在MongoDB中，日志(Journaling)是默认启动的，该功能允许快速恢复服务器，比如遭遇到了服务器奔溃或停电的问题。 丰富的查询语言(Rich Query Language)：MongoDB支持丰富的查询语言来支持读写操作(CRUD)，数据聚合(Data Aggregation)，全文搜索(Text Search)。 水平可伸缩性(Horizontal Scalability)：MongoDB提供了横向可伸缩性。 支持多个存储引擎(Support for Multiple Storage Engines)：在MongoDB3.2以后默认引擎为: WiredTiger Storage Engine，允许第三方为MongoDB开发存储引擎。 database和collectionMongoDB stores BSON documents. databasesIn MongoDB,databases hold collections of documents.如果一个数据库不存在，当你第一次存储数据时，MongoDB会自动创建数据库。这意味着可以切换到不存在的数据库。 默认情况下，集合不要求其文档具有相同的模式；文档不要求具有相同的字段集；字段的数据类型在集合的文档间可以有所不同。 123456789#select a dbuse &lt;db&gt;#create a dbuse newdbdb.newcoll.insert(&#123;name:'zhang'&#125;)db.newcoll.insert(&#123;filed01:'filed01', filed02:'filed02', filed03:'filed03', filed04:'filed04'&#125;)db.newcoll.insert(&#123;groups: ['A', 'B', 'C']&#125;)db.newcoll.find().pretty() collectionMongoDB stores documents in collections.collection类似于关系型数据库中的table。 12db.coll02.insert(&#123;x:1&#125;)db.coll03.createIndex(&#123;y:1&#125;) 显式创建(explicit creation)MongoDB提供了db.createCollection()方法来显式创建一个附带各种选项的集合。如设置document最大大小，文件验证规则等选项。如果不需要指定这些选项，就不需要使用显式创建集合，而直接向集合中插入数据即可。修改collection选项，使用collMod方法。 视图(View)视图的定义是公开的，视图的解释操作将包括定义视图的管道。因此，避免直接引用视图定义中的敏感字段和值。 创建/删除视图： 12345678910db.runCommand(&#123; crete: &lt;view&gt;, viewOn: &lt;source&gt;, pipeline: &lt;pipeline&gt;&#125;)db.createView(&lt;view&gt;, &lt;source&gt;, &lt;pipeline&gt;, &lt;collation&gt;)db.collection.drop() 视图行为： 视图存在以下行为： 视图只读，视图上的写操作将会出错； 视图使用底层集合的索引； 如果视图的基础集合被分割，视图也被认为可分割； 不能重命名视图； 视图上的字符串使用视图的默认排序规则。 限制集限制集是固定大小的集合支持基于文档插入顺序的高吞吐率的插入、检索、删除操作。限制收集工作在某种程度上类似于循环缓冲区：一旦一个文档填满分配给它的空间，它将通过在限制集中重写老文档来给新文档让出空间。 行为插入顺序限制集合能够保留插入顺序。因此，查询并不需要索引来保证以插入顺序来返回文档。减少了索引的消耗，限制集可以支持更高的插入吞吐量。 最旧文档的自动删除为了给新文档腾出空间，再不需要脚本或显示删除操作的前提下，限制集自动删除集合中最旧的文档。 例如replication set中的oplog.rs集合。考虑潜在用于集合封顶的用例： 存储高容量系统生成的日志信息。没有索引的情况下向限制集中插入文档的速度接近于直接在文件系统中写日志的速度； 在限制集中缓存少量的数据。 _id索引限制集合有一个_id字段并且默认在_id字段上创建索引。 限制和建议更新更新限制集中的文档，创建一个索引保证这些更新操作不需要进行集合扫描。 文档大小一个更新或替换操作改变了文档大小，操作将会失败。 文档删除不能从一个限制集中删除文档！为了从一个集合中删除所有文档，使用drop()方法来删除集合然后重新创建限制集。 分片不能对限制集分片。 查询效率用自然顺序监视限制集中大部分最近插入的文档。 程序创建一个限制集必须使用db.createCollection()方法创建限制集。且必须指定以字节为单位的最大集合大小。MongoDB将会预先分配集合。另外，可为限制集指定最大文档数据，用max字段。 大小参数是必须的。MongoDB会在达到最大限制前删除旧的文件。 1234567use &lt;db&gt;#限制集大小db.createCollection("log", &#123;capped: true, size: 1000000&#125;)#限制集和文档大小db.createCollection("log", &#123;capped: true, size: 5242880, max: 5000&#125;) 查询一个限制集如果没有对限制集指定排序，则MongoDB的结果顺序和插入顺序相同。 检查一个集合是否是限制集isCapped()方法 123db.collection.isCapped()#db.coll01.isCapped()#false 将集合转换为限制集convertToCapped()方法 123db.runCommand(&#123;"covertToCapped": "coll01", size: 1000000&#125;);#db.coll01.isCapped()#true 在规定的时间周期之后将自动移除数据通过设置MongoDB的TTL时集合中的数据过期。TTL collection与限制集不兼容。 Tailable游标类似于Unix中的taif -f documentMongoDB存储数据记录为BSON文档。BSON是JSON文档的二进制表示，因此它包含比JSON更多的数据类型。 document structureMongoDB字段由key-value对组成。字段值可以是任一BSON数据类型，包括其他文档，数组，阵列。 1234567891011121314151617181920212223&#123; filed1: value1; filed2: value2; ... filedN: valueN&#125;g#data typevar mydoc =&#123; _id: ObjectId("5099803df3f4948bd2f98391"), name: &#123; first: "Alan", last: "Turing" &#125;, birth: new Date('Jun 23, 1912'), death: new Date('Jun 07, 1954'), contribs: [ "Turing machine", "Turing test", "Turingery" ], views : NumberLong(1250000)&#125;_id是ObjectID；name是嵌入式文档；birth是日期类型；contribs是字符串数组；view是NumberLong类型。 字段名(field name)字段名是字符串。document对field name有以下限制: 字段名称_id保留用作主键(primary key)，它的值在collection中必须唯一，不可变。它的类型可以是数组外的任何类型； 字段名称不能以$字符开头； 字段名称不能包含.字符； 字段名称不能包含null字符。 BSON documents 可能有多个字段名称相同的字段。然而，大多数的MongoDB Interface，MongoDB结构（如hash表），并不支持重复字段名称。如果需要操作具有多个相同名称字段的文档，请参考 mongo driver。 一些由内部MongoDB进程创建的documents可能会有重复的字段，但是没有MongoDB进程会向一个已经存在的user document中添加重复字段。 字段值限制(field value limit)For indexed collections，indexed fields的值有一个最大索引值长度限制(maximum index key length)。 圆点表示法(dot notation)MongoDB使用圆点表示法来访问数组中的元素，访问嵌套文档中的字段。 数组(array)通过基于0的索引位置来指定或访问数组中的元素。 123456789&lt;array&gt;.&lt;index&gt;&#123; contribs: [ 'Turing machine', 'Turing test', 'Turingery' ]&#125;#contribs.0 == 'Turing machine'#contribs.1 == 'Turing test'#contribs.2 == 'Turingery' 嵌套文档(embedded documents)通过圆点表示法来指定或访问嵌套文档中的字段。 123456789&lt;embedded document&gt;.&lt;field&gt;&#123; name: &#123; first: 'AAA', last: 'ZZZ'&#125;, contact: &#123; phone: &#123; type: 'cell', number: '1-22-333' &#125;&#125;&#125;#name.first == 'AAA'#contact.phone.number == '1-22-333' 文档限制(document limitation)**文档大小限制(size limit)BSON document最大size为：16MB。 最大document size确保一个单一document不能使用过量的RAM，或是传输期间的过量带宽。MongoDB提供了GridFS API，用来保存超过最大size的文档。 文档字段序列(field order)MongoDB用write operation来作为document的序列，除了一下情况： _id字段总是document中的第一个field； 包含重命名的update操作，会导致document中的field重新排序。 _id字段在MongoDB中，每个保存在collection中的document都要求一个唯一的_id，用以担任主键(primary key)。如果向document中insert数据是忽略的_id字段，则MongoDB driver会为_id字段自动生成一个ObjectID。 1234567#默认生成_iddb.coll01.insert(&#123;name: 'zhang', sex: 'man', hobby: 'woman'&#125;)# "_id" : ObjectId("5a32166ebf2c986e8106f891")#自定义_iddb.coll01.insert(&#123;_id:'ZhangCustomDefine', name:'zhang', sex: 'man', arr: [0, 1, 2, 3], emmdoc: &#123;emm01:'Emm01', emm02: 'Emm02', emm03: 'Emmo3'&#125;&#125;)#"_id" : "ZhangCustomDefine" _id字段有以下行为和约束： 默认情况下，MongoDB在collection创建document时，会创建一个唯一的_id作为索引； _id字段总是document中的第一个字段。如果server接受的document中_id不在第一个字段，那么Server会移动_id到第一个字段； _id字段的数据类型除了数组外的任意BSON 数据类型； 不要存储BSON正则表达式的类型在_id字段中。 _id字段值的常用选项： 使用ObjectId； 使用了自然唯一的标识符，节省了空间并避免了额外的索引； 生成一个自动递增的数字； 在应用程序代码中生成UUID； 文档结构的其他用途查询过滤文档(query filter)使用:表达式来指定条件。 12345&#123; &lt;field1&gt;: &lt;value1&gt; &lt;field2&gt;: &lt;value2&gt; ...&#125; 更新特定文档(update)使用db.collection.update()操作更新数据。 BSON类型BSON是一个用来存储document和MongoDB进行远程调用的二进制序列化格式。BSON支持一下数据类型作为文档中的值。每个数据类型都有一个相应的数字和字符串别名，可与$type操作符一起使用，以便按照bson类型查询文档。 Type Number Alias double 1 “double” 字符串 2 “string” 对象 3 “object” 数组 4 “array” 二进制数据 5 “binData” 未定义 6 “undefined” ObjectId 7 “objectId” Boolean 8 “bool” 日期 9 “date” 空 10 “null” 正则表达式 11 “regex” DBPointer 12 “dbPointer” JavaScript 13 “javascript” 符号 14 “symbol” JavaScript(带范围) 15 “javascriptWithScope” 32位整数 16 “int” 时间戳 17 “timestamp” 64位整数 18 “long” Decimal128 19 “decimal” Min key -1 “minKey” Max key 127 — 如果你想要将BSON转换为JSON，参考Extended JSON。 ObjectIdObjcetIds are small, likely unique, fast to generate, and ordered.ObjectIds有12个字节组成，其中前4个字节是反映ObjectId创建的时间戳(timestamp)。 一个4字节的值，代表从Unix纪元开始的秒数； 一个3字节的机器标识符； 日期对象排在时间戳对象之前； MongoDB在比较过程中，会把一些类型看成相等。 栗子：{ &quot;_id&quot; : ObjectId(&quot;5a33354068b6c5e5fb6f213f&quot;), &quot;name&quot; : &quot;ZHANG&quot; }。 在mongo shell中，可以访问ObjectId的创建时间，使用ObjectId.getTimestamp()方法。在_id字段中存储的ObjectId值的排序，大致相当于按其创建时间排序。ObjectId的值顺序与生成时间之间并不严格。 字符串BSON字符串都是UTF-8编码。一般来说，每种编程语言的驱动程序在序列化和反序列化BSON的时候，都会从语言的字符串形式转化为UTF-8。这就使得使用BSON字符串简单存储大多数国际字符变为可能。 时间戳BSON有一个特殊的时间戳类型用于MongoDB内部使用，与普通的日期类型无关。而在应用开发中可使用BSON日期类型。时间戳值是一个64位的值： 前32位是与Unix纪元相差的秒数，后32位是在某秒总操作的一个递增的序列数。 在MongoDB复制集中，oplog有一个ts字段。这个字段的值使用BSON时间戳表示了操作时间。 1234db.coll02.insert( &#123; ts: new Timestamp() &#125; )db.coll02.find()#&#123; "_id" : ObjectId("5a333e3f68b6c5e5fb6f2141"), "ts" : Timestamp(1513307711, 1) &#125; 日期BSON日期是一个64位整数，表示利当前Unix新纪元(1970.01.01)的毫秒数，可到未来的2.9亿年。BSON日期类型是有符号的，负数表示1970年之前的时间。 123456var date1 = new Date()var date2 = ISODate()#date1#date2#ISODate(&quot;2017-12-15T03:28:08.227Z&quot;) MongoDB Extended JSONJSON只能表示BSON类型的一个子集。为了保留类型信息，MongoDB对JSON格式添加了如下扩展性： Strict mode： Any JSON parser can parse these strict mode representations as key/value pairs; mongo shell mode： The MongoDB internal JSON parser and the mongo shell can parse this mode. 多种数据类型的表示取决于JSON解析的上下文！ 解析器(parser)和支持的格式(format)Input in Strict mode如下可在严格模式下被解析并识别类型信息。 REST Interface; mongo import; –query; MongoDB Compass. Input in mongo shell mode如下可在严格模式下被解析并识别类型信息。 REST Interface; mongo import; –query; MongoDB Compass. Output in Strict modemongoexport, REST, HTTP Interfaces. Output in mongo shell modebsondump. BSON数据类型和关联表示Binary Strict mode mongo shell mode { “$binary”: ““, “$type”: ““ } BinData ( , ) 12&lt;bindata&gt;是二进制base64表示；&lt;t&gt;是由单字节的数据类型表示。 Date Strict mode mongo shell mode { “$date”: ““ } new Date ( ) 12In Strict mode, &lt;date&gt;是 ISO-8601的日期格式的时区字段，类型如**YYYY-MM-DDTHH:mm:ss.mm&lt;+/-offset&gt;;MongoDb JSON解析器目前暂不支持载入ISO-8601日期类型。 Timestamp Strict mode mongo shell mode { “$timestamp” ; { “t”: , “i” } } Timestamp( , ) 12&lt;t&gt;是32位无符号整数的JSON表现形式；&lt;i&gt;是增量的32位无符号整数。 Regular Expression Strict mode mongo shell mode { “$regex”: , “$options”: ““ } // 1234&lt;sRegex&gt;是有效地JSON字符串；&lt;jRegex&gt;是一个可能包含有效的JSON字符和未转义的双引号(&quot;)，但可能不包括未转义的斜杠(/)字符；&lt;sOptions&gt;是一个正则表达式选项；&lt;jOptions&gt;是一个只能包含字符&quot;g&quot;, &quot;i&quot;, &quot;m&quot;, &quot;s&quot;的字符串。 OID Strict mode mongo shell mode { “$oid”: ““ } ObjectId( ““ ) &lt;id&gt;是一个24字符的十六进制(hexadecimal)字符串 DB Reference strict mode mongo shell mode { “$ref”: ““, “$id”: ““ } DBRef(““, ““) 12&lt;name&gt;是一个有效的JSON字符；&lt;id&gt;是任一extended JSON type。 Undefined Type strict mode mongo shell mode { “$undefined”: true } undefined MinKey/MaxKey strict mode mongo shell mode { “$minkey”: 1 } MinKey { “$maxkey”: 1 } MaxKey NumberLong strict mode mongo shell mode { “$numberLong”: ““ } NumberLong( “number&gt;” ) 12Number是一个64位有符号整数。必须使用&quot;，否则它将被解释为浮点数，从而导致损失精度；db.json.insert&#123;&#123; longquoted: NumberLong(&quot;12345678901234345&quot;) &#125;) MongoDB安装参考: https://docs.mongodb.com/manual/administration/install-on-linux/; https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/; MongoDB有社区版(Community)和企业版(Enterprise)。社区版免费，企业版在商业方面收费。 MongoDB在仓库中提供官方支持的包，包含以下软件包： Package Description monogdb-org 将自动安装下面四个组件包 mongodb-org-server 包含mongod守护进程和相关配置和init脚本 mongodb-org-mongos 包含mongos守护进程 mongodb-org-shell 包含mongo-shell mongodb-org-tools 包含相关MongoDB工具，如mongoimport,mongoexport,mongodump,mongorestore… mongodb-org-server包提供了一个/etc/mongod.conf配置文件来开始和初始化mongod。默认配置文件默认bind_ip为 127.0.0.1，当你有需要和副本集时请修改它。 自建mongodb.repo仓库安装仓库地址：https://repo.mongodb.org 12345678910111213vim /etc/yum.repos.d/mongodb34.repo#编辑仓库[mongodb34]name=MongoDB34 Repositorybaseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.4/x86_64/gpgcheck=0enabled=1#安装mongodbyum install -y mongodb-org 下载rpm包安装1234567cd /root/mongodbwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/3.4/x86_64/RPMS/mongodb-org-3.4.10-1.el7.x86_64.rpmwget https://repo.mongodb.org/yum/redhat/7/mongodb-org/3.4/x86_64/RPMS/mongodb-xxx-3.4.10-1.el7.x86_64.rpm#共五个包yum ./mongo-org* 源码安装1234567wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.6.0.tgztar -axvf mongodb-linux-x86_64-rhel70-3.6.0.tgz -C ./#默认路径/usr/localmake &amp;&amp; make install 开启mongodb123456789101112131415161718192021#默认启动方式systemctl start mongod#指定配置文件启动#注意修改配置文件里面的某些路径和名称，不然会和默认配置文件冲突mongod -f /etc/mongo_27018.confmongod -f /etc/mongo_27019.conf``&lt;br&gt;## 卸载mongodb```shsystemctl stop mongodyum remove $(rpm -qa | grep mongodb-org)rm -rf /var/log/mongodbrm -rf /var/dbpath/mongo mongodb异常关闭后12345#首先查看日志文件tail /var/log/mongodb/mongod.log#删除rm /var/run/mongodb/mongod.pid /var/db/mongodb/mongod.lock MongoDB配置文件MongoDB的配置文件格式使用了YAML格式。YAML维基百科，Yet Another Markup Language。强调以数据为中心，而不是标记语言为重点，用方向缩略语重命名。 默认配置文件/etc/mongod.conf 的几个大块： 123456789101112131415161718192021systemLog: #日志storage: #存储processManagement: #进程管理net: #网络security: #安全operationProfiling: #性能分析器replication: #主从复制sharding: #架构setParameter: #自定义变量auditLog: #检测日志snmp: #简单网络管理协议 systemLog日志相关参数： 123456789101112131415systemLog: verbosity: &lt;int&gt; #日志级别，默认0,1-5均会包含debug信息 quiet: &lt;boolean&gt; #安静，true时mongod将会减少日志的输出量 traceAllExceptions: &lt;boolean&gt; #打印异常详细信息 syslogFacility: &lt;string&gt; #指定用于登录时信息到syslog Facility水平，前提是启用syslog path: &lt;string&gt; #日志路径，默认情况下，MongoDB将覆盖现有的日志文件 logAppend: &lt;boolean&gt; #mongod重启后，在现有日志后继续添加日志，否则备份当前日志，然后创建新日志 logRotate: rename|reopen #日志轮询，防止一个日志文件特别大。rename重命名日志文件，默认值；reopen使用Linuxrotate特性，关闭并重新打开日志文件，前提为logAppend: true destination: &lt;string&gt; #日志输出目的地，可为file或syslog，若不指定，则会输出到 std out timeStampFormat: &lt;string&gt; #指定日志格式的时间戳，有 ctime, Iso869-utc, iso8691-local component: #为不同的组件指定各自的日志信息级别 accessControl: verbosity: &lt;int&gt; command: verbosity: &lt;int&gt; storage存储引擎相关参数: 123456789101112131415161718192021222324252627282930313233storage: dbPath: &lt;string&gt; #mongodb进程存储数据目录，此配置进队此mongod进程有效，你使用配置文件开启的mongod就可以指定额外的数据目录 indexBuildRetry: &lt;boolean&gt; #当构件索引时mongod意外关闭，那么在此启动是否重建索引，默认true repairPath: &lt;string&gt; #在repair期间使用此目录存储临时数据，repair结束后此目录下数据将被删除 journal: enabled: &lt;boolean&gt; #journal日志持久存储，journal日志用来数据恢复，通常用于故障恢复，建议开启 commitIntervalMs: &lt;num&gt; #mongod日志刷新值，范围1-500毫秒，默认100，不建议修改 directoryPerDB: &lt;boolean&gt; #是否将不同的数据存储在不同的目录中，dbPath子目录 syncPeriodSecs: &lt;int&gt; #fsync操作将数据flush到磁盘的时间间隔，默认为60秒，不建议修改 engine: &lt;string&gt; #存储引擎 mmapv1: #mmapv1存储引擎，3.2前默认 preallocDataFiles: &lt;boolean&gt; nsSize: &lt;int&gt; quota: enforced: &lt;boolean&gt; maxFilesPerDB: &lt;int&gt; smallFiles: &lt;boolean&gt; journal: debugFlags: &lt;int&gt; commitIntervalMs: &lt;num&gt; wiredTiger: #WiredTiger存储引擎，3.2后默认 engineConfig: cacheSizeGB: &lt;number&gt; #最大缓存大小 journalCompressor: &lt;string&gt; #日志压缩算法，可选值有 none，snappy(默认)，zlib directoryForIndexes: &lt;boolean&gt; #是否将索引和collections数据分别存储在dbPath单独的目录中 collectionConfig: blockCompressor: &lt;string&gt; #collection数据压缩算法，可选none, snappy，zlib indexConfig: prefixCompression: &lt;boolean&gt; #是否对索引数据使用前缀压缩。对那些经过排序的值存储有很大帮助，可有效减少索引数据的内存使用量。 inMemory: #inMemory内存存储引擎，bate版 engineConfig: inMemorySizeGB: &lt;number&gt; processManagement进程相关参数: 123processManagement: fork: &lt;boolean&gt; #是否以fork模式运行mongod进程，默认情况下，mongod不作为守护进程运行 pidFilePath: &lt;string&gt; #将mongod进程ID写入指定文件，如未指定，将不会创建PID文件 net网络相关参数: 123456789101112131415161718192021222324252627282930net: prot: &lt;int&gt; #监听端口，默认27017 bindIp: &lt;string&gt; #绑定IP，如果此值是“0.0.0.0”则绑定所有接口 maxIncomingConnections: &lt;int&gt; #mongod进程允许的最大连接数，如果此值超过系统配置的连接数阈值，将不会生效(ulimit) wireObjectCheck: &lt;boolean&gt; #当客户端写入数据时，检查数据的有效性（BSON）。如果数据格式不良，update,insert等操作将会被拒绝 ipv6: &lt;boolean&gt; #是否支持多实例之间使用ipv6 unixDomainSocker: #适用于Unix系统 enabled: &lt;boolean&gt; pathPrefix: &lt;string&gt; filePermissions: &lt;int&gt; http: # enabled: &lt;boolean&gt; JSONEnabled: &lt;boolean&gt; RESTInterfaceEnabled: &lt;boolean&gt; ssl: sslOnNormalPorts: &lt;boolean&gt; mode: &lt;string&gt; PEMKeyFile: &lt;string&gt; PEMKeyPassword: &lt;string&gt; clusterFile: &lt;string&gt; clusterPassword: &lt;string&gt; CAFile: &lt;string&gt; CRLFile: &lt;string&gt; allowConnectionsWithoutCertificates: &lt;boolean&gt; allowInvalidCertificates: &lt;boolean&gt; allowInvalidHostnames: &lt;boolean&gt; disabledProtocols: &lt;string&gt; FIPSMode: &lt;boolean&gt; compression: compressors: &lt;string&gt; security安全相关参数: 1234567891011121314151617181920212223242526272829303132333435security: authorization: enabled #MondoDB认证功能 keyFile: /path/mongo.key #MongoDB副本集节点身份验证密钥文件 clusterAuthMode: &lt;string&gt; #集群members间的认证模式 transitionToAuth: &lt;boolean&gt; javascriptEnabled: &lt;boolean&gt; #是否允许执行JavaScript脚本 redactClientLogData: &lt;boolean&gt; sasl: hostName: &lt;string&gt; serviceName: &lt;string&gt; saslauthdSocketPath: &lt;string&gt; enableEncryption: &lt;boolean&gt; encryptionCipherMode: &lt;string&gt; encryptionKeyFile: &lt;string&gt; kmip: keyIdentifier: &lt;string&gt; rotateMasterKey: &lt;boolean&gt; serverName: &lt;string&gt; port: &lt;string&gt; clientCertificateFile: &lt;string&gt; clientCertificatePassword: &lt;string&gt; serverCAFile: &lt;string&gt; ldap: servers: &lt;string&gt; bind: method: &lt;string&gt; saslMechanism: &lt;string&gt; queryUser: &lt;string&gt; queryPassword: &lt;string&gt; useOSDefaults: &lt;boolean&gt; transportSecurity: &lt;string&gt; timeoutMS: &lt;int&gt; userToDNMapping: &lt;string&gt; authz: queryTemplate: &lt;string&gt; operationProfiling慢查询相关参数： 1234operationProfiling: slowOpThresholdMs: &lt;int&gt; #数据库profiler判定一个操作是“慢查询”的时间阈值，单位毫秒。mongod会把慢查询记录到日志中，默认100ms mode: &lt;string&gt; #数据库profiler级别，操作的性能信息将会被写入日志文件中，可选值“off”--关闭profiling，“slowOp”--只包包含慢操作，“all”--记录所有操作 #数据库profiling会影响性能，建议只在性能调试阶段开启 replication副本集： 12345replication: oplogSizeMB: &lt;int&gt; #replication操作日志的最大尺寸，如果太小，secondary将不能通过oplog来同步数据，只能全量同步 replSetName: &lt;string&gt; #副本集名称，副本集中所有的mongod实例都必须有相同的名字，Sharding分布式下，不同的sharding应该使用不同的repSetName secondaryIndexPrefetch: &lt;string&gt; #副本集中的secondary，从oplog中应用变更操作之前，将会先把索引加载到内存 enalbeMajorityReadConcern: &lt;boolean&gt; #允许readConcern的级别为“majority” sharding分片相关参数： 123sharding: clusterRole: &lt;string&gt; #在sharding集群中，此mongod实例可选的角色。configsvr,默认监听27019端口 和 shardsvr,默认监听27018端口 archiveMovedChunks: &lt;boolean&gt; #当chunks因为“负载均衡”而迁移到其他节点时，mongod是否将这些chunks归档，并保存在dbPath/movechunk目录下，mongod不会删除moveChunk下的文件 setParameter自定义变量： 1234setParameter: &lt;parameter1&gt;: &lt;value1&gt; &lt;parameter2&gt;: &lt;value2&gt; enableLocalhostAuthBypass: false #栗子 auditLog审计相关参数： 12345auditLog: destination: &lt;string&gt; #指定审计记录的输出方式，有syslog, console, file format: &lt;string&gt; #输出格式，有JSON 和 BSON path: &lt;string&gt; #如果审计时间输入为文件，那么就需要指定文件完整路径及文件名 filter: &lt;string&gt; #过滤器，可限制审计系统记录的操作类型，该选项需要一个表单的查询文档的字符串表示形式 Mongo Shellmongo shell是一个交互式的JavaScript结构的MongoDB。使用mongo shell来查询和更新数据以及执行管理操作。 mongo shell基础知识启动monso shell启动mongo shell前确保MongoDB实例正在运行。 1234567891011121314mongo [option] [db address] [.js]#以默认配置启动mongo#以特定配置启动mongo --port 27018#连接远程mongo shellmongo --host $host --port $port -u $user -p $passwdmongo &lt;db&gt;mongo &lt;host&gt;/&lt;db&gt;mongo &lt;hsot:port&gt;/&lt;db&gt; .mongorc.js文件mongo shell开始运行时，mongo将在用户主目录下检查.mongorc.js的js文件。如果找到，mongo将在首次命令行之前解释执行.mongorc.js的内容。如果你使用mongo shell执行一个js或表达式，无论是通过mongo --eval，或指定一个.js文件，mongo都将在js处理完成之后读取.mongorc.js文件。可使用 --norc选项禁止加载.mongorc.js。 12ll /root/.mongorc.js# -rw------- 1 root root 0 Dec 27 2016 /root/.mongorc.js 使用mongo shell可能在启动mongo shell的时候会警告: WARNING: /sys/kernel/mm/transparent_hugepage/defrag is ‘always’. We suggest setting it to ‘never’. WARNING: /sys/kernel/mm/transparent_hugepage/defrag is ‘always’. We suggest setting it to ‘never’ WARNING: Access control is not enabled for the database. hugepage(大内存页面)，是Linux操作系统一种管理内存的方式。和通常方式相比，hugepage模式下内存分配管理会有所差异。MongoDB显然不希望这个特定被启用。新版MongoDB增加了安全性设计，推荐用户创建使用数据库时进行验证。所以我们需要创建用户认证。 关闭hugepage: 123456vim /etc/rc.d/rc.localecho never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defragchmox a+x /etc/rc.d/rc.local 创建用户认证: 1234567891011121314&gt;use admin&gt;db.createUser(&#123; user: "zhang", pwd: "zhang", roles: [&#123; role: "root", db: "admin"&#125;]&#125;)mongo -u zhang -p zhang --authenticationDatabase admin#或mongouse admindb.auth("zhang", "1314520") 12345678910mongo#显示当前使用数据库&gt;db#切换数据库&gt;use &lt;database&gt;#查看所有数据库&gt;show dbs 你可以切换到一个并不存在的数据库。当你第一次向数据库存储数据，如创建一个集合，MongoDB将自动创建数据库。 123use nodbdb.nocollestion.insert(&#123;x:1&#125;); 格式化打印结果db.collection.find()方法返回一个cursor(游标)。如果返回的游标未使用var关键字指定变量，则游标将自动迭代最多20次，以打印出与查询匹配的前20个documents 1234567#在操作中添加`.pretty()`，以格式化打印结果#使用.pretty显示结果很舒服db.collection.find().pretty()print() #无格式打印printjson() #用JSON打印 mongo shell中的多行操作mongo shell中如果你以( , { , [开始，那么知道你输入了对应的) , } , ]才算结束命令。 Tab命令补全和键盘快捷键mongo shell支持键盘快捷键，例如： 使用 上/下箭头 进行历史命令切换； 使用 Tab键 自动补全命令。 mongo shell批量操作123456mongo -u xxx -p xxx --authenticationDatabase=xxx &lt;&lt; EOFshow dbsuse zhangdb.coll01.drop()db.coll02.update( &#123; _id: "xxx" &#125;, &#123; name: "zhang" &#125;)EOF 退出mongo shell12345quit()exitCtrl+c 配置mongo shell可在mongo shell中设置变量prompt的值来修改提示符内容。prompt变量可以存储字符串以及JavaScript代码。 也可以在.mongorc.js文件中增加提示符的逻辑操作来设置每次启动mongo shell的提示符。 自定义提示符自定义提示符展示操作符： 在mongo shell中定义一下变量。 12345678910cmdCount = 1;prompt = function() &#123; return (cmdCount++) + '&gt; ';&#125;#效果1&gt;2&gt;... 自定义提示符显示数据库和主机名： 形式为：@$ 12345678host = db.serverStatus().host;prompt = function() &#123; return db+'@'+host+'$'&#125;#效果test@localhost$ 自定义提示符展示服务器启动时间和文档数： 1234567prompt = function() &#123; return 'Uptime:' + db.serverStatus().uptime + 'Documents:' + db.stats().objects + '&gt; ';&#125;#效果Uptime:1234 Documents:5 &gt; 注意：在mongo shell里面定义的prompt变量知识临时生效的，退出shell后便没有。如果想要当前用户永久生效，可写入~/.mongorc.js文件。则此用户每次启动mongo shell前都会执行这个文件。 123456vim ~/.mongorc.jshost = db.serverStatus().host;prompt = function() &#123; return db+"@"+host+"&gt; "; &#125; 在mongo shell中使用外部编辑器可在启动mongo shell之前设置EDITOR环境变量来在mongo shell中使用自己的编辑器。 12345678910111213export EDITOR=vimmongo#edit &lt;variable&gt;|&lt;function&gt;function myfunc()&#123;&#125;edit myfunc#此时是edit使用vim编辑myfuncfunction myfunc()&#123; print("It was edited by vim!")&#125;myfunc() 修改mongo shell批处理大小db.collection.find()是一种JavaScript方法，返回一个cursor(游标)。如果返回的游标未使用var关键字指定变量，则游标将自动迭代最多20次，以打印出与查询匹配的前20个documents。可以设置DBQuery.shellBatchSize属性来修改默认20篇文档。 1DBQuery.shellBatchSize = 10; 获取mongo shell帮助合理运用Tab键补全命令！ 1234567891011121314151617181920212223242526###命令行帮助mongo --help###mongo shell里查看帮助列表help###数据库帮助#db.&lt;method&gt;show dbsdb.help()###集合帮助#db.&lt;collection&gt;.&lt;method&gt;show collectionsdb.collections.help()###游标帮助db.collection.find().help()###封装对象帮助help misc 给mongo shell写脚本可使用JavaScript为mongo shell编写脚本，用于处理MongoDB中的数据或执行管理操作。 打开新连接在mongo shell或JavaScript文件中，可使用Mongo()构造函数来实例化数据库连接： 12345678910111213141516171819new Mongo()new Mongo(&lt;host&gt;)new Mongo(&lt;host:port&gt;)#栗子conn = new Mongo();db = conn.getDB('mydb'); #将全局db变量设置为mydb#连接db = connect('localhost:27017/mydb');#认证db.auth(&lt;user&gt;, &lt;passwd&gt;)db.auth(&#123; user: &lt;user&gt;, pwd: &lt;passed&gt;&#125;) 交互式和脚本化mongo的区别mongo shell中的帮助与JavaScript中帮助不一样！ mongo shell帮助 JavaScript等量 show dbs db.adminCommand(‘listDatabases’) use db = db.getSiblingDB(‘‘) show collections db.getCollectionNames() show users db.getUsers() show log db.adminCommand({‘getLog’ : ‘‘}) 脚本使用mongo shell来计算JavaScript的值。 –eval mongo执行 --eval后的js命令 1mongo test --eval &quot;printjson(db.getCollectionNames())&quot; 执行JavaScript文件 12345mongo localhost:27017/test myjs.js#在shell中执行.js&gt;load("myjs.js")&gt;loca("/root/mongo/myjs.js") mongo shell中的数据类型MongoDB BSON提供了除JSON之外的其它数据类型的支持。Driver提供了对这些数据类型在主机语言的本地化支持，mongo shell也提供了一些帮助类来支持这些数据类型在mongo javascript shell中的使用。 日期mongo shell提供了多种方法返回日期: Date() 方法返回当前日期为一个字符串； new Date() 构造函数返回一个使用ISODate()包装返回的Date对象； ISODate() 构造函数返回一个使用ISODate()包装返回的Date对象。 返回一个日期为字符串： 123var myDateString = Date();#查看变量值myDateString 验证类型： 12typeof myDateString()#string 返回Date： 123456var myDate = new Date();myDate#ISODate(&quot;2017-12-12T08:43:31.405Z&quot;)#验证myDate instanceof Date ObjectIdmongo shell对objectid数据类型提供objectId()包装类。 new ObjectId NumberLongmongo shell默认将所有数字处理为浮点值。 用numberlong()包装来处理64位整数。 1NumberLong("2090845886852") NumberInt用NumberInt()构造函数来显式指定32位整数。 NumberDecimalmongo shell默认将所有的数字处理为64位浮点的double值。mongo shell提供了NumberDecimal()构造函数限制指定128位基于十进制的浮点值，能够以精确的精度仿效十进制近似值。这个功能在金融、税务以及科学计算等方面应用。 12&gt;NumberDecimal('1000.55')#强烈建议加上引号，没加引号可能会存在精度丢失的情况 ### 在mongo shell中检查类型 instanceof返回一个bool值来验证一个值是否为某些类型的实例。 12mydoc._id instanceof ObjectId#true typeof返回一个字段的类型。 12typeof mydoc._id#object mongo shell快速参考mongo shell 历史命令mongo shell历史命令保存在~/.dbshell文件中，cat ~/.dbshell。也可以使用上/下键切换历史命令。 命令行选项 option description --help 显示命令行选项 --nodb 启动mongo shell而不连接到数据库 --shell 执行文件后运行mongo shell mongo shell命令助手 help methods and commands description help 显示帮助 db.help 显示数据库方法的帮助 db.collection.help() 显示集合方法的帮助 show dbs 打印服务器上的所有数据库列表 show databases 打印所有可获取的数据库列表 use &lt;db&gt; 切换数据库 show collections 打印当前数据库上的所有集合列表 show users 打印当前数据库的用户列表 show roles 打印当前数据库的所有角色(user-define and built-in)列表 show profile 打印花费1ms或更多时间的五个最近的操作 load() 在shell中执行一个JavaScript文件，建议使用绝对路径 mongo shell的基本JavaScript操作mongo shell为数据库操作提供了一个JavaScript API。db引用当的是前数据库的变量。 JavaScript db-operation description db.auth() 在安全模式下认证用户 coll = db.&lt;collection&gt; 将当前db中的特定collection设置为coll，可在此变量上执行操作，如coll.find(); db.collection.find() 查找集合中的所有文档，并返回一个游标 db.collection.insert() 插入一个新文档到集合中 db.collection.update() 更新集合中一个存在的文档 db.collection.save() 插入或更新 集合中的文档 db.collection.remove() 从集合中删除文档 db.collection.drop() 删除整个集合 db.collection.createIndex() 在集合中创建索引 db.getSiblingDB() 跨数据库查询 键盘快捷键 keysrtoke function Up/Down arrow 前/后 历史命令 Left/Right arrow 左右移动 Home/End 行首/行尾 Tab 自动补全 ctrl+c 退出 ctrl+L 清屏 mongo shell查询方法在mongo shell中，使用find()和findOne()方法执行读操作。 read-operations description db.collection.find(&lt;query&gt;) 查找集合中与匹配的文档，如果未指定或为空，则读取操作会选择集合中的所有文档 db.collection.find(&lt;query&gt;, &lt;projection&gt;) 查找与匹配的文档，返回特定字段 db.collection.find().sort(&lt;sort order&gt;) 返回排序结果 db.collection.find(&lt;query&gt;).sort(&lt;sort order&gt;) 返回匹配和排序结果 db.collection.find(...).limit(&lt;n&gt;) 限制输出结果为行 db.collection.find().pretty().limit() 匹配，格式化，限制输出 db.collection.find().limit().pretty() 同上 db.collection.find(...).skip(&lt;n&gt;) 跳过前行 db.collection.count() 返回集合中文档总数 db.collection.find().count() 返回匹配文档总数 db.collection.findOne(&lt;query&gt;) 查找并返回单一的文档，null表示未找到 管理命令助手 js db-administrative-methods description db.cloneDatabase(&lt;host&gt;) 从指定主机克隆当前数据库，noauth mode db.copyDatabase(&lt;from&gt;, &lt;to&gt;, &lt;host&gt;) copy db to db db.fromColl.renameCollection(&lt;toColl&gt;) rename collection db.repairDatabase() 修复当前db db.dropDatabases() 删除当前数据库 打开附加连接可以在mongo shell中创建一个新连接。 12345&gt;db = connect("&lt;host&gt;:&lt;port&gt;/&lt;db&gt;")#db = connect("192.168.1.11/admin")&gt;conn = new Mongo()&gt;db = conn.getDB("dbname") MongoDB CRUD操作CRUD操作就是创建(create)，读取(read)，更新(update)，删除(delete)文档(document)! 创建(create)操作创建或插入， 即是向 collection 添加新的 document。如果插入时集合不存在，插入操作会创建该集合。 123db.collection.insert()db.collection.insertOne()db.collection.insertMany() 读取(read)操作读操作，获取 collection 中的 document。 1db.collection.find() 更新(update)操作更新操作，修改 collection 中已经存在的 document。 1234db.collection.update()db.collection.updateOne()db.collection.updateMany()db.collection.replaceOne() 删除(delete)操作删除操作，是从一个 collection 中删除 document 的操作。 123db.collection.remove()db.collection.deleteOne()db.collection.deleteMany() 插入文档(Insert) 插入方法MongoDB提供了如下插入方法向collection中插入document： db.collection.insert(), 向集合中插入一个或多个文档; db.collection.insertOne(), 向集合中插入一个文档; db.collection.insertMany(), 向集合中插入多个文档. db.collection.insert()db.collection.insert(),向collection中插入一个或多个document。要想插入一个document，传递一个文档给该方法；要想插入多个documents，传递文档数组给该方法。 12345678910111213141516171819#插入一个文档db.user.insert( &#123; _id: "ZhangTest", name: "zhang", age: 2017, sex: "man" &#125;)#插入多个文档db.user.insert( [ &#123; name: "AAA", age: 20, status: "A" &#125;, &#123; name: "BBB", age: 21, status: "B" &#125;, &#123; name: "CCC", age: 22, status: "C" &#125; ]) db.collection.insertOne()db.collection.insertOne(),向collection中插入单个document。 12345678910db.user.insertOne( &#123; name: "zhang", age: "2017", sex: "man", education: "bachelor" &#125;)#此处并未自定义_id字段，因此它会自动添加_id字段 db.collection.insertMany()db.collection.insertMany(),向collection插入多个documents。 123456789db.user.insertMany( [ &#123; name: "AAA", age: "20", status: "A" &#125;, &#123; name: "BBB", age: "21", status: "B" &#125;, &#123; name: "CCC", age: "22", status: "C" &#125; ])#自动生成3个document的_id字段 插入操作的行为表现创建集合插入的时候如果collection不存在，那么插入操作会创建collection。 _id字段在MongoDB中，存储于collection中的每一个document都需要一个唯一的_id字段作为primary_key。如果一个插入的document操作遗漏了_id字段，则MongoDB driver会自动生成一个ObjectId。 原子性MongoDB中所有的写操作在单一文档层级上是原子的。 查询文档(Read)MongoDB提供了db.collection.find()方法从collection中读取document。 1234db.collection.find( &lt;query filter&gt;, &lt;projection&gt; )#&lt;query filter&gt;指明返回哪些document#&lt;projection&gt;指明返回匹配document的那些filed 示例12345678910111213141516171819202122232425db.user.insertMany( [ &#123; _id: 1, name: "A", favorites: &#123; artist: "Picasso", food: "pizza" &#125;, finished: [ 11, "AA" ], points: [ &#123; points: 85, bonus: 30 &#125;, &#123; points: 85, bonus: 10 &#125; ] &#125;, &#123; _id: 2, name: "B", favorites: &#123; artist: "Miro", food: "merigue" &#125;, finished: [ 22, "BB" ], points: [ &#123; points: 85, bonus: 20 &#125;, &#123; points: 64, bonus: 12 &#125; ] &#125;, &#123; _id: 3, name: "C", favorites: &#123; artist: "Gaogeng", food: "cake" &#125;, finished: [ 33, "CC" ], points: [ &#123; points: 67, bonus: 8 &#125;, &#123; points: 55, bonus: 21 &#125; ] &#125; ]) 查询和规划操作符Comparison: 1234567$eq$gt$gte$lt$ne$in$nin Logical： 1234$or$and$not$nor Element: 12$exists$type Evaluation: 1234$mod$regex$text$where Geospatial: 1234$geoWithin$geoIntersects$near$nearSphere Array: 123$all$eleMatch$size Bitwise: 1234$bitsAllSet$bitsAnySet$bitsAllClear$bitsAnyClear Comments: 1$comment Projection Operators: 1234$$eleMatch$meta$slice 选择collectino中所有document一个空的query filter会选择集合汇总所有文档。 12db.users.find(&#123;&#125;)db.user.find() 指定查询过滤条件1. 指定等于条件 1234&#123; &lt;field1&gt;: &lt;value1&gt;, ...&#125;#栗子db.user.find( &#123; name: &quot;C&quot; &#125; ) 2. 使用查询操作符指定条件 1234&#123; &lt;field1&gt;: &#123; &lt;operator1&gt;: &lt;value1&gt; &#125;, ... &#125;#栗子db.user.find( &#123; name: &#123; $in: [ &quot;A&quot;, &quot;B&quot; ] &#125; &#125; ) 3. 指定逻辑查询条件条件逻辑查询(AND, OR, NOT)。符合查询可以在集合文档的多个字段上指定条件。 123456789101112131415#ANDdb.user.find( &#123; name: &quot;A&quot;, age: &#123; $lt: 30&#125; &#125; )#ORdb.user.find( &#123; $or: [ &#123; name: &quot;A&quot; &#125;, &#123; age: &#123; $lt: 30 &#125; &#125; ]&#125; )#AND和ORdb.user.find( &#123; name: &quot;A&quot;, $or: [ &#123;age: &#123; $lt: 30 &#125; &#125;, &#123; type: 1 &#125; ]&#125; ) 嵌入式文档的查询当字段中包含嵌入文档时，查询可以指定嵌入文档中的精确匹配或使用圆点(.)表示法对嵌入文档的单个字段指定匹配。 12345678#精确匹配db.user.find(&#123; favorites: &#123; artist: &quot;Picasso&quot;, food: &quot;pizza&quot; &#125;&#125;)#圆点.表示法，记得加引号db.user.find( &#123; &quot;favorites.artist&quot;: &quot;Picasso&quot; &#125; ) 数组上的查询当字段包含数组，可查询精确的匹配数组或数组中特定的值。如果数组包含嵌入文档，可使用圆点表示法查询内嵌文档中特定的字段。 123456789101112131415161718192021222324252627#精确匹配db.user.find(&#123; finished: [ 11, &quot;AA&quot; ] &#125;)#匹配一个数组元素，会显示整个文档db.user.find(&#123; finished: &quot;BB&quot; &#125;)#匹配数组中指定元素，会返回整个文档db.user.find(&#123; &quot;finished.1&quot;: &quot;CC&quot; &#125;)#指定数组中的多个查询条件db.user.find(&#123; finished: &#123; $elemMatch: &#123;$gte: 11, $lt: 33&#125; &#125; &#125;)db.user.find(&#123; finished: &#123; $gt: 11, $lt: 33 &#125; &#125;)#嵌入文档数组db.user.find(&#123; &apos;points.points&apos;: &#123;$lte: 80 &#125; &#125;)db.user.find(&#123; &quot;points.0.points&quot;: &#123;$lte: 80&#125; &#125;)#元素组合满足查询条件db.user.find(&#123; &quot;points.points&quot;: &#123;$lte: 80&#125;, &quot;points.bouns&quot;: 20&#125;) 返回查询的映射字段默认地，MongoDB中的查询返回匹配文档中的所有字段。为了限制MongoDB发送给应用的数据量，我们可以在查询操作中包括一个projection文档。 映射文档映射文档限制了返回所有匹配文档的字段。映射文档可以致命包括哪些字段或排除哪些字段。这个就很不错了，可以过滤掉我们不需要的信息。 12345db.users.find( &#123;name: &quot;AAA&quot;&#125; ,&#123;_id: 0, name: 1, age: ture&#125; )db.user.find( &#123; name: &quot;BBB&quot;&#125;, &#123;_id: false&#125; )1或true，表示在返回的文档中包含字段；0或false，排除该字段； 更新文档(Update)更新方法： db.collection.updateOne(), 更新一个文档 db.collection.updateMany(), 更新多个文档 db.replaceOne(), 替换一个文档 db.collection.update(), 更新或替换一个文档 更新的行为表现 原子性：MongoDB中所有的写操作在单一文档层级上是原子的。 _id字段：不能更新_id字段的值，也不能用不同_id字段值的替换文档来替换已存在的文档。 文档大小：当执行更新操作增加的文档大小超过了为该文档分配的空间时，更新操作会在磁盘上重定位该文档。 字段顺序：MongoDB按照文档写入的顺序整理文档字段。但_id字段始终是文档中第一个字段；renaming操作可能会导致文档中的字段重新排序。 Update OperatorFields name description $currentDate 将字段值设置为当前日期(date or timestamp) $inc 按指定的数字递增字段的值 $min 指定的值小于字段的值时才更新 $max 指定的值大于字段的值时才更新 $mul 将字段的值乘以指定的数字 $rename 重命名一个字段 $set 设置文档中字段的值 $setOnInsert 如果更新导致文档插入，则设置字段的值 $unset 从文档中删除指定的字段， Array name description $ 用作更新与查询条件匹配的第一个元素的占位符 $[] 用作更新与查询条件匹配的文档的数组的所有元素的占位符 $[] xxx $addToSet 在集合中不存在元素时添加元素到数组 $pop 移除数组中的第一项或最后一项 $pull 删除所有匹配指定查询的数组元素 $push 向数组中添加项 $pullAll 从数组中删除所有匹配的值 Modifiers name description $each 修饰$push and $addToSet， 向数组中添加多个项 $position 修饰$push，在数组中指定位置添加元素 $slice 修饰$push，限制更新数组的大小 $sort 修饰$push，重新排列存储在数组中的文档 BitWise 1$bit 执行按位AND,OR,XOR更新 更新文档字段中指定字段为了修改文档中的字段，MongoDB提供了update operators，如用来修改值的$set。 1234567891011121314151617181920212223&#123; &lt;update operator&gt;: &#123; &lt;field&gt;: &lt;value&gt;, ...&#125;&#125;#更改指定字段的值db.user.update( &#123; _id: 1 &#125;, &#123; $set: &#123;name: &quot;SET&quot;&#125; &#125;)#删除指定字段，文档中其他字段还在db.user.update( &#123; _id: 1 &#125;, &#123; $unset: &#123;name: &quot;SET&quot;&#125; &#125;)#db.user.updateMany( &#123; _id: 2&#125;, &#123; $set: &#123;name: &quot;AAA&quot;, age: 222&#125; &#125;) 文档替换(Replace)当替换文档时，替换的文档必须仅仅有 &lt;field&gt;: &lt;value&gt;组成。替换文档可以有不同于源文档的字段，但_id字段是不变的。 **建议使用_id作为过滤条件，因为它是唯一的。 123456789101112db.collection.replaceOne()db.user.replaceOne( &#123; name: &quot;AAA&quot; &#125;, &#123; name: &quot;A&quot;, age: 2, sex: &quot;man&quot;, favorites: &#123; artist: &quot;Dali&quot;, food: &quot;banana&quot; &#125; &#125;)db.user.update( &#123; _id: 1&#125;, &#123; name: &quot;A&quot;, age: 2, sex: &quot;man&quot;, favorites: &#123; artist: &quot;Dali&quot;, food: &quot;banana&quot; &#125; &#125;) 删除文档(Delete)方法： db.collection.remove(), 删除一个文档，或所有满足匹配的文档; db.collection.deleteOne(), 删除匹配最多条件的单个文档，即使可能有多个文档可能与指定过滤条件匹配; db.collection.deleteMany(), 删除所有匹配指定过滤条件的文档。 删除的行为表现 Indexes删除操作不会删除索引，即使从集合中删除了所有的文档。 原子性MongoDB中所有的写操作在单一文档层级上是原子的。 删除1234567891011121314#删除所有文档db.collectin.deleteMany(&#123;&#125;)db.collection.remove(&#123;&#125;)#删除所有满足条件的文档db.user.remove( &#123; name: &quot;A&quot; &#125; )db.user.deleteMany( &#123; name: &quot;A: &#125; )#仅删除一个满足条件最多的文档db.user.deleteOne( &#123; name: &quot;A&quot; &#125; )db.users.remove( &#123; name: &quot;A&quot;&#125;, 1) 聚合(Agrregation)聚合操作处理数据记录并返回计算的结果。聚合操作将多个文档中的值(value)分组，并对分组的数据进行各类操作以返回单个结果。 MongoDB提供了三种方式进行聚合： aggregation pipeline(聚合管道); map-reduce function(映射化简); single aggregation methods(聚合指南) Aggregation Pipeline(聚合管道) MongoDB的聚合框架(aggregation framework)是仿照数据处理管道的概念(concept)。Document输入多级管道，它将Document转换为聚合结果。 最基本的pipeline stage提供了：类似查询(query)操作的过滤器(filter)和类似修改(modify)输出文档格式的文档转换。 其他pipeline operation提供了按特定字段对文档进行分组和排序的工具，以及聚合数组内容(包括文档数组)的字段或工具。此外，pipeline stage可以使用运算符(operators)来处理任务。(如计算平均值和连接等…) pipeline通过在MongoDB中使用本地操作，从而提供了高效的数据聚合。所以也是MongoDB中数据聚合的首选方法。 aggregation pipeline能够在一个共享的集合上操作。 aggregation pipeline可以使用索引来提高某些阶段的性能(performance)。另外，管道聚合还有一个内部优化阶段(optimization phase)。 Map-Reduce(映射化简) 一般来说，map-reduce操作有两个阶段： map stage: 处理每个文档并未每个输入文档发出一个或多个对象(object)； reduce stage: 结合映射操作的输出。 可选地，map-reduce有一个对结果做最后修改的最后阶段。与aggregation-operation类似，map-reduce可以指定查询条件来选择一个输入文档，以及对结果进行排序和限制。 map-reduce使用自定义的JavaScript函数执行映射和化简操作，以及可选的最终操作。与聚合管线相比，自定义的JavaScript提供了很大的灵活性。一般来说，map-reduce比aggregation pipeline效率更低，更复杂。 map-reduce能够在一个共享的集合上操作，同样也可以输出到共享集合。 Single Purpose Aggregation Operations(聚合指南) MongoDB同样提供了db.collection.count()和db.collection.distinct()。 所有这些操作都从单个集合中聚合文档，虽然这些操作提供了对常见聚合过程的简单访问，但它们缺少aggregation pipeline和map-reduce的灵活性和功能。 Aggregation Pipeline(聚合管道)MongoDB的聚合框架是仿照数据处理管道的概念。文档输入多级管道，它将文档转换为聚合结果。 当map-reduce的复杂性可能是没有保证的，aggregation pipeline为map-reduce提供了一个可选也可能是聚合任务的首选解决方案。aggregation pipeline对key value和result size有一些限制。 映射化简 聚合指南 MongoDB文本索引MongoDB支持在字符串内容上执行文本检索(text search)的查询操作。视图不支持文本检索。为了执行文本检索，MongoDB使用text index和$text操作符。text索引可以包括任何值为字符串或字符串元素数组的字段。 栗子： 1234567db.sample.insert( [ &#123; _id: 1, name: "A", description: "AAA" &#125;, &#123; _id: 2, name: "B", description: "BBB" &#125;, &#123; _id: 3, name: "C", description: "CCC" &#125; ]) 为了执行文本检索查询，你必须在集合有一个text索引，一个集合只能有一个文本检索索引，但是这个索引可以覆盖多个字段。 启动在name和description字段上的文本检索： 123db.sample.createIndex( &#123; name: "text", description: "text" &#125;) 使用$text查询操作符在一个有text index的集合上执行文本检索 123456789101112131415db.sample.find(&#123; $text: &#123; $search: &quot;A B&quot; &#125;&#125;)#精确检索db.sample.find(&#123; $text: &#123; $search: &quot;A \&quot;B\&quot;&quot; &#125;&#125;)#词语排除db.sample.find(&#123; $text: &#123; $search: &quot;A B -AAA&quot; &#125;&#125;) MongoDB默认返回没排序的结果。然而文本检索将会对每个文档计算一个相关性分数，表明该文档与查询的匹配程度。为了使用相关性分数进行排序，你必须使用 $meta textScore字段进行映射然后基于该字段进行排序。 1234db.sample.find( &#123; $text: &#123; $search: &quot;A AAA B&quot; &#125; &#125;, &#123; score: &#123; $meta: &quot;textScore&quot; &#125; &#125;).sort( &#123; score: &#123; $meta: &quot;textScore&quot; &#125; &#125;) 文本检索可以在聚合管道中使用。 文本索引 文本检索操作符 在管道聚合中使用文本索引 使用基本技术Rosette语义平台的文本索引 文本检索语言 MongoDB数据模型MongoDB的数据具有灵活的模式，集合本身没有对文档结构的规则性校验。 数据模型设计介绍关系型数据库要求你再插入数据之前必须先定义好一个表的模式结构，而MongoDB的集合并不限制文档结构。这种灵活性让对象和数据库文档之间的映射变得很容易。即使数据记录之间有很大的变化，每个文档也可以很好的映射到各条不同的记录。当然，在实际使用中，同一个集合中的文档往往都有一个比较类似的结构。 数据模型设计中最具挑战性的是在应用程序需求，数据库引擎性能要求和数据读写模式之间的权衡考量。 文档结构引用(reference)引用方式通过存储链接或引用信息来实现两个不同文档之间的关联。应用程序可以通过解析这些数据库引用来访问相关数据。简单来讲，这就是规范化的数据模型。 内嵌(embedded data)内嵌方式指把相关联的数据保存在同一个文档之内。MongoDB的文档结构允许一个字段或一个数组内的值为一个嵌套的文档。这种冗余的数据模型可以让应用程序在一个数据库内完成对相关数据的读取或修改。 写操作的原子性在MongoDB中，写操作在文档级别是原子的(atomic)，没有一个单独的写操作可以原子地影响多个文档或多个集合。但，对原子性写操作利好的内嵌数据模型会限制应用程序对数据的使用场景。 嵌入(embdded)数据的非规格化(denormalized)数据模型将单个文档所表示的实体(entity)的所有相关数据组合在一起。这有利于原子写操作，因为单个写操作可以插入或更新实体的数据； 规格化(normalizing)数据通过多个集合拆分数据，并需要多个不是原子集合的写操作。 文档的增长如果文档的大小超出分配给文档的原空间大小，那么MongoDB就需要把文档从磁盘上的现有位置移动到一个新的位置以存放更多的数据。这种数据增长的情况也会影响到是否要使用规范化或非规范化。 数据的使用和性能设计文档模型时，一定要考虑应用程序会如何使用你的数据。 例如： 假如应用程序通常只会使用最近插入的文档，那么可以考虑使用限制集； 假如应用会做大量的读操作，那么可以加多一些索引的方法来提升常见查询的性能。 文档验证MongoDB提供了在更新和插入期间验证(validate)文档的功能(capability)。验证规则是在每个集合中指定使用验证符(validator)选项，利用一个文档指定验证堆栈或表达式。 通过collMod命令附带验证符选项向一个已经存在的集合添加文档验证； 利用db.createCollection()命令附带验证符选项来创建文档验证规则。 123456789db.createCollection( "contacts", &#123; validator: &#123; $or: [ &#123; phone: &#123; $type: "string" &#125; &#125;, &#123; email: &#123; $regex: /@mongodb\.com$/ &#125; &#125;, &#123; status: &#123; $in: [ "Unknown", "Incomplete" ] &#125; &#125; ] &#125;&#125; ) MongoDb同样提供了validationLevel选项，它确定了MongoDb在更新期间如何将验证规则应用到已有文档，以及验证操作选项。它确定MongoDB是否错误并拒绝违反验证规则的文档，或者警告日志中的违规，但允许无效的文档。 行为验证发生在更新和插入期间。当向一个文档添加验证，在修改之前，现有文档不会进行验证检查。 现有文档 可使用validationLevel选项来控制MongoDB怎样处理现有文档。 默认情况下，MongoDB是严格的，并且将验证规则应用于所有插入和更新操作。 12345678#moderate level#在中等级别下，对不符合验证标准的现有文档更新将不会检查有效性db.runCommand(&#123; collMod: "contacts", validator: &#123; $or: [ &#123; phone: &#123; $exists: true &#125; &#125;, &#123; email: &#123; $exists: true&#125;&#125; ] &#125;, validationLevel: "moderate"&#125;) 设置validationLevel为off以禁用验证功能。 接受或拒绝无效文档 validationAction选项决定了MongoDB如何处理违反(violate)验证规则的文档。 默认情况下，validationAction是错误的，并且拒绝任何违反验证条件的插入和更新操作。 123456789101112131415161718#当validationAction为warn时，MongoDB记录所有违反行为，但允许插入或更新操作。db.createCollection( "contacts", &#123; validator: &#123; $or: [ &#123; phone: &#123; $type: "string" &#125; &#125;, &#123; email: &#123; $regex: /@mongodb\.com$/ &#125; &#125;, &#123; status: &#123; $in: [ "Unknown", "Incomplete" ] &#125; &#125; ] &#125;, validationAction: "warn" &#125;)#如下违规操作将会报警，并由于是warn，所以写入成功db.contacts.insert( &#123; name: "Amanda", status: "Updated" &#125; ) 约束(restriction) 无法在admin,local,config数据库的集合 和 system.*集合 里面指定验证符(validator)。 绕过文档验证 通过bypassDocumentValidation选项来绕过文档验证。 数据建模理论数据模型设计一个高效的数据模型能够很好的满足应用程序的需求。设计一个文档数据结构最关键的考量就是决定是使用嵌套(embdded)还是引用(reference)。 内嵌式数据模型(非规范化)在MongoDB里面，可以把相关的数据包括在一个单个的结构或者文档下面。这样的数据模型也叫作非规范化模式。 内嵌数据可以让应用程序把相关的数据保存在同一条数据记录里面，这样，应用程序就可以发送较少的请求给MongoDB来完成常用的查询和更新请求。 一般来说，下述情况建议使用内嵌数据模型： 数据对象之间有包含(contain)关系； 数据对象间有一对多的关系。 通常情况下，内嵌数据会对读操作有比较好的性能提高，可以使应用程序在一个单个操作就可以完成对数据的读取。同时，内嵌数据也对更新相关数据提供了一个原子性写操作。 规范化数据模型一般来说，下述情况可以使用规范化模型： 内嵌数据会导致很多数据的重复，并且读性能的优势又不足与盖过数据重复的弊端时； 需要表达比较复杂的多对多关系时； 大型多层次结构数据集。 MongoDB特性和数据模型的关系MongoDB的数据建模不仅仅取决于应用程序的数据需求，也要考虑MongoDB本身的一些特性。 文档增长性(increase)如果更新操作导致文档大小增加，那么可能需要重新设计数据模型，在不同文档之间使用引用的方式而非内嵌、冗余的数据结构。MongoDB会自动调整空白填充的大小以尽可能的减小文档迁移。你也可以使用一个预分配策略来防止文档的增长。 原子性(atomic)在MongoDB中，所有在文档级别的操作都具有原子性。一个单个写操作最多只可以修改一个文档。即使是一个会改变同一个集合中多个文档的命令，在同一时间也只会操作一个文档。即便是涉及多个子文档的多个操作，只要是在同一文档之内，这些操作仍旧是有原子性的。 尽可能保证那些需要在一个原子操作内进行修改的字段定义在同一个文档里面。如果你的应用程序允许对两个数据的非原子性更新操作，那么可把这些数据定义在不同的文档内。 把相关数据定义到同一个文档里的内嵌方式有利于这种原子性操作。对于那些使用引用来关联相关数据的数据模型，应用程序必须再用额外的读和写操作去取回和修改相关的数据。 分片(sharding)MongoDB使用分片来实现水平扩展。使用分片的集群可以支持海量的数据和高并发读写。使用分片技术把一个数据库内的某一个集合的数据进行分区，从而达到把数据分到多个mongod实例(或分片上)的目的。 MongoDB依据分片键分发数据和应用程序的事务请求。选择一个合适的分片键对性能有很大的影响，也会促进或阻碍MongoDB的定向分片查询和增强的写性能。所以在选择分片键的时候要仔细考量分片键所用的字段。 索引(index)对常用操作可以使用索引来提高性能。对查询条件中常见的字段，以及需要排序的字段创建索引。MongoDB会对_id自动创建唯一索引。 创建索引时，需要考虑索引的下述特征： 每个索引要求至少8KB的数据空间； 每增加一个索引，就会对写操作性能有一些影响。对于一个写多读少的集合，索引会变得很费时。因为每个插入必须要更新所有索引； 每个索引都会占一定的硬盘空间和内存(对于活跃的索引)。索引可能会用到很多这样的资源，因此对这些资源要进行管理和规划，特别是在计算热点数据大小的时候。 集合的数量某些情况下，可能需要把相关的数据保存到多个集合里面。比如： 12&#123; log: &quot;dev&quot;, ts:..., info: ... &#125;&#123; log: &quot;debug&quot;, ts:..., info: ... &#125; 一般来说，很大的集合数量对性能没有什么影响，反而在某些场景下有不错的性能。使用不同的集合在高并发批处理场景下会有很好的帮助。 当使用有大量集合的数据模型时，请注意： 每个集合有几KB的额外开销； 每个索引(包含_id)，需要至少8KB的数据空间； 每个MongoDB的数据库有且仅有一个命名文件(namespace file)(.ns)。这个命名文件保存了数据库的所有元数据，每个索引和集合在这个文件里都有一条记录； MongoDB的命名文件有大小的限制(默认16MB)。利用db.system.namespaces.count()查看。 包含大量小文档的集合如果你有一个包含大量小文档的集合，则应该考虑为了性能而嵌入。如果你可以通过一些逻辑关系将这些小文档分组，并且你经常通过这个分组来检索文档，那么你应该考虑将小文档”卷起来”成为包含一系列嵌入式文档的大文档。 将这些小文档“卷起来”成为逻辑分组，意味着检索一组文档的查询设计顺序读取和较少的随机磁盘访问。此外，将文档“卷起”并将公共字段移动到较大的文档会使字段上的索引受益。公共字段的副本将会减少，并且相应索引中的关联键条目也会减少。 然而，如果你通常只需要检索分组中的一个文档的子集，那么“滚动”文档可能无法提供更好的性能。此外，如果晓得，独立的文档代表数据的自然模型，那你应该维护改模型。 小文档的存储优化(storage optimization)每个MongoDB文档都包含一定的开销(overhead)，这些开销通常是无关紧要的。但如果文档只有几个字节，那就相当重要了。 考虑以下有关优化这些集合的存储利用率的建议： 显示地使用_id字段； 使用较短的字段名称； 嵌套文档。 数据生命周期管理数据模型决策应考虑数据生命周期管理。 集合的*TTL功能在一段时间后标识文档到期。如果应用程序需要一些数据才能在数据库中持久化一段有限的时间，请考虑使用TTL特性。 此外，你的应用程序仅使用最近插入的文档，请考虑限制集。 数据模型例子与范式文档关系建模一对一关系建模：内嵌文档模型用内嵌文档方式实现一对一关系。 一对多关系建模：内嵌文档模型用内嵌文档方式实现一对多关系。 一对多关系建模：文档引用模式用文档引用实现一对多关系。 树结构建模父文档引用父文档引用模式用一个文档来表示树的一个节点。每一个文档除了存储节点的信息，同时也保存该节点父节点文档的id值。 1234567891011121314db.test.insert(&#123; _id: "MongoDB", parent: "Databases" &#125;)db.test.insert(&#123; _id: "Databases", parent: "Programming" &#125;)db.test.insert(&#123; _id: "Programming", parent: "Books" &#125;)db.test.insert(&#123; _id: "Books", parent: null &#125;)#查询父节点db.test.findOne(&#123; _id: "MongoDB" &#125;).parent#对parent字段创建索引，这样可以快速的按照父节点查找db.test.createIndex(&#123; parent: 1 &#125;)#查询一个父节点的所有子节点db.test.find(&#123; parent: "Databases" &#125;) 子文档引用子文档引用模式用一个文档来表示树的一个节点。每一个文档除了存储节点信息外，同时也用一个数组来保存该节点的所有子节点的id值。 1234567891011121314db.test.insert(&#123; _id: "MongoDB", children: [] &#125;)db.test.insert(&#123; _id: "Databases", children: [ "MongoDB", "dbm" ]&#125;)db.test.insert(&#123; _id: "Programming", children: [ "Languages", "Databases" ]&#125;)db.test.insert(&#123; _id: "Books", children: [ "Programming" ]&#125;)#查询子节点db.test.findOne(&#123; _id: "Databases"&#125;).children#对children字段创建索引，这样就可以快速按照子节点查找db.test.createIndex(&#123; children: 1 &#125;)#查找一个子节点的父节点和同级节点db.test.find(&#123; children: "MongoDB" &#125;) 祖先数组(ancestors array)祖先数组模式用一个文档来表示树的一个节点。每一个文档除了存储节点的信息，同时也存储了对父文档及祖先文档的id值。 1234567891011121314db.test.insert(&#123; _id: "MongoDB", ancestors: [ "Books", "Programming", "Databases" ], parent: "Databases" &#125;)db.test.insert(&#123; _id: "Databases", ancestors: [ "Books", Programming" ], parent: [ "MongoDB", "dbm" ]&#125;)db.test.insert(&#123; _id: "Programming", ancestors: [ "Books" ], parent: "Books" &#125;)db.test.insert(&#123; _id: "Books", ancestors: [ ], parent: null &#125;)#查询一个节点的祖先节点db.test.findOne(&#123; _id: "MongoDB" &#125;).ancestors#对ancestors创建索引db.test.createIndex(&#123; ancestors: 1 &#125;)#利用ancestors字段来查找某个节点的所有子代节点db.test.find(&#123; ancetors: "Programmming" &#125;) 物化路径(materialized path)物化路径模式将每个树节点存储在文档中。除了存储节点信息外，同时也存储了祖先文档或路径的id值。 123456789101112131415db.test.insert(&#123; _id: "Books", path: null &#125;)db.test.insert(&#123; _id: "Programming", path: ",Books," &#125;)db.test.insert(&#123; _id: "Databases", path: ",Books,Programming," &#125;)db.test.insert(&#123; _id: "MongoDB", path: ",Books,Programming,Databases," &#125;)#查询整个树的所有节点并按path排序db.test.find().sort(&#123; path: 1 &#125;)#可以在path字段上使用re来查询db.test.find(&#123; path: /,Programming,/ &#125;)db.test.find(&#123; path: /^,Books,/ &#125;)#在path字段上创建索引db.test.createIndex(&#123; path: 1 &#125;) 嵌套集合(nested set)嵌套集合模式对整个树结构进行一次深度优先的遍历。遍历时候对每个节点的压栈和出栈作为两个不同的步骤记录下来。每一个节点就是一个文档，除了节点信息外，文档还保存父节点的id以及遍历的两个步骤编号。压栈是的步骤保存到left字段里，而出栈时的步骤编号则保存到right字段里。 12345678db.test.insert(&#123; _id: "Books", parent: 0, left: 1, right: 12 &#125;)db.test.insert(&#123; _id: "Programming", parent: "Books", left: 2, right: 11 &#125;)db.test.insert(&#123; _id: "Databases", parent: "Programming", left: 5, right: 10 &#125;)db.test.insert(&#123; _id: "MongoDB", parent: "Databases", left: 6, right: 7 &#125;)#查询摸个节点的子代节点db.test.find(&#123; left: &#123; $gt: db.test.findOne(&#123; _id: "Databases" &#125;), right: &#123; $lt: db.test.findOne(&#123;"_id: "Databases"&#125;) &#125; &#125;) 具体应用模型举例原子性事务建模如何使用内嵌技术来保证同一文档内相关字段更新操作的原子性。 举例来说，假设你在设计一个图书馆的借书系统，你需要管理书的库存量以及出借记录。一本书的可借数量加上借出数量的和必须等于总的保有量，那么对这两个字段的更新必须是原子性的。 关键词搜索建模描述了一种把关键词保存在数组里并使用多键索引来实现关键词搜索功能的方法。 为实现关键词搜索，在文档内增加一个数组字段并把每一个关键词加到数组里。然后你可以对该字段建一个 多键索引。这样你就可以对数组里面的关键词进行查询了。 货币数据建模处理货币数据的应用程序通常需要捕获小数(franctional)货币单位，并在执行算术时需要精确地模拟十进制四舍五入。许多现代系统(float,double)使用的基于二级制的浮点运算不能精确地表示小数，而且需要某种程度的近似，因而不适合于货币运算。因此，在货币数据建模时，这一约束是一个重要的考虑因素。 数字模型如果需要查询数据库中精确、数学书有效匹配或需要执行Server端算术，则数字模型可能是适合的。 非数字模型如果需要在Server端做一些对货币数值的数学计算，那么严格精度可能会更合适一些。 时间数据模型MongoDB默认存储UTC时间，并将任何本地时间转换成这种形式。 MongoDB管理administration The administration 文档说明了MongoDB实例和部署正在进行的操作和维护。本文档包括这些问题的高级概述，以及涵盖操作MongoDB的特定过程的教程。 操作清单(operation checklist)如下清单，提供了帮助你避免在MongoDB部署中出现问题的建议。 文件系统(file system) 将磁盘分区与RAID配置对齐； 避免对dbpath使用NFS。使用NFS会导致性能下降和不稳定； 针对Linux/Unix的文件格式，建议使用XFS或EXT4。如果可能的话，对MongoDB使用XFS性能会更好； 对于WiredTiger存储引擎，强烈建议使用XFS来避免使用EXT4时发现的性能问题； 针对Windows，不要使用FAT(FAT16/32/exFAT)文件系统，请使用NTFS文件系统。 复制(replication) 验证所有非隐藏副本集成员的RAM, CPU, 磁盘, 网络设置, 配置等方面是否相同； 配置oplog的大小来适合你的用例； 确保副本集包好至少3个以journaling方式运行的数据承载节点； 在配置副本集成员时使用主机名(hostname)，而不是IP地址； 确保所有的mongod实例之间使用全双工网络； 确保每台主机都能解析它自己； 确保副本集包含奇数个投票的成员(voting members)，确保票数不会相等则一定会有主被选举出来； 确保mongod实例有0或1票； 为了高可用(high availability)，副本集集群最少部署3台数据中心。 分片(sharding) 将配置服务器放置于专用硬件，以便在大型集群中实现最佳性能。确保硬件有足够的RAM来讲数据文件完全存储到内存中，并且有专门的存储； 使用NTP同步分片集群上所有组件的时钟； 确保Mongod, mongos和配置服务器之间的全双工网络连接； 使用CNAME将配置服务器标识到集群中，以便可以在不停机的情况下重命名和重新编号配置服务器。 Journaling 确保所有实例都使用journaling； 将journal放置于低延迟(low-latency)磁盘上，用于编写密集的工作负载。注意，这将影响快照式备份(snapshot)，因为构成数据库状态的文件将驻留在单独的volume上。 硬件(hardware) 使用RAID10和SSD能够获得最佳性能； 确保每个mongod为它的dbpath提供了IOPS； 在虚拟环境中运行时，避免动态内存功能； 避免将所有副本集成员放置于相同的SAN(存储区网络)中。 部署到云上 AWS; Azure; Aliyun; Tencent. 操作系统配置Linux 关闭hugepages和defrag； 调整存储数据库文件设备上的readahead设置，以适应用例； 在虚拟环境中的RHEL7/CENTOS7上禁用优化工具； 为SSD驱动使用noop或deadline磁盘调度； 禁用NUMA或将vm.zone_reclaim_mode设置为0，并运行node interleaving的mongod实例； 调整硬件的ulimit值以适应实例； 对dbpath挂载点使用noatime； 对你的部署配置足够的文件句柄(fs.file-max value of 98000)，内核pid限制(kernel.pid_max value of 64000)，每个进程的最大线程数(kernel.threads-max value 0f 64000)； 确保你的系统配置有swap交换分区； 确保系统默认TCP keepalived设置正确。 Windows 考虑禁用NTFS的最后访问时间更新。这类似与在Unix-like系统上禁用atime。 备份(backup) 安排备份和恢复过程的定期测试，以便手头有时间估计，并恢复其功能。 监控(monitor) 监视Server的硬件统计信息(磁盘使用，CPU，可用磁盘空间…) 监视mongodb的状态。 负载均衡(load balance) 配置负载均衡启用”sticky session”或“client affinity”，对现有连接有足够的超时时间； 避免放置负载均衡器在MongoDb集群或副本集组件。 开发清单(development checklist)如下清单，提供了帮助你避免在MongoDB部署期间出现的问题的建议。 数据持久性(data durability) 确保副本集至少包含3个(带有w:majority)数据承载节点，这3个数据承载节点需要为副本集的高数据持久性； 确保所有实例都是用了journaling。 架构设计(schema design)MongoDB中的数据具有动态结构。collection并不要求document结构。这有助于迭代开发和多态性。然而，集合中的文档通常具有高度的同类结构。 确保你需要的集合集中的索引(indexes)支持你的查询(query)。除了_id索引，你必须显式的创建所有索引； 确保你的架构设计支持你的开发类型； 确保你的架构设计不依赖于长度不受绑定的索引数组； 再架构设计时考虑文档大小限制。 复制(replication) 使用奇数个副本集成员以确保选举顺利进行。如果有偶数个成员，请使用仲裁者(arbiter)以确保级数的选票； 确保使用监控工具和适当的写关注来保持从库数据最新； 不要使用从库读取来扩展整体读取吞吐量。 分片(sharding) 确保你的sharded key将负载均匀地分配到分片上； 对需要按分片数进行缩放的工作负载(workload)使用有针对性的操作； 对非目标(non-targeted)查询，总是从主节点读取可能对陈旧或孤立的数据很敏感； 当向新的非散列(hash)分片集合中插入大数据集时，Pre-split and manually balance chunks。 驱动(drivers) 使用连接池(connection pooling)； 确保你的应用程序在复制集选举期间还能够处理瞬时写入(transient write)和错误读取； 确保你的应用程序处理失败的请求并适时地重试它们； 使用指数退避逻辑重试数据库请求； 如果需要计算数据库操作的编译执行时间，对读操作使用cursor.maxTimeMS()，对写操作使用wtimeout。 性能(MongoDB Perfomance) 当遇到性能下降时，通常与数据库的访问策略、硬件可用性和开放的数据库连接数相关； 一些用户可能由于不适当的索引策略或结果不足而经历性能限制，或由于架构设计模式差； 性能问题可能表明数据库正以最大限度运行，是时候给数据库添加额外的容量(capacity)了。尤其是，应用程序的工作集应该有足够的物理内存。 锁紧性能(lock performance) MongoDB使用锁系统来确保数据集的一致性。如果某些操作需要长时间运行(long-running)，或队列窗体，随着请求和操作等待lock，性能将会下降； 锁相关的减速是可以间歇的，可查看lock部分是否影响了性能； locak.deadlockCount提供了遭遇死锁(deadlocks)的次数； 如果globalLock.currentQueue.total很高，则可能有大量的请求在等待lock。这表明并发问题(concurrency issue)可能影响性能； 如果globalLock.totalTime时间比uptime高，那么数据库在锁定状态中存在了大量时间； 长查询(long query)可能会导致索引无效使用、非最佳(non-optimal)建构设计、差的查询结构、系统体系结构问题、RAM不足导致页面错误(page fault)和磁盘读取。 连接数(number of connections)在某些情况下，应用程序和数据库之间的连接数量可能超出服务器处理请求的能力。serverStatus文档中的以下字段可以提供观察： globalLock.activeClients包含正在进行或排队的活动操作的客户端总数； connnections由以下两个字段组成： 1，connections.current连接到数据库实例的当前客户端总数； 2，connections.available可用的连接总数。 如果有大量的并发程序请求，则数据库可能无法满足需求。那么就需要增加部署的容量。 对于读操作巨大(read-heavy)的应用程序，增加你的副本集大小并将读操作分发给SECONDARY。对于写操作巨大(write-heavy)的应用程序，部署分片并将一个或多个分片添加到分片集群中，以便在mongod实例之间分配负载。 连接数到达峰值也可能是应用程序或驱动错误所导致的结果。 除非收到系统范围的限制，否则MongoDB对传入连接没有限制。在基于Unix系统上，可使用ulimit命令或修改/etc/sysctl系统文件来修改系统限制。 数据库性能分析(database profiling)MongoDB的profiler是一种数据库分析系统，可以帮助识别低效的查询和操作。 有如下分析级别(profiling-level)可用： Level Settiing 0 Off.No profiling 1 On.Only includes “slow” operations 2 On.Includes all operations 在mongo shell中运行如下命令来配置性能分析器： 123#dbsetProfilingLever()db.setProfilingLevel(1) slowOpThresholdMs的设置定义了什么是一个slow操作，要设置一个慢操作的阈值(threshold)，可以在运行时作为db.setProfilingLevel()操作的一个参数来配置slowOpThresholdMs。 默认情况下，mongod将会把所有的慢查询(slow query)记录到日志，这是由slowOpThresholdMs定义的。 通过在mongo shell中使用show profile，你可以在数据库中的system.profile集合中查看性能分析器的输出。或者执行如下操作： 12345#返回超过100ms的所有操作，这个值请高于阈值`slowOpThresholdMs`db.system.profile.find( &#123; millis: &#123; $gt: 100 &#125; &#125;) 你必须使用查询操作符去访问system.profile文档中的查询字段。 数据库性能分析器(databases profiler)数据库性能分析器(db profiler)收集有关MongoDB的写操作、游标和运行在mongod实例上的命令的细微数据，你可以在每个数据库或每个实例基础上启用性能分析(profiling)。默认情况系，分析器是关闭的。启用profiling的时候需要配置profiling leverl。 The database profiler将所有的数据收集到system.profile集合中，它是一个限制集(capped collection)。 分析等级(Profiling levels) 0， 关闭分析器，不收集任何数据。mongod总是将操作时间长于slowOpThresholdMs的值写入日志。这是默认分析器级别； 1， 只收集慢操作的分析数据。默认是以100ms； 2， 收集所有数据库操作的分析数据。 启用分析器(profiling)和设置分析级别(profiling level)当启用profiling，也要设置profiling level，分析器将数据记录到system.profile集合。当你在数据库中启用profiling后，MongoDB会在数据库中创建system.profile集合。 使用db.setProfilingLevel()来设置profiling level和启用profiling。 1db.setProfilingLevel(1) 指定慢操作的阈值(the Threshold for slow operations) 慢操作的阈值(threshold)应用于整个mongod实例。当你修改了阈值，那你就对所有的数据库实例进行了修改。修改了数据库慢操作的阈值同样也会影响整个mongod实例性能分析子系统的慢操作阈值。默认情况下，慢操作的阈值为100ms。性能分析level-1将会记录长于阈值的慢操作到日志。 要更改阈值，请将两个参数(parameter)在mongo shell传递给db.setProfilingLevel()。第一个参数是为当前的数据库设置profiling level，第二个参数是为整个mongod实例设置默认的慢操作阈值。 栗子： 123456mongo&gt;use zhang&gt;db.serProfilingLevel(1,100)#会在zhang数据库下生产system.profile集合 检查分析等级(check profiling level) 1234567db.getProfilingStatus()#default#&#123; &quot;was&quot; : 0, &quot;slowms&quot; : 100 &#125;db.getProfilingLevel()#0 为一个完整的mongod实例启用profiling 在测试环境中，处于开发目的，你可以为一个完整的mongod实例启用profiling功能。性能分析等级应用于mongod实例中的所有数据库。 12#设置level：1，slowOpThresholdMs: 50mongod --profile 1 --slowms 50 数据库分析和分片 无法对mongos实例启用profiling。要对分片集群启用profiling功能，你必须对分片集群中的每个mongod实例启用profiling功能才行。 查看性能分析器的数据(profiler data)数据库性能分析器关于数据库操作的日志信息放置于system.profile集合中。如需查看性能信息，请查询该集合。 栗子： 1234567891011121314151617db.system.profile.find()db.system.profile.find().limit(10).sort(&#123; ts: -1 &#125;).pretty()#指定时间db.system.profile.find( &#123; millis: &#123; $gt: 5 &#125; &#125; ).pretty()#除了某个命令外db.system.profile.find(&#123; op: &#123; $ne: &apos;cmd&apos; &#125; &#125;).pretty#某个特定集合db.system.profile.find( &#123; ns: &apos;db.collection&apos; &#125; ).pretty()#显示最近的事件show profile 分析器开销(profiler overhead)分析器对性能影响很小。system.profile集合是一个默认大小为1MB的限制集。这样大小的集合通常可以存储上千份分析文档，但一些应用程序可能在每次操作中只使用或多或少的分析数据。 在Primary上面修改system.profile集合的大小 停止profiling； 删除(drop)system.profile集合； 新建一个system.profile集合； 重启profiling。 12345use dbdb.serProfilingLevel(0)db.system.profile.drop()db.createCollection( &quot;system.profile&quot;, &#123; capped: true, size: 4000000 &#125; )db.setProfilingLevel(1) 在Secondary上修改system.profile集合的大小 在Secondary上修改system.profile集合的大小，你必须停止Secondary，然后以standalone模式运行它，之后执行修改步骤。当做完上述步骤之后，以一个副本集成员的方式使用standalone模式重启它。 禁用显见的大页面(Disable Transparent Huge Pages)Transpatent Huge Pages(THP)是一个Linux的内存管理系统，通过使用更大的内存页，减少了在具有大量内存的机器上进行Translation Lookaside Buffer(TLB)查找的开销。 然而，数据库工作负载(workload)在THP中的性能往往很差，因为它们往往具有稀疏的(sparse)而不是连续的(contiguous)内存访问模式。你应该在Linux机器上禁用THP来确保MongoDB获得最佳的性能。 1. 创建init.d脚本 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bash### BEGIN INIT INFO# Provides: disable-transparent-hugepages# Required-Start: $local_fs# Required-Stop:# X-Start-Before: mongod mongodb-mms-automation-agent# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Disable Linux transparent huge pages# Description: Disable Linux transparent huge pages, to improve# database performance.### END INIT INFOcase $1 in start) if [ -d /sys/kernel/mm/transparent_hugepage ]; then thp_path=/sys/kernel/mm/transparent_hugepage elif [ -d /sys/kernel/mm/redhat_transparent_hugepage ]; then thp_path=/sys/kernel/mm/redhat_transparent_hugepage else return 0 fi echo 'never' &gt; $&#123;thp_path&#125;/enabled echo 'never' &gt; $&#123;thp_path&#125;/defrag re='^[0-1]+$' if [[ $(cat $&#123;thp_path&#125;/khugepaged/defrag) =~ $re ]] then # RHEL 7 echo 0 &gt; $&#123;thp_path&#125;/khugepaged/defrag else # RHEL 6 echo 'no' &gt; $&#123;thp_path&#125;/khugepaged/defrag fi unset re unset thp_path ;;esac 2. 使之可执行 1chmod 755 /etc/init.d/disable-transparent-hugepages 3. 配置操作系统以在开机的时候运行它 12345678910#Debian系列update-rc.d disable-transparent-hugepages defaults#RedHat系列chkconfig --add disable-transparent-hugepages#SUSEinsserv /etc/init.d/disable-transparent-hugepages 4. 如果适用，覆盖(override)tuned和ktune 12345678910111213#RedHat/CentOS7mkdir /etc/tuned/no-thpvim /etc/tuned/no-thp/tuned.conf[main]include=virtual-guest[vm]transparent_hugepages=nevertuned-adm profile no-thp 5. 测试你做的改变 1234cat /sys/kernel/mm/redhat_transparent_hugepage/enabledcat /sys/kernel/mm/redhat_transparent_hugepage/defrag#always madvise [never] 另一种简便的方式来禁用THP 123456vim /etc/rc.d/rc.localecho 'never' &gt; /sys/kernel/mm/transparent_hugepage/enabledecho 'never' &gt; /sys/kernel/mm/transparent_hugepage/defragchmod u+x /etc/rc.d/rc.local Unix系统下的ulimit的设置大多Unix-Like系统，都提供了限制每个进程和每个基本用户使用线程，文件和网络连接等系统资源的一些方法。ulimits防止单个用户使用太多的系统资源。有时，这些限制的默认值太小，这会导致MongoDB操作过程中出现一系列问题。 123#限制文件#/etc/security/limits.conf#/etc/security/limits.d/ 资源利用mongod和mongos每次使用线程和文件描述符来跟踪连接和管理内部操作。 通常情况下，所有的mongod和mongos实例： 利用每一个文件描述符和线程来跟踪每个即将到来的连接； 将每个内部线程或pthread作为一个系统进程来跟踪。 mongod mongod实例使用的每个数据文件都有一个文件描述符； 当storage.journal.enabled为true是，mongod进程实例使用的每个日志文件都有一个文件描述符； 在复制集中，每个mongod保持一个连接复制集中所有其他集合成员的连接。 mongos mongos实例与每个分片都保持一个连接池，所有mongos可以重用连接，这样因为不用建立新连接，从而能快速的满足请求； 通过限制连接数，可以防止mongos因在mongod实例上创建太多连接而产生级联效应。 资源限制的设置ulimit是指每个user使用各种资源的限制值。因此，无论你的mongod实例是以单个用户多进程执行还是以多mongod进程执行，都可以看到对这些资源的连接。 ulimits有hard和soft两个方式。 hard：是指用户在任何时候都可以活动的进程的最大数量，这是上限。没有任何non-root进程能够增加hard ulimit； soft：是对会话或进程实际执行的限制，但任何进程都可以将其增加到hard ulimit的最大值。 较低的soft limit可能无法创建新线程(thread)，如果连接数太高，则关闭错误连接。因此，将soft和hard的值都设置为推荐值是非常重要的。 修改ulimit设置之后，要重启程序修改值才会有效。可通过/proc文件系统查看运行进程当前的限制值。 使用ulimit对系统限制的改变在系统重启后都会恢复到默认值。需要修改其它文件来确保修改一直生效。 ulimit 123456789101112131415161718ulimit -acore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 7170max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 7170virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited 修改ulimit 123456789#-f (文件大小)#-t (cpu 时间)#-v (虚拟内存)#-n (单个进程文件打开数)#-m (memory size)#-u (可打开的进程/线程)ulimit -t unlimitedulimit -u 64000 配置和维护(maintenance)Run-time databases configurationcommand line和configuration file interfaces为MongoDB管理员提供了控制数据库系统操作的大量选项和设置。 使用配置文件启动MongoDB实例： 12mongod --config /etc/mongod.confmongod -f /etc/mongod.conf 配置数据库mongodb的配置文件从MongoDB3.0以后使用YAML格式。 1234567891011121314151617vim /etc/mongod.confprocessManagement: fork: truenet: bindIp: 127.0.0.1 port: 27017storage: dbPath: /var/lib/mongodbsystemLog: destination: file path: "/var/log/mongodb/mongod.log" logAppend: truestorage: journal: enabled: true 对于大多数以standalone模式运行的servers，以上是一个足够的基本配置。Unix-Like操作系统需要以超级用户(root)权限才能运行端口小于1024的程序。 安全考虑(security consideration)下面的配置选项集合对于限制对于mongod实例的访问非常有用。 123456net: port: 27017 bindIp: 127.0.0.1,192.168.1.11security: authorization: enabled 复制集和分片配置(replication and sharding configuration)复制集的配置非常简单，只需要replSetName在集合中的所有成员具有一致的副本集名字。 12replication: replSetName: zhang 开启副本集认证： 12345678910#利用openssl生成keyFileopenssl rand -base64 256 &gt; /dir/path/mongodb/keyFilesecurity: replSetName: zhang keyFile: /dir/path/mongodb/keyfilechown -R mongod:mongod /dir/path/mongodb 设置keyFile启用身份认证，并为复制集成员在相互身份认证时使用的认证文件指定一个密钥文件。密钥文件的内容是任意的，但在复制集中的所有成员和连接到该集合的mongos实例之间必须相同。不然怎么能认证通过呢。秘钥文件的大小必须小于1KB，并且只能包含base64集中的字符，并且此密钥文件在Unix系统上必须not have group或not have world permissions。 分片配置(sharding configuration)分片要求配置服务器和分片服务器的Mongod实例具有不同的mongod配置文件。配置服务器存储集群的元数据(metadata)，而分片服务器存储数据(data)。 在配置文件中给mongod实例配置配置服务器(config-server)，给sharding.clusterRole指定配置服务器。 12345678910#配置config-servernet: bindIp: 192.168.1.11 port:27001replication: replSetName: zhangsharding: clusterRole: configserver#configserver必须要是一个部署的副本集 在同一个系统上运行多个数据库实例(multiple database instances)在许多情况下，在单个系统(single system)上运行多个数据库实例是不推荐的。 但可能由于一些部署或者测试的目的，你需要在单个系统上运行多个mongod实例。在这些情况下，请为每一个mongod实例使用一个基本的配置文件，但要额外配置如下值： dbpath(必须); pidFilePath(必须); systemLog(非必须，但建议开启); 栗子： 1234567891011121314151617181920212223242526#mongod_27017实例vim /etc/mongod_27017.confsystemLog: path: /var/log/mongod_27017.logstorage: dbPath: /var/lib/mongodb27017processManagement: pidFilePath: /var/lib/mongodb27017/mongod_27017.pid#mongod_27018实例vim /etc/mongod_27018.confsystemLog: path: /var/log/mongod_27018.logstorage: dbPath: /var/lib/mongodb27018processManagement: pidFilePath: /var/lib/mongodb27018/mongod_27018.pid##启动实例mongod -f /etc/mongod_27017.confmongod -f /etc/mongod_27018.conf 诊断配置(diagnostic configuration)以下配置选项可控制各种mongod行为，用以诊断的目的： operationProfiling.mode设置database profiler level。profiler在默认情况下不处于活动状态，因为它本身可能会影响性能。除非启用它，否则不会对查询进行分析； operationProfiling.slowOpThresholdMs配置慢操作的阈值以确定查询是否慢，用以作为分析器记录日志的目的。默认阈值是100ms； systemLog.verbosity控制mongod写入日志的日志输出量。只有在遇到未在正常日志记录级别中反映的问题是才启用此选项。 升级(upgrade)到最新的MongoDB修订(revisions)提供了security patches、bug fixes以及不包含任何反向破坏更改的新的或更改的功能。但是，最新版本也可能存在一些兼容性问题，请注意。 升级之前(before upgrading) 确保备份了最新的数据集； 有关特定MongoDb版本的特殊事项和兼容性问题，请注意查看； 如果你的安装包包括了复制集，在升级期间预定维护窗口(maintanence window)。 升级程序(upgrade procedure)在升级之前请一定要备份所有数据！ 按照如下步骤升级： 对于使用认证的部署，首先升级所有的MongoDB drivers； 升级分片集群； 升级任一standalone实例； 升级不属于分片集群的任一副本集。 升级一个MongoDB实例要升级mongod或mongos实例，使用如下方法之一： 使用操作系统的包管理工具和官方MongoDB包进行升级(推荐的方法)； 使用新二进制文件替换现有二进制文件来升级实例。 替换现有二级制文件(replace the existing binaries)在升级MongoDB前请一定备份你的所有数据！ 首选的升级方式是使用包管理工具和官方的MongoDB包。 通过替换现有二进制文件来升级mongod或mongos实例，执行如下操作： 下载最新MongoDB二进制文件到本地，并解压缩到MongoDB安装目录； 关闭实例； 替换二进制文件； 重启实例。 升级分片集群 禁用分片集群的平衡器(blancer)； 升级配置服务器(config-server)； 升级每个分片； 升级每个mongos实例； 重新启用平衡器。 升级复制集若要升级复制集，请单独升级每个副本集成员。从Secondary开始，最后以Primary结束。 升级SECONDARY 升级SECONDARY的mongod实例； 升级一个Secondary之后，在升级下一个实例之前，请等待Secondary恢复(recover)到SECONDARY state。使用rs.status()命令来检查复制集成员的状态。 升级PRIMARY 使用rs.stepDown命令来退出primary，以启动正常的故障转移过程； 查看是否有另外的SECONDARY节点成为了PRIMARY节点； 关闭并升级实例。 管理mongod进程开启mongod进程1234567891011121314151617mongod#指定数据目录mongod --dbpath /dir/mongodb/#指定TCP端口mondod --port 12345#将mongod以守护进程的方式启动mongod --fork --logpath /var/log/mongod.log#其他选项mongod --help 停止mongod进程1234567891011121314151617#使用shutdownServer()use admindb.shutdownServer()#使用--shutdownmongod --shutdown#使用ctrl+cctrl+c#使用kill#千万不要使用kill -9(SIGKILL)来终止mongodkill mongod_pidkill -2 mongod_pid 停止一个复制集步骤： 检查SECONDARY的oplog的时间戳； 如果从节点的时间戳落后于主节点10s内，mongod将会返回不会被关闭的消息。你可以传递一个timeoutSecs参数给shutdown命令来等待从节点追上主节点； 一旦从节点追上进度或60s后，主节点将会关闭。 强制关闭复制集：db.adminCommand( { shutdown: 1, force: true } ) 如果没有节点能立刻更新到最新的数据，发送shutdown加上timeoutSecs参数来在指定的时间内保持对从节点的检查。如果在分配的时间内有任意的一个从节点追上，主节点将会关闭。反之，主节点不会关闭。 1234db.adminCommand(&#123; shutdown: 1, timeoutSecs: 5 &#125;)#或db.shutdownServer(&#123; timeoutSecs: 5&#125;) 终止(Terminate)运行的操作MongoDB提供了两种方法来终止正在运行的操作。 maxTimeMS() db.killOp() maxTimeMS()maxTimeMS()方法给一个操作(operation)设置了时间限制(time limit)。这个时间单位默认是毫秒(ms)。当这个操作达到了指定的时间限制时，MongoDB将在下一个中断点(interrupt point)中断这个操作。 栗子： 123456789101112131415db.location.find( &#123; "town": &#123; "$regex": "(Pine Lumber)", "$options": 'i' &#125; &#125; ).maxTimeMS(30)db.runCommand( &#123; distinct: "collection", key: "city", maxTimeMS: 45 &#125;) killOpkillOp()方法将在下一个中断节点中断正在运行的操作。killOp()方法通过操作ID(operation ID)来标识目标操作。 栗子： 12345db.killOp(&lt;opID&gt;)#查看正在运行的操作db.currentOp() 注意：终止正在运行的操作时一定要谨慎！只使用db.killOp()方法来终止由客户端发起的操作，而不要终止内部数据库(internal database)的操作。 轮询(rotate)日志文件当使用--logpath选项或systemLog.path设置时，mongod或mongos实例会将所有活动和操作的实时账户报告给日志文件。默认情况下，只有当使用了logRotate命令，或者mongod或mongos进程从操作系统接收到一个SIGUSR1信号时，才会进行日志轮询响应。 MongoDB的标准日志轮询方法会存档当前日志文件并启动一个新的日志文件。为此，mongod或mongos实例将通过ISODate日期格式的UTC时间戳来重命名当前日志文件。然后它会打开一个新的日志文件，关闭旧的日志文件，并将所有新的日志发送到新的日志文件。 你也可以通过配置MongoDB的systemLog.logRatate或--logRotate选项，来支持Unix/Linux的日志轮询功能。最后，你可以使用--syslog选项来配置mongod发送日志数据到系统日志。在这种情况下，你可以选用其他的日志轮询工具。 默认日志轮询行为在mongo shell中轮询日志： 123456789101112131415#开启一个实例mongod -v --logpath /var/log/mongodb/test01.log#列出日志文件ls /var/log/mongodb/test01.log*#轮询日志文件mongo&gt;use admin&gt;db.runCommand(&#123; logRotate: 1 &#125;)#查看新的日志文件ls /var/log/mongodb/test01.log*#new: test01.log#old: test01.log-2018-01-11T08-22-50 使用--logRotate reopen选项轮询日志： 1234567mongod -v --logpath /var/log/mongodb/test01.log --logRotate reopen --logapendls /var/log/mongodb/test01.log*mongo&gt;use admin&gt;db.runCommand(&#123; logRotate: 1 &#125;) 系统日志轮询(Syslog log rotate)1mongod --syslog 使用SIGUSR1强制日志轮询对于基于Unix/Linux的系统，可以使用SIGUSR1信号来轮询单个进程的日志。 1kill -SIGUSR1 &lt;mongod-pid&gt; 数据中心意识(data center awareness)MongoDB部署中的分离(segregation)操作MongoDB拥有许多特性，包括允许数据库管理员和开发者在部署数据库的过程中通过一些功能或地理组群对数据库应用进行分割操作。 MongoDB支持跨越不同维度的操作的分段，这可能包括了在单个数据中心(single data center)的部署中的多数据中心(multi-date center)部署、机架、网络或电源电路的多个数据中心和地理区域。 MongoDB还支持基于功能或操作参数的数据库分离操作，以确保某些mongod实例仅用于报告工作负载，或只在特定的分片上分离集合的某些高频部分。 特别是在MongoDB中你可以： 确保写操作传播到复制集的特定成员； 确保复制集中的特定成员响应了查询操作； 确保分片键在具体范围上的平衡，并且驻留在特定的分片上。 区域(zone)管理分片区域(manage shard zones) 按位置分割数据(segementing data by location) 为SLA或SLO改变分层硬件 按应用程序或客户分割数据 Distributed Local Writes for Insert Only Workloads 管理分片区域 MongoDB备份方案(backup methods)在生存中部署MongoDB时，如果发生数据丢失的事件，你应该指定一个捕获和恢复备份的策略(strategy)。 back up with MongoDB cloud manager or Ops manager MongoDB Cloud Manager Ops Manager 复制底层数据文件进行备份(back up by copying underlying data files) 使用文件系统快照备份(back up with filesystem snapshots) 你可以通过复制MongoDB的底层数据文件来创建MongoDB部署的备份。如果MongoDB储存其数据文件的卷(volume)支持时间点快照(point-in-time snapshots)，则可以使用这些快照在某个时刻创建MongoDB系统的备份。文件系统的快照是一个操作系统的卷管理器的功能，并没有具体到MongoDB。通过文件系统快照，操作系统将卷的快照用作数据备份的基准。快照的机制取决于底层的存储系统。例如，在Linux上，逻辑卷管理器(LVM)可以创建快照。 要获得运行中的MongoDB进程的正确快照，必须启用日志记录(jorunaling)，并且日志必须与其它MongoDB数据文件存储在相同的逻辑卷上。如果没有启用日志记录，则无法保证快照将是一致有效地。 为了获得分片集群一致的快照，你必须禁用平衡器(balancer)和捕捉每一个分片的快照以及大约在同一时刻的配置服务器。 使用cp或scp备份 如果你的系统不支持快照功能，则可以使用cp，rsync或类似的工具直接复制文件。由于复制多个文件不是原子操作，因此你必须在复制文件之前停止对mongod的所有写入。否则，你将复制处于无效状态的文件。 复制底层数据而产生的备份不支持复制集的时间恢复节点，并且难以管理更大的共享集群。此外，这些备份很大。因为它们包括索引和复制底层存储填充和分片。相反，mongodump会创建较小的备份。 使用mongodump备份 如果在mongodump创建备份的同时，应用程序对数据进行修改，那么mongodump将会与这些应用竞争资源。 mongodump从一个MongoDB数据库中读取数据，并创建高保真度(high fidelity)的BSON文件。mongorestore工具可使用这个文件来进行MongoDB数据库恢复。mongodump和mongorestore是用于备份和恢复小型MongoDB部署的简单和高效的工具，但对于捕获较大的系统并不理想。 mongodump和mongorestore针对正在运行的mongod进程进行操作，可以直接操纵底层的数据文件。默认情况下，mongodump不会捕获local database数据库的内容。 mongodump只捕获数据库中的文档(documents)，用以给备份节省空间，但mongorestore或mongod必须在恢复数据之后重建索引。 当连接到MongoDB实例时，mongodump可能会对MongoDB的性能产生不利影响。如果你的数据大小大于系统内存，查询可能会将工作单元从内存中推开，从而导致页面错误。 当mongodump在捕获输出时，应用程序可以继续修改数据。对于复制集来说，mongodump提供了--oplog选项来用以在mongodump操作期间包含数据的oplog条目。这允许相应的mongorestore操作去还原所捕获的oplog。 然而，对于复制集来说，请考虑使用MongoDB Cloud Manager 或 Ops Manager来备份。 使用文件系统快照进行备份和恢复(back up and restore with filesystem snapshots)使用系统工具创建MongoDB系统的备份，诸如LVM，或block-level备份方法。使用系统工具来创建MongoDB数据文件的设备的副本。这些方法完成迅速、工作可靠，但是需要在MongoDB之外进行额外的系统配置。 快照综述(snapshots overview)快照的工作方式是在实时数据(live data)和一个特定快照卷之间创建指针(pointer)。这个指针在理论上等同于硬链接(hard link)。作为工作数据偏离的快照，快照过程使用写时复制(copy-on-write)策略。结果，快照又只存储修改的数据。 创建快照后，在文件系统上挂载(mount)快照镜像，并从中复制数据。生成的备份包含所有数据的完整副本。 Valid database at the time of snapshot 当快照生成时数据库必须有效。这就意味着数据库所接收的所有写入(write)都需要完整的写入磁盘————无论是journal还是数据文件。如果备份发生时磁盘上没有写入(write)，备份将不反映这些更改。 对于WiredTiger storage engine，数据文件反映了最后一个检查点(last checkpoint)的一致状态。每2GB的数据或每分钟就会出现检查点。 Entire disk image 快照创建一个整个磁盘镜像的镜像。除非你需要备份你的整个系统，否则考虑隔离(isolate)你的MongoDB数据文件、journal，并配置一个不包含任何其他数据的逻辑磁盘。或者，将所有的MongoDB数据文件保存在一个专用的设备上，这样你就可以在没有重复(duplicating)和无关(extraneous)数据的情况下进行备份。 Site failure precaution 确保将数据从快照复制到其他系统。这确保了在站点故障(site failure)的时候数据是安全的。 No incremental backups 本教程不包含增量备份(incremental backups)的过程。虽然不同的快照方法提供了不同的功能，但下面列出的LVM方法不提供捕获增量备份的任何容量。 Snapshots with journaling 如果你的mongod实例启用了journaling，则可以使用任何类型的文件系统和volume/block level快照工具来创建备份。 如果你在基于Linux的系统上管理你自己的基础架构，请使用LVM配置你的系统以提供磁盘包并提供快照功能。 在Linux上使用LVM进行备份和还原生产备份系统必须考虑一些特定环境的应用程序特定需求和因素。 Crete a snapshot 确保你创建的快照具有足够的空间来考虑数据的增长； 如果快照超出了空间，快照镜像将无法使用。请放弃这个逻辑卷并创建另外一个； 命令执行完毕时快照将存在。你可以随时直接从快照进行还原，也可以创建新的逻辑卷并从此快照还原到备用镜像； 虽然快照对于快速创建高质量的备份非常好，但它们并不是理想的作为存储备份数据的格式； 快照通常取决于并位于与原始磁盘镜像相同的存储基础架构上。因此，将这些快照存档并将其存储在别处至关重要。 12345#下面的这个vg-name指卷组名，这个卷组首先需要建立#系统卷组和设备的位置和路径可能因LVM的配置二略有不同#此大小不反映数据大小lvcreate --size 1G --snapshot --name mongodb-snap20180111 /dev/vg-name/mongodb Archive a snapshot 创建好snapshot之后，挂载mount快照并将数据复制到单独的存储中。 压缩快照： 12umount /dev/vg-name/mongodb-snap01dd if=/dev/vg-name/mongodb-snap01 | gzip &gt; mongodb-snap01.gz Restore a snapshot 同样适用LVM进行还原。 12345#lv-mongodb, vg0-vgnamelvcreate --size 1G --name mongodb vg0gzip -d -c mongodb-snap01.gz | dd of=/dev/vg0/mongodbmount /dev/bg0/mongodb /dir/path 还原的快照中有一个陈旧的mongo.lock文件，如果你没有从快照中删除此文件，那么MongoDB可能会认为锁文件指示的是不正常的关闭。如果你开启了storage.journal.enabled，但没有使用db.fsyncLock()的话，那不需要删除mongo.lock文件，反之，删除它。 Restore directly form a snapshot 不使用gz压缩文件下还原备份。 1234umount /dev/vg-name/mongodb-snap01lvcreate --size 1G --name mongodb vg0dd if=/dev/vg0/mongodb-snap01 of=/dev/vg0/mongodbmount /dev/vg0/mongodb /dir/path Remote backup storage 可以使用组合的进程和SSH实施离线备份。 12345umount /dev/vg-name/mongodb-snap01dd if=/dev/vg0/mongodb-snap01 | ssh user@host gzip &gt; /dir/path/mongodb-snap01.gzlvcreate --size 1G --name mongodb vg0ssh user@host gzip -d -c /dir/path/mongodb-snap-01.gz | dd of =/dev/vg0/mongodbmount /dev/vg0/mongodb /dir/path 使用单独卷上的Journal日志文件或没有Journal日志文件进行备份实例从MongoDB3.2开始，为了使用WiredTiger对MongoDB实例进行volume-level备份，数据文件和Journal日志文件不再要求驻留在一个卷上。 如果你的mongod实例没有使用Journal，或者启用了将Journal志文件放置于一个单独的卷上，则必须刷新(flush)对磁盘的所有写入，并在备份期间锁住数据库用以阻止写操作。如果有复制集(replica set)配置，那么你可以在SECONDARY上不接收读取用以备份数据。 1. 刷新写入磁盘并锁定数据库以防止进一步的写入： 12#锁住数据库db.fsyncLock(); 2. 使用快照备份数据库： 3. 解锁数据库： 12#解锁数据库db.fsyncUnlock(); 使用MongoDB工具进行备份和恢复(back up and restore with MongoDB tools)使用MongoDB提供的备份还原工具——mongodump和mongorestore来处理BSON data，对于创建小型部署的备份是很有用的。对于弹性(resilient)备份和非破坏性(non-disruptive)备份，使用文件系统或块级磁盘快照。 因为mongodump和mongorestore操作通过与正在运行中的mongod实例进行交互(interacting)，它们会影响正在运行的数据库的性能(performance)。这些工具不仅会为正在运行的数据库实例创建流量，还会强制数据库通过内存读取所有的数据。当MongoDB读取不经常(infrequently)使用的数据时，它会驱逐(evict)频繁(frequently)访问的数据，导致数据库正常工作负载的性能下降。 当使用MongoDB’s tools 来备份你的数据时，考虑如下建议： 标签文件(label file)，以便你可以识别备份的内容以及备份所反映的时间点 如果对你来说，mongodump和mongorestore对性能的影响是不可接受的，请使用替代备份策略——filesystem snapshot或MongoDB CloudManager 使用--oplog去捕获在mongodump期间的传入写(write)操作，以确保备份一致性的数据状态 通过将备份文件还原到测试环境中，以确认备份是可用的 MongoDB toolsMongoDB工具介绍及区别： mongoexportmongoexport is a utility that produces a JSON or CSV export of data stored in a MongoDB instance. mongoimportThe mongoimport tool imports content from an Extended JSON, CSV, or TSV export created by mongoexport, or potentially, another third-party export tool. mongodumpmongodump is a utility for creating a binary export of the contents of a database. mongodump can export data from either mongod or mongos instances.mongodump excludes the content of the local database in its output.The mongodump utility backs up data by connecting to a running mongod or mongos instance. mongorestoreThe mongorestore program writes data from a binary database dump created by mongodump to a MongoDB instance. 步骤(Procedures)使用mongodump备份 `mongodump·备份数据库，如果数据库启用了访问控制，则必须拥有每个备份的数据库查询的权限。内置的备份角色提供了执行任何和数据库备份有关所需的权限。 这就意味着你使用mongodump的user必须要对所备份的数据库有读取权限。 mongodump能够为整个服务器、数据库或集合创建备份，或者使用查询仅备份集合的一部分。 mongodump默认排除local数据库。 mongodump必须要能够连接到正在运行的mongod或mongos实例。默认连接为127.0.0.1:27017。 mongodump默认创建在当前目录下创建./dump备份文件。 如果mongodump备份目录中已经存在备份数据目录，那么mongodump将会覆盖它们。 指定认证库来认证你的用户名和密码。 使用oplog进行时间点操作 在mongodump中使用--oplog选项来收集oplog条目，用以在副本集中构建数据库的实时快照。 使用--oplog，mongodump会从源数据库复制所有的数据，包括备份开始到结束这段时间所有的oplog记录。 在mongorestore还原时使用--oplogReplay选项，允许你还原特定时间节点的备份。这就对应在mongodump期间oplog的记录。 123456789101112131415161718192021#127.0.0.1:27017 ./dumpmongodump#--host,-h --portmongodump -h mongodb.example.net --port 27107mongudump -h 127.0.0.1 --port 27018#-o, --outmongoodump -o /var/mongodb_backup/mongodump --host 127.0.0.1 --port 27017 --out /var/mongodb_backup/#--collection, --dbmongodump --db zhang --out /var/mongodb_backup/zhangmongodump --db zhang --collection test#--authenticationDatabasemongodump --port 27018 -u zhang -p "passwd" --authenticationDatabase admin -d zhang -o /var/mongodb_backup/zhang 使用mongorestore还原若要将数据还原到启用了访问控制的MongoDB部署，如果备份数据不包括system.profile集合数据，则restore角色提供了对数据库的访问权限。 如果备份数据包含了system.profile集合并且目标数据库不包含system.profile集合，那么mongorestore会去创建这个集合即使mongorestore并没有还原system.profile文档。因此，用户就需要额外的权限才能在system.profile集合中上执行createCollection和convertToCapped。 如果使用--oplogReplay，这个restore角色还不足以重放oplog。所以如果需要重放oplog，请使用一个能够重放oplog的角色。 1234567mongorestore /var/mnogodb_backupmongorestore /var/mnogodb_backup --oplogReplaymongorestore --port 27018 -u zhang -p "passwd" --authecticationDatabase admin -d zhang /var/mongodb_back/zhang 批量化操作mongo shell(EOF)1234567for coll in &#123;collection1,collection2,...&#125;do mongo host:port/db -u x -p xx &lt;&lt; EOF use db db.$coll.drop() EOFdone 从MongoDB备份中还原副本集你不能将单个数据集(data set)还原为三个新的mongod实例，然后为此创建一个副本集(replication set)。如果你将数据集复制到每个mongod实例，然后创建副本集，则MongoDB将强制SECONDARY执行initial sync。 向一个单一副本集节点中还原数据(Restore Database into a Single Node Replica Set) 获取备份数据库文件 使用备份数据库文件作为数据库路径启动一个mongod实例 1234567891011#方法1，直接启动mongod --dbpath /dir/path/mongodump --replSet &lt;replName&gt;#方法2，使用配置文件启动，推荐vim /etc/mongod.confstorage: dbPath: /dir/path/mongodumpreplication: replSetName: zhang 连接到mongo shell 初始化这个新的副本集 12#对于有且仅有一个成员的副本集使用rs.initiate()rs.initiate() 向副本集中添加成员(Add Members to the Replica Set)MongoDB对于还原副本集SECONDARY节点提供了两种选择： 手动复制数据库文件到数据目录 允许initial sync 建议： 如果备份的数据库文件很大，那么initial sync可能需要很长的时间才能完成。对于大型数据库，最好将数据库文件复制到每台主机上。 Copy Database File and Restart mongod Instance Shut down the mongod instance that you restored 使用 --shutdown 或 db.shutdownServer()来确保一个正常干净的关闭 复制Primary的数据目录到每个从节点 Start the mongod instance that you restorerd Add the secondaries to the replica set 1PRIMARY&gt;rs.add() Update Secondaries using Initial Sync 确保副本集成员的数据目录为空 将每个潜在成员添加到副本集 备份和还原分片集群(sharded cluster) 通过文件系统快照(fs snapshots)备份一个分片集群 通过Database Dumps备份一个分片集群 Schedule Backup Window for Sharded Clusters 还原一个分片集群 从意外关闭中恢复(Recover a standalone after an unexpected shutdow)当一个standalone模式的mongod实例关闭了journaling功能后，一个unclean的shutdown可能会导致数据处于不一致的状态。当unclean shutdown之后，如果在dbPath下存在一个非空的mongod.lock文件，则mongod实例会记录如下信息： Dectected unclean shutdown - mongod.lock is not empty 这样的话你必须要修复你的数据库，才能正常的启动mongod。 警告：不要用如下方法处理副本集 unclean shutdown。相反，你应该从备份或者从另一个副本集的成员恢复。 默认情况下，MongoDB在启用journaling的情况下运行，以防止发生unclean shutdown时数据不一致的问题。 使用运行mongod实例的那个用户来进行修复，避免由权限不一致而导致的新问题。 Create a backup of the data files Start mongod with –repair 监控(Monitoring)MongoDB监控是数据库管理的重要组成部分，充分了解MongoDB的运行状态，并在没有危机的情况下维护和部署。此外，了解MongoDB的正常操作参数将允许你在问题升级成为故障前诊断他们。 Monitoring for MongoDB Monitoring Strategies(策略)有三种方法可以从运行中的MongoDB实例中收集状态信息： MongoDB提供的一组实时上报程序，提供数据库活动的实时报告； 数据库命令以更大的保真度返回有关当前数据库状态的统计信息； MongoDB Atlas，MongoDB Cloud Manager； 每个策略在不同的情况下都是很有用的，所以它们能够很好地进行互补。 MongoDB Reporting ToolsUtilities MongoDB提供了许多可以返回活动统计信息的实用工具，这对于诊断问题和评估操作非常有用。 mongostat mongostat按类型捕获并返回数据库操作的计数(insert,query,update,delete…) mongotop mongotop通过类型捕获和返回数据库操作(insert,query,update,delete) CommandsMongoDB包含了许多上报数据库状态的命令。这些命令可以提供比上面的实用程序更精细的粒度级别。考虑在脚本和程序中使用它们的输出来开发自定义警报。db.currentOp方法是一个识别当前数据库实例正在进行的操作。 db.serverStatus() db.serverStatus()，返回数据库状态的一般概述，详细的磁盘使用，内存使用，连接，journaling日志和索引访问。它返回快速并不影响MongoDB性能。 db.stats() db.stats()，提供了database上的统计信息。返回使用的存储量，数据库包含的数据量及对象，collection和索引计数器。 db.collection.stats() db.collection.stats()，提供了collection上的统计信息。包含集合中的对象数量，结合大小，集合磁盘空间用量，索引信息。 rs.status() rs.status()，返回一个复制集状态的概述。 第三方工具许多第三方(third party)工具支持对MongoDB的监控。 Nagios Zabbix Ganglia Motop … Monitor MongoDB with SNMP on LinuxSNMP is only available in MongoDB Enterprise Monitor MongoDB Windows with SNMP MongoDB索引Indexes 索引支持在MongoDB中高效地(effecient)执行查询。没有索引，MongoDB就必须采取collection scan。扫描每个集合中的每个文档，用以匹配查询。如果查询存在适当的索引，则MongoDB可以使用该索引来限制它必须检查的文档数量。 索引是特殊的数据结构，将集合数据集中的一小部分以易于遍历(traverse)的形式存储。索引存储特定字段或字段集的值，按字段值排序。索引条目的排序支持高效的相等匹配和基于范围的查询操作。除此之外，MongoDB可以使用索引中的排序返回排序后的结果。 从根本上来说(fundamentally)，MongoDB中的索引类似于其他数据库的索引。MongoDB在collection级别定义索引，并支持集合的文档的任何字段或子字段上的索引。 默认_id索引 在创建一个collection期间，MongoDB在_id字段上创建一个唯一的索引。你也可以自定义_id的值。你不能在_id字段上删除此索引。 创建一个索引 db.collection.createIndex方法只有在同一规范不存在时才创建索引。 1db.collection.createIndex(&lt;key and index type&gt;, option) 索引类型MongoDB提供了许多不同的索引类型来支持特定类型的数据和查询。 Single Field 除了MongoDB定义的_id索引，MongoDB还支持在文档的单个字段上创建用户自定义的升序(ascending)/降序(descending)索引。 对于单字段索引和排序操作，MongoDB可以在任何方向遍历索引。 Compound(复合) Index MongoDB也支持多个字段的用户自定义索引。 Multikey Index MongoDB使用多键索引来索引存储在数组中的内容。 Geospatial(地理空间) Index 为了支持对地理空间坐标数据的有效查询，MongoDB提供了两个特殊的索引：2d index返回平面几何的2D索引；2dsphere index返回球形几何结果。 Text Index MongoDB提供了一个文本(text)类型索引，用以支持搜索集合中的字符串内容(string content)。 Hashed(散列) Index 为了支持基于散列的分片，MongoDB提供了散列索引类型，它索引字段值的散列值。但只支持相等的匹配，而不能支持基于范围的查询。 Index Properties(特性) Unique Index 索引的唯一性是MongoDB拒绝索引字段的重复值。 Partial Index 部分索引仅索引复合指定过滤器表达式的集合中的文档。 Sparse(稀疏) Index 索引的稀疏属性确保索引仅包含具有索引字段的文档的条目，跳过没有索引字段的文档。 TTL Index TTL索引是MongoDB可以用来在一定时间后自动从集合中删除文档的特殊索引。对于某些类型的消息，如机器生成的事件数据，日志和会话信息等，只需在数据库库中保存有限的时间，这是非常理想的。 Index Use索引能够提高读操作的效率。 Index and Collation要使用索引进行字符串比较，操作还必须指定相同的排序规则。 Coverd Query当查询条件和查询投影仅包含索引字段时，MongoDB将直接从索引返回结果，而不扫描任何文档或将文档带入内存。 Single Filed Index Compound Index Multikey Index Text Index 2dsphere Index 2d Index geoHaystack Index Hashed Index Index Property Index Build Operation on a Populated Collection Index Intersection Manage Index Measure Index Use Indexing Strategy Index Reference MongoDB存储Storage FAQ: MongoDB Storage: https://docs.mongodb.com/v3.4/faq/storage/ 存储引擎(storage engine)是MongoDB管理数据库主要的组件。 journal日志，用于数据库不正常关闭时修复数据库。有几种可选的配置项，用以平衡数据库的性能和可用性。 GridFS是一个适合处理大文件的多功能的存储系统，例如那些超过16MB文档大小限制的文件。 Storage Engine存储引擎是数据库的组件，负责管理数据库在内存(in-memory)和磁盘中(on-disk)两种存储方式。由于不同的存储引擎在特定的工作负载下有更好的性能，所以，为你的应用程序选择一个适当的存储引擎会提高性能。 WiredTiger是从MongoDB3.2开始的默认存储引擎。它非常适合大多数工作负载，并推荐使用它来进行部署。WiredTiger提供了文档级并发模型，检查点和要说等特性。 MMAPv1是一个原始的MongoDB存储引擎，它是MongoDB3.2以前的默认存储引擎。它在大量读取和写入以及更新方面的工作负载表现良好。 In-Memory要在MongoDB Enterprise中才能获取。它不是将文档保存在磁盘上，而是将它们保留在内存中，以获得可预测的数据延迟。 WiredTiger存储引擎MongoDB3.2以后使用WiredTiger存储引擎作为默认存储引擎。 12345678mongod --storageEngine wiredTiger#或vim /etc/mongod.confstorage: engine: wriedTiger 文档级别并发(currency)WiredTiger使用文档级并发来控制写操作。因此，多个客户端可以同时修改一个集合中的不同文档。 对于大多数读写操作，WiredTiger使用乐观的并发控制。WiredTiger仅在global、database和collection-levels使用intent lock。当存储引擎检测到两个操作之间的冲突时，其中一个操作将引发写冲突，从而导致MongoDB透明地重试该操作。 快照和检查点WiredTiger users multiVersion Concurrency Control(MVCC).在操作开始时，WiredTiger向事务提供数据的实时快照。快照显示内存中数据的一致性视图。 当写入磁盘时，WiredTiger将快照中的所有数据以一致性的方式跨越所有数据文件写入磁盘。持久(durable)的数据充当数据文件中的检查点。检查点确保数据文件与最后一个检查点保持一致性，并包括最后一个检查点。 MongoDB配置WiredTiger来创建检查点(即将快照数据写入磁盘)，间隔时间为60s，或2G日志数据。 在写入新检查点期间，前一个检查点仍然有效。 当WiredTiger的元数据表被原子地更新以引用新的检查点，新的检查点将变得可访问和永久。一旦新检查点可以访问，WiredTiger就会从旧的检查点这种释放页面(free page)。 JournalWiredTiger采用预写事务日志联合检查点，用以确保数据的持久性(durability)。你也可以关闭journal功能来减少维护日志的开销。 WiredTiger日志坚持在检查点之间修改所有数据。如果MongoDB在检查点之间退出，它将使用日志重放自上一个检查点以来修改的所有数据。 WiredTiger journal使用snappy compression Library来进行压缩。 WiredTiger最小日志记录的大小是128Byte，如果日志记录小于等于128Byte，则WiredTiger不会压缩日志文件。 对于以standalone模式运行的mongo实例，关闭journal日志功能意味着当MongoDB意外地在检查点之前退出时，你将丢失一些数据修改。对于复制集成员，复制过程和恒提供足够的持久性保证。 Compression使用WiredTiger，MongoDB支持压缩所有的collections和indexes。通过使用CPU进行压缩，减少了储存空间的使用。 默认地，WiredTiger使用snappy compression library对所有的collections进行block压缩，对所有索引进行前缀(prefix)压缩。 对于collection，也可以使用zlib进行block压缩。可通过storage.wiredTiger.collectionConfig.blockCompressor设置压缩方法。对于index，使用storage.wiredTiger.indexConfig.prefixCompression关闭prefix压缩。 对于大多数工作负载，默认压缩设置平衡了存储效率和处理要求。 Memory Use对于WiredTiger，MongodB使用WiredTiger内部缓存和文件缓存。 从MongoDB3.4开始，WiredTiger内部缓存将使用一下两种类型中更大的一种： 50% of RAM minus 1GB 256MB WiredTiger内部缓存中的数据与磁盘上格式的数据使用不同的表现形式： 文件系统缓存的数据与磁盘上的格式相同，包括了对数据文件进行压缩的好处。操作系统使用文件系统缓存来减少磁盘I/O 指标加载在WiredTiger内部缓存有一个不同的磁盘上的数据表示格式，但仍然可利用 prefix index compression来减少内存使用。索引前缀压缩重复数据删除常用前缀的索引字段。 WiredTiger内存缓存的collection数据是未压缩的，并使用与磁盘格式不同的表现形式。block compression能够节省大量磁盘空间，但必须解压缩数据后服务器才能操作。 通过文件系统缓存，MongoDB自动使用 (WiredTiger缓存或其他进程不使用)空闲内存。 调整WiredTiger内部缓存的大小，避免将WiredTiger的内初缓存值增加到默认值之上。 1234567#命令行--wiredTigerCacheSizeGB#配置文件storage.wiredTiger.engineConfig.cacheSizeGB Change Standalone to wiredTigerMongoDB version 3.0 or later in order to use wiredTiger storage engine! 过程： mongod is running export data using mongodump create a data directory for the new mongod running with wiredTiger start mongod with wiredTiger upload the dumpdata using mongorestore Change Replica Set to wiredTigerReplica sets can have members with different storage engines.因此，你可以把所有成员的存储引擎更换为WiredTiger。MongoDB version 3.0 or later in order to use wiredTiger storage engine! 过程： shutdown the secondary member.–db.shutdownServer prepare a data directory for the new mongod running with wiredTiger start mongod with wiredTiger repeat the procedure for other replica set secodaries you wish to upgrade Change Sharded Cluster to wiredTigerif the shard is a standalone, see Change Standalone to wiredTiger;if the shard is a replica set, see Change Replica Set to wiredTiger. Change config server to wriedTiger如果你打算更新config server使用WiredTiger，那么必须全部更新！ 过程： disable the balancer–sh.disableBalancer() shutdown the third config server to ensure read-only metadata.–db.shutdownServer() export the data of the second config server with mongodump For the second config server, create a new data directory for use with WiredTiger. Stop the second config server.–db.shutdownServer() Start the second config server mongod with the WiredTiger storage engine option. Upload the exported data using mongorestore to the second config server. Shut down the second config server to ensure read-only metadata.–db.shutdownServer() Restart the third config server to prepare for its upgrade. Export the data of the third config server with mongodump For the third config server, create a new data directory for use with WiredTiger. Stop the third config server. Start the third config server with the WiredTiger storage engine option. Upload the exported data using mongorestore to the third config server. Export data of the first config server with mongodump. For the first config server, create a new data directory for use with WiredTiger. Stop the first config server. Start the first config server with the WiredTiger storage engine option. Upload the exported data using mongorestore to the first config server. Restart the second config server to enable writes to the sharded cluster’s metadata Re-enable the balancer.–sh.startBalancer() MMAPv1存储引擎 In-Memory存储引擎 Journaling为了在发生故障时提供持久性，MongoDB使用了县写日志记录到磁盘的日志文件。To provide durability in the event of a failure, MongoDB uses write ahead logging to on-disk journal files. journaling and the wiredTiger storage engine本节所指的log指的是WiredTiger的 write-ahead log(journal)，而不是MongoDB日志文件。 WiredTiger使用checkpoints在磁盘上提供一致的数据视图，并允许MongoDB从上一个checkpoint修复。然而，如果MongoDB在检查点之间以外退出，则需要使用journaling来修复上次检查点之后发生的信息。 使用journaling的修复过程： 在数据文件中查找上一个检查点的标识符(identifier) 在journaling文件中搜索与上一个检查点标识符匹配的记录 应用自上一个检查节点依赖journal文件中的操作 journal process通过journaling，WiredTiger为每个客户端启动的写操作创建一个journal记录。journal record包括有初始写入引起的任何内部写入操作。 例如，集合中文档的更新可能导致对index的修改，WiredTiger创建一个包含update操作及其相关index修改的单独的journal record。 MongoDB将WiredTiger配置为in-memory的buffering来存储日志记录。线程坐标来分配和复制到他们的缓冲区的一部分。所有日志记录高达128KB是缓存的。WiredTiger根据如下条件将journal record同步到磁盘。 每50ms MongoDB在WiredTiger中设置60s为间隔的用户数据检查点或2GBjournal数据已被写入，以先发生为准。 如果写操作包含有j:true的写关注点，则WiredTiger强制对journal文件进行同步。 MongoDB限制了journal文件大小为100MB，因此WiredTiger每100MB就会创建一个新的journal文件。当创建了一个新的journal文件时，WiredTiger会同步上一个journal文件。 在写操作之间，虽然日志记录保留在WiredTiger缓冲区中，但在mongod实例hard shutdown之后可能会丢失更新。 Journal FileMongoDB在数据库目录下创建一个journal子目录存放journal文件。名字为WiredTigerLog.&lt;sequence&gt;，从0000000001开始。如上图所示。 Journal文件包含对每一个写操作的记录。每个记录都有唯一的标识符。 MongoDB将WiredTiger配置为对journal数据使用快速压缩。最小日志大小为128KB，如果小于此，WiredTiger不会压缩此记录。最大大小为100MB，超过此，WiredTiger会创建一个新的journal文件。 MongoDB自动删除旧日志文件，以维护从上一个检查点恢复所需的文件。 Journaling and the MMAPv1 Storage Engine Journaling and the In-Memory Storage Engine Manage JournalingMongoDB uses write ahead logging to an on-disk journal to guarantee write operation durability. 启用journal后，如果MongodB意外退出，则程序可以恢复写入了journal日志文件的所有内容。MongoDB将在重启时重新应用写操作，并保持一致性。 过程 Enable journaling123456789mongod --jouranl##或vim /etc/mongod.confstorage: journal: enabled: true Disable journaling12345mongod --noJournal###或修改配置文件 警告不要在生产系统上禁用日记功能。如果在一个副本集上使用--noJournal关闭了journal日志，则还应该修改副本集配置文件。 Monitor journal status serverStatus Recover data after unexpected shutdown在奔溃后重启时，MongoDB会在服务器可用之前replay journal日志记录中的所有日志文件。 GridFSGridFS是一种用于存储和检索超过BSON文档大小限制值16MB的文件规范。 GridFS没有将单个文件存储到单个的文档中，而是将文件分割成部分(parts)或块(chunks)，并将每个块存储到单独的文档中。默认情况下，GridFS的块大小为255KB。也就是说，GridFS将文件分成255KB的块，最后一块大小就不确定了。 GridFS使用两个集合来存储文件。一个存储文件块(chunks)，另一个存储文件元数据(metadata)。 当你查询(query)GridFS文件时，驱动程序会根据需要重新组装这些块。你可对通过GridFS存储的文件执行范围查询。还可以从任意文件部分访问信息。 GridFS不仅可用于存储超过16MB的文件，还可用于存储需要访问的任何文件，而不必将整个文件加载到内存中。 何时使用GridFS在MongoDB中，使用GridFS存储大于16MB的文件。 某些情况下，在MongoDB数据库中存储大文件可能比在系统级文件系统上更有效。 如果文件系统限制了一个目录中的文件数量，则可使用GridFS存储所需的文件 当你想要访问大文件的部分信息时而不想将整个文件加载到内存中时，可使用GridFS收回文件的各个部分，而不必将整个文件读入内存 当你希望文件和元数据自动同步并部署在多个系统和设施中时，可使用GridFS 如果需要原子地(atomically)更新整个文件的内容，请不要使用GridFS。作为一种选择，你可以为每个文件存储多个版本，并在元数据中指定该文件的当前版本。 此外，如果文件都是小于16MB的BSON文件大小限制，则考虑手动存储在一个单文档中，而不必使用GridFS。 使用GridFS使用GridFS存储和检索文件，请使用如下任何一项： A MongoDB Driver The mongofile cmd-line tool GridFS集合GridFS把文件存储在两个集合里： chunks collection stores the binary chunks files collection stores the file’s metadata GridFS将这些集合放在一个普通的存储区(bucket)中，每个存储区前面加上名称。默认地，GridFS使用两个名为fs的存储区集合： fs.files fs.chunks 币可以选择一个不同的存储区名字，也可以在一个数据库中创建多个存储区。 The chunks collection块集合中的每个文档都表示一个独立的文件块。格式如下： 123456&#123; &quot;_id&quot;: &lt;ObjectId&gt;, &quot;files_id&quot;: &lt;ObjectId&gt;, &quot;n&quot;: &lt;num&gt;, &quot;data&quot;: &lt;binary&gt;&#125; 块集合中的文档包含如下字段： chunks._id The unique ObjectId of the chunk chunks.files_id The _id of the “parent” document chunks.n The sequence number of the chunk，GridFS从0开始标号所有块 chunks.data BSON Binary type file集合GridFS的file集合，格式如下： 1234567891011&#123; &quot;_id&quot;: &lt;ObjectId&gt;, &quot;length&quot;: &lt;num&gt;, &quot;chunkSize&quot;: &lt;num&gt;, &quot;uploadData&quot;: &lt;timestamp&gt;, &quot;md5&quot;: &lt;hash&gt;, &quot;filename&quot;: &lt;string&gt;, &quot;contentType&quot;: &lt;string&gt;, &quot;aliases&quot;: &lt;string array&gt;, &quot;metadata&quot;: &lt;any&gt;&#125; files._id The unique identifier for this document files.length The size of the document in bytes files.chunSize The size of each chunk in bytes files.uploadDate The date the document was first stored by GridFS files.md5 An MD5 hash of the complete file file.filename Optional. A human-readable name for the GridFS file file.contentType Optional. A valid MIME type for the GridFS file files.aliases Optional. An array of alias strings files.metadata Optional. The metadata field may be of any data type and can hold any additional information you want to store GridFS索引为了提高效率，GridFS在每个chunks and files collections上使用索引。 chunks索引GridFS使用一个唯一的、混合的索引。在chunks集合上使用files_id和n字段。 12345db.fs.chunks.find( &#123; files_id: myFileID &#125; ).sort( &#123; n:1 &#125;)#创建索引db.fs.chunks.createIndex(&#123; files_id: 1, n:1 &#125;, &#123; unique: true &#125;); files索引GridFS使用索引，在files集合上使用filename和uploadDate字段。 12345db.fs.files.find(&#123; filename: myFileName &#125;).sort(&#123; uploadDate: 1 &#125;)#创建索引db.fs.files.createIndex(&#123; filename:1, uploadDate: 1 &#125;); 分片GridFS如果需要分片GridFS数据存储，使用chunks集合设置: { files_id: 1, n:1} or { files_id: 1 }作为分片key索引。 不能对chunks集合使用hash分片。 files_id是一个ObjectId。 MongoDB安全Security MongoDB提供了各种特性(features)，如身份认证(authentication)、访问控制(access control)、加密(encryption)，以保护MongoDB部署。 Security Checklist 启用访问控制和强制认证 Enable Access Control and Enforce Authentication 可使用默认的MongoDB认证机制或现有的外部框架 配置基于角色的访问控制 Configure Role-Based Access Control 首先创建administrator，接着在创建其他用户 创建角色，定义一组用户所需的确切访问权限 加密通信 Encrypt Communication 配置MongoDB使用TLS/SSL加密连接 加密和保护数据 Encrypt and Protect Data 限制网络曝光 Limit Network Exposure 确保MongoDB运行在一个受信任的网络环境上，并限制MongoDB的监听接口 审计系统活动 Audit System Activity 跟踪对数据库配置和数据的访问和更改 使用专用用户运行MongoDB Run MongoDB with a Dedicated User 使用专用的操作系统用户账户运行MongoDB进程 使用安全配置选项运行MongoDB Run MongoDB with Secure Configuration Options MongoDB为了支持某些服务端操作执行：mapReduce,group,$where 如果你不使用这些操作，请关闭服务器端脚本执行--noscripting 请求一个安全技术执行指南 Request a Security Technical Implementation Guide 考虑安全标准合格性 Consider Security Standards Compliance 认证Authentication 要作为用户进行身份认证，必须提供用户名(username)，密码(password)和与用户关联的身份验证数据库(authentication database)。 1234567mongo --host --username --password --authenticationDatabase#Ormongo&gt;use &lt;authenticationDatabase&gt;&gt;db.auth(&apos;username&apos;,&apos;password&apos;) 认证机制 Authentication Mechanisms MongoDB支持多种认证机制 SCRAM-SHA-1 MongoDB Challenge and Response (MONGODB-CR) x.509 Certificate Authentication LDAP proxy authentication(MongoDB Enterprise) Kerberos authentication(MongoDB Enterprise) 内部认证 Internal Authentication 除了验证客户端的身份外。MongoDB还可以要求副本集和分片集的成员对其各自的成员进行认证 用户Users 要在MongoDB中验证客户端，必须向MongoDB添加相应的用户。 用户管理接口 User Management Interface 使用db.createUser()方法创建用户 添加用户时，可为用户分配角色以授予权限 在数据库管理中创建的第一个用户应该是具有管理其他用户权限的administrator 也可以更新/删除一个已经存在的用户的权限 认证数据库 Authentication Database 在特定的数据库中创建用户，这个数据库是用户的认证库 用户名和认证库充当该用户的唯一标识符。如果两个用户具有相同的用户名，但是在不同的数据库中创建，则它们是两个单独的用户 用户可拥有不同数据库的权限，而不限于认证库 通过数据库角色给用户分配相应的权限 认证一个用户 Authentication Database 使用用户名、密码、认证库验证一个用户 集中的用户数据 Centralized User Data MongoDB将所有的用户名、密码和认证库信息，保存到admin库的syste.users集合中 使用用户管理命令而不要直接访问这个集合 分片集群用户 Sharded Cluster Users 添加用户Add Users MongoDB使用基于角色的访问控制(RBAC)来确定用户的访问权限。用户被授予一个或多个角色，这些角色确定用户对MongoDB资源的访问或权限，以及用户可以执行的操作。用户应该只具有确保系统最小权限所需要的最小权限。 前提(Prerequisites) 对于用户创建，你必须拥有以下权限 在数据库中创建一个新用户，必须在数据库资源上有createUser操作 对一个用户授权角色，必须在角色数据库中有grantRole操作 栗子 123456789101112131415161718use admindb.createUser( &#123; user: &apos;zhang&apos;, pwd: &apos;passwd123&apos;, roles: [ &#123; role: &apos;root&apos; &#125;, &#123; db: &apos;admin&apos; &#125; ] &#125;)#在配置文件中开启用户认证vim /etc/mongod.confsecurity: authorization: enabled 认证机制Authentication Mechanisms SCRAM-SHA-1 MONGODB-CR x.509MongoDB对于客户端身份认证和副本集、分片集成员的内部认证支持x.509证书认证。 x.509证书认证需要安全的TLS/SSL连接。 证书授权(Certificate Authority) 在生产使用中，MongoDB的部署应该使用由认证机构签名和生成的有效证书。 Client x.509 Certificates 要想服务器验证身份，客户端可以使用x.509证书而不是用户名和密码。 Client Certificate Requirements： 单个证书颁发机构(CA)必须同时为客户端和服务器颁发证书 客户端证书必须包含如下字段： 12keyUsage = digitalSignatureextendedKeyUsage = clientAuth 每个唯一的MongoDB用户必须有一个唯一的证书 一个客户端x.509证书的主题，包含了可辨识名称(DN)。必须不同于成员x.509证书 MongoDB user and $external database 若要使用客户端证书进行认证，必须先将客户端证书中的subject值添加为MongoDB用户。每个唯一的x.509客户端证书对因孤独一个MongoDB用户。 在$external database中添加用户，认证库便是外部数据库。 Authenticate 使用x.509客户端进行身份验证，请通过TLS/SSL连接到MongoDB。--ssl and --sslPEMKeyFile Member x.509 Certificates 对于内部认证，分片集和副本集的成员可以使用x.509证书来代替使用SCRAM-SHA-1认证机制的keyfile。 Member Certificate Requirements CA必须为所有分片集，副本集成员颁发x.509证书 成员证书的主题中找到Distinguished Name(DN)必须为以下至少一个属性指定非空值：Organization(O)，Organization Unit(OU)，Domain Component(DC) 组织属性，组织单元属性和域组件必须与其他集群成员的证书相匹配。12CN=host1,OU=Dept1,O=MongoDB,ST=NY,C=USC=US, ST=CA, O=MongoDB, OU=Dept1, CN=host2 MongoDB Configuration 配置文件：security.clusterAuthMode and net.ssl.clusterFile cmd-line options: –clusterAuthMode and –sslClusterFile Member Certificate and PEMKeyFile 配置文件： net.ssl.PEMKeyFile cmd-line option: –sslPEMKeyFile Enterprise Authentication Mechanisms MongoDB认证和角色要想了解MongoDB的权限必须先了解如下一些关键字： user 用户，用于提供客户端连接MongoDB的认证账户 role 角色，数据权限的集合，创建用户的时候必须要指定对应的角色，否则用户无法操作数据库 resource 资源，包括database或collection 也可以是database和collection的组合 actions 权限操作，定义了 user 能够对 resource document 执行的操作。如 增、删、改、查 privilege 权限，privilege 是一组 resource 和 action的组合，对资源拥有什么操作称为权限 authenticationDatabase 认证库，即创建角色或用户时所在的库 角色管理MondoDB支持基于角色的访问控制（RBAC）来管理对MongoDB系统的访问。一个用户可以被授权一个或多个角色以决定该用户对数据库资源和操作的访问权限。在权限以外，用户是无法访问系统的。 数据库角色在创建用户的role参数中设置。角色分为內建角色和自定义角色。 内建角色 数据库用户角色 read：允许用户读取指定数据库 readWrite：允许用户读写指定数据库 数据库管理员角色 dbAdmin：允许用户进行索引创建、删除，查看统计或访问system.profile，但没有角色和用户管理的权限 userAdmin：提供了在当前数据库中创建和修改角色和用户的能力 dbOwner：提供对数据库执行任何操作的能力。这个角色组合了readWrite、dbAdmin和userAdmin角色授权的特权 集群管理角色 hostManager：提供监视和管理服务器的能力 clusterManager：在集群上提供管理和监视操作。可以访问配置和本地数据库，这些数据库分别用于分片和复制 clusterMonitor：提供对监控工具的只读访问 clusterAdmin：提供最强大的集群管理访问(副本集、分片、主从等)。组合了clusterManager、clusterMonitor和hostManager角色的能力，还提供了dropDatabase操作 备份恢复角色 backup：提供备份数据所需的能力 restore： 提供使用mongorestore恢复数据的能力 所有数据库角色 readAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读权限 readWriteAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的读写权限 userAdminAnyDatabase：只在admin数据库中可用，赋予用户所有数据库的userAdmin权限 dbAdminAnyDataBase：只在admin数据库中可用，赋予用户所有数据库的adAdmin权限 超级用户角色 root：超级权限，只能针对admin库 内部角色 __system：提供对数据库中任何对象的任何操作的特权 自定义角色 MongoDB内置角色一般来说都是够用的，但当内置角色不满足需求时就可以自定义角色了。使用 db.createRole() 方法来自定义角色。 只能在admin库中创建角色： 123456789101112use admindb.createRole( &#123; role:&lt;role_name&gt;, #定义角色名称 privilege:[ #权限集 &#123; resource:&#123;cluster:true, actions:[&lt;action_name&gt;] &#125;, &#123; resource: &#123;db:&lt;db_name&gt;, collection:&lt;coll_name&gt; &#125;, &#123; actions:[&lt;action_name&gt;] &#125; #定义对这个库或集合可进行的权限操作，这是一个数组 ], roles:[ &#123; role:&lt;role_name&gt;, db:&lt;db_name&gt; &#125; ] #是否继承其他的角色 &#125;) 角色创建完毕后MongoDB会在系统库admin下创建一个collection名叫 system.roles，里面存储的即是角色相关的信息。 1db.system.roles.find() 操作角色1234567891011#查看角色db.getRole()#角色继承#角色授权db.grantRolesToRole()#角色移权db.revokeRolesfromRole() 用户管理创建用户： 123456db.createUser(&#123; user:&quot;xxx&quot;, pwd:&quot;xxxx&quot;, customDate:&quot;xxx&quot;, roles:[&#123; #指定角色名称以及认证库 role:&quot;xxx&quot;, db:&quot;xxxx&quot; &#125;]&#125;) 开启认证： 1234567891011vim /etc/mongo.confsecurity: authorization：enableddb.auth(&quot;user&quot;,&quot;passwd&quot;) #在use db后或mongo -u user -p passwd --authenticationDatabase xxx#在哪个库创建的用户就需要使用哪个库进行认证 查看用户： 12db.getUser(&quot;user&quot;)db.system.users.find() 删除用户： 12db.dropUser(&quot;user&quot;)db.dropAllUsers() 添加用权限： 1db.grantRolesToUser() 修改用户密码： 1db.changeUserPassword(&quot;user&quot;,&quot;new_passwd&quot;) 在MongoDB中删除库和集合并不会级联删除对应的角色和用户。因此如果想彻底删除对应的业务应该先删除库与其对应的角色和用户。 如果既想实现精细化权限控制又想简化用户管理，原则上建议只给开发创建一个账户，并且使用admin做认证库，这样可以避免清理过期业务库而导致无法登陆的问题。 内部认证Internal Authentication 可以对副本集和分片集成员进行验证。对于成员的内部认证，MongoDB可以使用keyfile或x.509证书。 KeyFile keyfiles的内容作为成员的共享密码，其长度必须在6-1024个字符之间，只能包含base64 set中的字符。 1234567891011121314openssl rand -base64 512 &gt; /etc/mongodb.keyfilechmod 600 /etc/mongodb.keyfilechown mongod:mongod /etc/mongodb.keyfile#配置文件：security.keyFile#cmd-line option: --keyFilevim /etc/mongod.confsecurity: authorization: enabled keyFile: &quot;/etc/mongodb.keyfile&quot; clusterAuthMode: &quot;keyFile&quot; x.509 内部认证使用x.509进行验证。 CA必须为所有分片集，副本集成员颁发x.509证书 成员证书的主题中找到Distinguished Name(DN)必须为以下至少一个属性指定非空值：Organization(O)，Organization Unit(OU)，Domain Component(DC) 组织属性，组织单元属性和域组件必须与其他集群成员的证书相匹配。12CN=host1,OU=Dept1,O=MongoDB,ST=NY,C=USC=US, ST=CA, O=MongoDB, OU=Dept1, CN=host2 MongoDB Configuration 配置文件：security.clusterAuthMode and net.ssl.clusterFile cmd-line options: –clusterAuthMode and –sslClusterFile 在副本集中强制秘钥文件访问控制Enforce Keyfile Access Control in a Replica Set 对副本集执行访问控制需要配置： 使用内部身份验证副本集成员之间的安全性 使用用户访问控制连接客户端和副本集间的安全性 步骤： 创建一个密钥文件 Create a keyfile 通过密钥文件进行身份验证，副本集中的每个mongod实例都使用密钥文件的内容作为共享密码，用于验证部署中的其它成员。 12345#yum install -y opensslopenssl rand -base64 756 &gt; &lt;path-to-keyfile&gt;chmod 400 &lt;path-to-keyfile&gt;chown &lt;owner&gt;:&lt;owner&gt; 复制密钥文件到每个副本集成员 Copy the keyfile to each replica set member 将密钥文件复制到每一台主机的副本集成员中。确保运行mongod实例的用户就是keyfile的所有者，并可以访问密钥文件。 关闭所有的副本集成员 Shut down all members of the replica set 关闭每个副本集中的mongod，从Secondary开始。知道所有的成员都脱机为止，包括任何仲裁者(Arbiter)。Primary是最后一个关闭的成员。 12use admindb.shutdownServer() 启动访问控制并重启副本集成员 123456789101112vim /etc/mongod.confsecurity: keyFile: &lt;path-to-keyfile&gt; clusterAuthMode: keyfilereplication: replSetName: &lt;replcaSetName&gt;#cmd-linemongod --keyFile &lt;path-to-keyfile&gt; --clusterAuthMode keyfile --replSet &lt;replicaSetName&gt; 连接到mongo shell 在Primary上使用rs.status()来标识副本集成员。 创建一个administrator Create the user administrator 必须在Primary上创建用户。 12345678admin = db.getSiblingDB(&quot;admin&quot;)admin.createUser( &#123; user: &apos;zhang&apos;, pwd: &apos;password&apos;, roles: [&#123; role: &apos;userAdminAnyDatabase&apos;, db: &apos;admin&apos; &#125;] &#125;) 开启用户认证 12345678vim /etc/mongod.confsecurity: authorization: enabled keyFile: &lt;path-to-keyfile&gt; clusterAuthMode: keyfilereplication: replSetName: &lt;replcaSetName&gt; 以管理员身份进行认证 Authenticate as the User Administrator 123456mogno&gt;db.getSiblingDB(&quot;admin&quot;).auth(&apos;zhang&apos;,&apos;password&apos;)#ormongo -u &apos;zhang&apos; -p &apos;password&apos; --authenticationDatabase &apos;admin&apos; 创建集群管理员(可选) Create the cluster administrator (Optional) 1234567db.getSiblingDB(&quot;admin&quot;).createUser( &#123; &quot;user&quot; : &quot;ravi&quot;, &quot;pwd&quot; : &quot;changeme2&quot;, roles: [ &#123; &quot;role&quot; : &quot;clusterAdmin&quot;, &quot;db&quot; : &quot;admin&quot; &#125; ] &#125;)]]></content>
      <categories>
        <category>Database</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP请求方法和状态码]]></title>
    <url>%2F2017%2F12%2F01%2FHTTP-method-status%2F</url>
    <content type="text"><![CDATA[常见HTTP请求方法HTTP协议的请求方法有：GET, POST, HEAD PUT DELETE, OPTIONS, TRACE, CONNECT Method Description GET 向Server请求文件 POST 向Server发送数据并让Server进行处理 PUT 向Server发送数据并存储在Server端 HEAD 检查一个对象是否存在 DELETE 从Server上删除一个文件 CONNECT 对通道提供支持 TRACE 跟踪到Server的路径 OPTION 查询Server的性能 HTTP Status Code当我们从Client向Server发送请求时，Server会向我们返回StatusCode。StatusCode会告诉我们Server的响应的状态，通过它，我们就可以知道当前请求是成功还是出现了问题。 HTTP StatusCode放置在HTTP Response报文中。 StatusCode由三位数字组成，第一个数字定义了响应类型，有五种可能值： 状态码 响应类别 描述 1xx 指示信息 服务器正在处理请求 2xx 成功 请求以正常处理完毕 3xx 重定向 需要进行额外操作以完成请求 4xx 客户端错误 客户端原因导致服务器无法处理请求 5xx 服务器错误 服务器原因导致处理请求出错 常见HTTP状态码 状态码 描述 200-OK 服务器成功返回网页，这是成功的HTTP请求返回的标准状态码 301 - Moved Permanently 永久跳转，所有请求的网页将永久跳转到被设定的新位置 400 - Bad Request 客户端请求有语法错误，不能被服务器理解 403 - Forbidden 禁止访问，这个请求时合法的，但是服务器端因为匹配了预先设置的规则而拒绝响应客户端的请求，此类问题一般为服务器权限配置不当所致 404 - Not Found 服务器找不到客户端请求的指定页面，可能是客户端请求了服务器不存在的资源所导致 500 - Internal Server Error 内部服务器错误，服务器遇到了意料不到的情况，不能完成客户的请求。这是一个较为笼统的报错，一般为服务器的设置或内部程序问题所致 502 - Bad Gateway 坏的网关，一般是代理服务器请求后端服务器时，后端服务不可用或没有完成响应网关服务器。一般为代理服务器下面的节点出了问题 503 - Service Unavailable 服务当前不可用，可能为服务器超载或停机维护所致，或者是代理服务器后面没有可以提供服务的节点 504 - Gateway Timeout 网关超时，一般是网关代理服务器请求后端服务时，后端服务没有在特定的时间内完成处理请求，一般为服务器过载所致，没有在指定的时间内返回数据给代理服务器 1xx1xx（临时响应），表示临时响应并需要请求者继续执行操作。 状态码 描述 100 - Continue 请求者应当继续提出请求 101 - Switching Protocols 请求者要求服务器更换协议，服务器已确认并准备更换 2xx2xx（成功），表示成功处理了请求。 状态码 描述 200 - OK Server已成功处理了请求 201 - Created 请求成功并且Server创建了新的资源 202 - Accepted Server以接受请求，但尚未处理 203 - Non-Authoritative Information Server已成功处理了请求，但返回的信息可能来自另一个来源 204 - No Content Server成功处理了请求，但没有返回任何内容 205 - Reset Content 没有新的内容，但浏览器应该重置它所显示的内容 206 - Partial Content 服务器成功处理了部分GET请求 3xx3xx（重定向），表示要完成请求需要进一步操作。 状态码 描述 300 - Multiple Choices 针对请求，Server可执行多种操作 301 - Moved Permanently 请求的网页已移动到新位置 302 - Found Server目前从不同位置的网页响应请求 303 - See Other 请求者对不同位置使用单独的GET请求来检索时 304 - Not Modified 自从上次请求后，请求的网页内容未修改过 305 - Use Proxy 请求者只能使用代理访问请求的网页 307 - Temporary Redirect Server从不同位置的网页响应请求，但请求者继续使用原有位置进行请求 4xx4xx（请求错误），表示请求可能出错，妨碍了Server的处理。 状态码 描述 400 - Bad Request Server不理解请求的语法 401 - Unauthorized 请求要求身份认证 403 - Forbidden Server拒绝请求 404 - Not Found Server找不到请求的网页 405 - Method Not Allowed 请求方法不被允许 406 - Not Acceptable 无法使用请求的恩日工特性响应请求的网页 407 - Proxy Authentication Required 请求需要代理授权 408 - Request Timeout Server等候请求时超时 409 - Conflict Server在完成请求时发生冲突 410 - Gone 请求的资源以永久删除 411 - Length Required Server不接受不含有效内容长度Header的请求 412 - Precondition Failed Server为满足请求者在请求中设置的一个前提条件 413 – Request Entity Too Large 请求实体太大，Server无法处理 414 - Request URI Too Long 请求的URI过长，Server无法处理 415 – 不支持的媒体类型 请求的格式不受支持 416 – Requested Range Not Satisfiable 页面无法提供请求的范围 417 – 执行失败 Server未满足期望请求Header的要求 451 基于法律上的的原因，不能像请求者展示网页内容 5xx5xx（服务器错误），表示服务器在尝试处理请求时发生内部错误。这些错误可能是服务器本身的错误，而不是请求出错。 状态码 描述 500 - Internal Server Error Server遇到错误，无法完成请求 501 - Not Implemented Server不具备完成请求的功能 502 - Bad Gateway Server作为网关或代理时，从upstream收到无效响应 503 - Service Unavailable Server暂时无法使用 504 - Gateway Timeout Server作为网关或代理时，没有及时从upstream收到请求 505 - HTTP Version Not Supported Server不支持请求中所用的HTTP版本]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filesystem Hierarchy Standard]]></title>
    <url>%2F2017%2F11%2F27%2FFHS%2F</url>
    <content type="text"><![CDATA[FHS介绍FHS(Filesystem Hierarchy Standard)，文件系统层次化标准：http://www.pathname.com/fhs FHS主要目的是希望让用户了解安装文件通常放置的目录。所以希望软件开发商、系统制定者以及维护系统的用户，都能够遵循FHS的标准。 FHS-compliant system： - 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr /opt /etc /boot 可变的(variable) /var/mail /var/spool/news /var/run /var/lock shareable： 可分享给其他系统(主机)挂载使用； unshareable： 不适合分享给其他主机； static： 有些数据基本是不会变化的； variable： 进程变更的数据。 FHS针对目录树架构仅定义出三层目录下应该放置什么数据，这三个目录下所应该放置的目录也都有特定规定。 /： The root filesystem, 与开机系统有关； /usr: The /usr hierarchy, Unix software resource； /var: The /var hierarchy, 与系统运行过程有关。 The Root Filesystem根目录(/)是系统最重要的一个目录。不但所有目录都是由根目录衍生出来，同时根目录还与系统的启动、还原、修复等操作相关。若系统出现问题，根目录必须要包含能够修复文件系统的程序才行。破坏根文件系统上的数据的错误比破坏其他任何分区都要严重！ 为了平衡这些考虑，建议尽可能保持根分区小。应用程序不应在根目录中创建特殊文件或子目录！ The following dirs or symbolic-links, are required in / 目录 描述 /bin 必要的二进制命令 /boot boot-loader的静态文件 /dev 设备文件 /etc 主机特定的系统配置文件 /lib 基本的共享库(shared libraries)和内核模块(kernel modules) /media 可移除媒体的挂载点 /mnt 临时挂载文件系统的挂载点 /opt 第三方软件包放置目录 /sbin 必要的系统二进制命令 /srv 系统提供的服务数据 /tmp 临时文件 /usr /usr层次结构 /var /var层次结构 除了上面列出必须存在的目录，下面这些目录很也很重要。 目录 描述 /lost+found 在ext文件系统里，当文件系统发生错误时，将一些遗失的片段放置到此目录下 /home 用户家目录 /root root用户家目录 /proc 虚拟文件系统，放置的数据都在内存当中，不占磁盘空间 /sys 虚拟文件系统，记录内核相关信息，不占磁盘空间 另外需要注意的是，因为根目录与开机有关，开机过程中仅有根目录被挂载。其他分区则是在开机完成后才会持续进行挂载。因此，根目录下与开机过程有关的目录就不能放到不同的分区中去。 如： /etc /bin /sbin /dev /lib /bin/bin, 基本用户二进制命令文件，供所有用户（系统管理员和用户）使用。 /bin下不能有子目录(subdirectory)。 The following commands or symbolic-links to commands, are required in /bin 命令 描述 cat 将文件连接到stdout的实用程序(Utility) chgrp 更改文件所有权 chmod 更改文件访问权限 chown 更改文件所有者和和组 cp 复制文件和目录 date 打印或设置系统数据和时间 dd 转换和复制文件 df 磁盘使用情况 dmesg 打印或控制kernel消息缓冲区 echo 显示一行文本 false do nothing, 不成功 true do nothing, 成功 hostname 系统主机名 kill 发送信号到进程 ln 在文件之间创建链接 login 在系统上开始会话 ls 列出目录内容 mkdir 创建目录 mknod 创建block或character特殊文件 more 文本翻页 mount 挂载文件系统 umount 解挂文件系统 mv move/rename文件 ps 报告进程状态 pwd 打印当前工作目录 rm remove文件或目录 sed sed流编辑器 sh Bourne command shell stty 更改或打印终端设置 su change uid sync 刷新文件系统缓冲区 uname 打印系统信息 The following programs or symbolic-links to programs, must be in /bin if the corresponding-system is installed: 命令 描述 csh The C shell(可选) ed 编辑器(可选) tar tar归档(可选) cpio cpio归档(可选) gzip GNU压缩工具(可选) gunzip GNU解压缩工具(可选) netstat 网络统计(可选) ping ICMP网络测试(可选) /boot/boot :static file of the boot-loader 该目录包含引导过程所需所有内容，处理引导是不需要的配置文件和映射安装文件外。因此，/boot储存kernel开始执行用户模式之前使用的数据。 操作系统kernel必须位于 / or /boot /dev/dev :device files /dev 目录是特殊或设备文件的位置。 /etc/etc :host-specific system configuration 配置文件是用来控制程序操作的本地静态文件，不能是可执行的二进制文件。 The following files or symbolic-links to files, must be in /etc if the corresponding-subsystem is installed. 文件 描述 备注 csh.login C shell登录的系统范围初始化文件 Optional exports NFS文件系统访问控制列表 Optional fstab 文件系统静态信息 Optional ftpusers FTP守护进程用户访问控制列表 Optional gateways 路由网关文件 Optional gettydefs getty终端设置 Optional group 用户组文件 Optional passwd 密码文件 Optional host.conf 解析器配置文件 Optional hosts 主机域名的静态信息 Optional hosts.allow Tcp-wrapper的主机访问文件 Optional hosts.deny Tcp-wrapper的主机禁止文件 Optional hosts.equiv rlogin, rsh, rcp的可信主机列表 Optional hosts.lpd lpd的可信主机列表 Optional inetd.conf inetd配置文件 Optional inittab init配置文件 inittab is no longer used when using systemd id.so.conf 搜索共享库的额外目录 Optional issue 预登录消息和 CentOS Linux 7(core) kernel \r on an \m motd 登录后信息 Welcome to $host mtab 文件系统动态信息 Optional mtools.conf mtools配置文件 Optional networks 网络名称的静态信息 Optional printcap lpd打印机功能数据库 Optional profile sh shell login的系统范围初始化文件 Optional protocols IP协议列表 Optional resolv.conf 域名服务器解析文件 Optional rpc RPC协议列表 Optional securetty root登录的TTY访问控制 Optional shells 有效登录shell的路径名 Optional syslog.conf syslogd配置文件 Optional /etc/opt/etc/opt :/opt的配置文件 第三方应用程序软件的特定主机配置文件，必须安装在/etc/opt/ 中。 /etc/xml/etc/xml :XML的配置文件 这里安装和定义XML系统的高级参数同通用配置文件。 /home (Optional)/home :用户主目录 /home是一个相当标准的概念，但它显然是一个特定于站点的文件系统。设置会因主机而异。因此，任何程序都不应该依赖这个目录。 /lib/lib :基本的共享库和内核模块 /lib目录中包含引导系统和运行在根文件系统的命令，即/bin和/sbin中的命令。 至少需要包含以下文件(链接)： 文件 描述 libc.so.* 动态链接C库 ld* 执行时间 链接器/加载器 /lib (Optional)/lib&lt;qual&gt; : 不同格式的基本共享函数库如：64位的/lib64; 32位的/lib32。 用来存放与/lib不同格式的二进制函数库，如支持64位的/lib64函数库等。 /media/media :可移除媒体的挂载点 此目录包含的子目录，可作为各移动介质(USB,cdrom,floppy…)的挂载点。 尽管在 /mnt 中使用子目录作为挂载点已经很常见了，但与直接使用/mnt作为临时挂载点的传统相去甚远。 /mnt/mnt :临时挂载文件系统的挂载点 /opt/opt :为第三方软件包保留的目录 要安装在/opt中的软件包必须将其静态文件放置在单独的/opt/&lt;packge&gt;目录树中。 目录/opt/bin, /opt/doc, /opt/include, /opt/info, /opt/lib, /opt/man 是保留给本地系统管理员使用。如果第三方软件包含Unix手册，而手册必须放置于/opt//share/man/，必须使用与/usr/share/man相同的子结构。 /root (Optional)/root :root用户的主目录 /sbin/sbin :系统二进制文件 系统管理的实用程序(命令)，存储在/sbin, /usr/sbin, /usr/local/sbin中。/sbin包含启动，恢复，修复系统，以及/bin中二进制文件所必须的二进制文件。本地安装的系统管理程序应放置在/usr/local/sbin中。 The following commands or symbolic-links to commands are required in /sbin1shutdown #关闭系统 The following files or symbolic-links to files，must be in /sbin if the corresponding subsystem is installed 命令 描述 备注 fastboot 重启系统而不检查磁盘 Optional fasthalt 停止系统而不检查磁盘 Optional fdisk 分区表操作器 Optional fsck 文件系统检查和修理工具 Optional fsck.* 针对特定文件系统检查和修复 Optionaleg：fsck.ext3 getty getty程序 Optional half 停止系统 Optional ifconfig 配置网络接口 Optional init 初始化进程 Optional mkfs 创建文件系统 Optional mkfs.* 创建特定文件系统 OPtionaleg: mkfs.ext4 mkswap 设置swap分区 OPtional reboot 重启系统 OPtional route IP路由表实用程序 OPtional swapon 启用分页和交换 OPtional swapoff Disable paging and swapping Optional update 守护进程定期刷新文件系统缓冲区 Optional /srv/srv :系统提供的服务(service)的数据 /tmp/tmp :临时文件 /tmp目录为临时需要文件的程序提供。程序不能在程序的调用之间保留/tmp中的任何文件或目录。尽管/tmp中数据可能会以某种特定方式删除，但建议在系统启动时删除/tmp中所有文件。 The /usr Hierarchy/usr 里面放置的数据是可分享与不可变动的。这就意味着可在各种符合FHS的主机之间共享，但不能写入。大型软件包不应在/usr层次结构下使用直接子目录。 The following dirs of symbolic-links to dirs are required in /usr 目录 描述 /usr/bin 大多数用户命令 /usr/include C程序包含的头文件 /usr/lib 库文件 /usr/local 本地层次结构 /usr/sbin 非重要的系统二进制文件 /usr/share 独立于架构的数据 其他选项： 目录 描述 备注 /usr/lib&lt;qual&gt; 可选格式库 Optional /usr/src 源代码 OPtional /usr/games 游戏和教育二进制文件 OPtional /usr/bin/usr/bin :大多数用户命令这是系统上可执行命令的主要目录。 The following files or symbolic-links to files must be in /usr/bin, if the corresponding subsystem is installed 命令 描述 备注 perl 实用提取和报告语言 OPtional python python解释语言 Optional tclsh tcl解释器的简单shell OPtional wish 简单 tcl/tk windowing shell Optional expect 程序交互式对话 Optional 因为shell script解释器(在shell script脚本的第一行 #!)不能依赖路径，所以标准化它们的位置是有利的。Bourne shell 和 C-shell解释器已经被固定在/bin中，但 perl,python,tcl经常在许多不同的地方。 /usr/include/usr/include :标准C包含文件的目录 这是C语言所有系统的通用包含文件应该被放置的地方。 /usr/lib/usr/lib :编程和包的所需要的库 /usr/lib包括 不打算由用户或shell script直接执行的目标文件、库和内部二进制文件。 /usr/lib (Optional)/usr/lib&lt;qual&gt; :可选格式库 /usr/local/usr/local :本地层次结构 /usr/local是给系统管理员安装本地软件使用。当系统软件更新时，需保证安全。它可以用于在一组主机之间共享，但在 usr中找不到的程序和数据。 本地安装软件必须放在 /usr/local 而不是 /usr，除非安装它来升级或替换usr的软件 The following dirs or symbolic-links to dis must be in /usr/local 目录 描述 /usr/local/bin 本地二进制文件 /usr/local/etc 本地二进制文件的特定配置文件 /usr/local/games 本地游戏二进制文件 /usr/local/include 本地C头文件 /usr/local/lib 本地库 /usr/local/man 本地在线手册 /usr/local/sbin 本地系统二进制文件 /usr/local/share 本地独立架构层次结构 /usr/local/src 本地源码 /usr/local/share目录内容的要求应与/usr/share相同，唯一附加约束是/usr/local/share/man和/usr/local/man目录必须是同步的。（基本上就是符号链接了！） /usr/sbin/usr/sbin :非必要的标准系统二进制文件 该目录包含系统管理员专门使用的任何非必要的二进制文件。系统修复、恢复、挂载/usr等其他重要必要功能必须放在/sbin中。 /usr/share/usr/share :独立于架构的数据 /usr/share层次 是为了所有只读架构独立数据。该层次可以在给定OS的所有体系架构平台之间共享。如具有i386和PPC平台站点可能会维护一个集中安装的/usr/share目录。但/usr/share一般不打算由不同的操作系统共享，或由同一操作系统的不同版本共享。 The following dis or symbolic-links to dirs must be in /usr/share 目录 描述 man 在线手册 misc 其他独立于架构的数据 The following dis or symbolic-links to dirs must be in /usr/share, if the corresponding subsystem is installed 目录 描述 备注 dict 单词列表 Optional doc 各种文档 Optional games /usr/games的静态文件 Optional info GNU Info system’s primary dir Optional locale 支持的区域信息 Optional zoneinfo Timezone info and conf Optional NLS Native language support Optional sgml SGML数据 Optional terminfo terminfo数据库目录 Optional xml xml数据 Optional /usr/share/dict/usr/share/dict :单词列表这个目录是系统上单词列表的家目录，只包含英文单词，它们由look和各种拼写程序使用。它们是所有拼写检查器唯一通用的文件。 文件 描述 备注 words 单词列表 Optional linu.words linux可用单词列表 Optional /usr/share/man/usr/share/man :手册页它包含了/, /usr文件系统下的命令和数据的手册信息 手册页存储在 /usr/share/man/&lt;locale&gt;/man&lt;section&gt;/&lt;arch&gt;中。 每个部分的描述： man1: 可公开访问的命令的手册页，用户需要使用的大多数程序文档放置于此； man2: 系统调用部分，描述所有的系统调用(请求内核执行操作)； man3: 函数库和子例程部分，描述不直接调用内核服务的程序库例程； man4: 特定文件部分，描述系统中特定文件，相关驱动程序和网络支持。通常，这包含/dev中找到的设备以及网络协议支持的内核接口； man5: 文件格式部分，许多数据文件的格式记录在此； man6: 游戏，演示和一般小程序； man7: 各种难以分类的手册页； man8: 系统管理员用于操作和维护系统的程序记录在这。 The following dirs or symboli-link to dirs must be in /usr/share/man/&lt;locale&gt;, unless they are empty 目录 描述 备注 man1 用户程序 Optional man2 系统调用 Optional man3 函数库调用 Optional man4 特定文件 Optional man5 文件格式 Optional man6 游戏 Optional man7 混杂的手册页 Optional man8 系统管理 Optional 必须在/usr/share/man结构中作出规定，以支持用不同语言编写的手册页。这些规定必须考虑到手册页的存储和参考，相关因素包括语言和字符编码集。 栗子： Language Country CharacterSet Dir English - ASCII /usr/share/man/en English United Kingdom ISO 8859-15 /usr/share/man/en_GB English United States ASCII /usr/share/man/en_US /usr/share/misc/usr/share/misc :与架构无关的数据 /usr/share/sgml/usr/share/sgml :SGML数据 /usr/share/xml/usr/share/xml :XML数据 /usr/src/usr/src :源代码Source Code可能放置在此目录的子目录中，仅供参考。 /var Hierarchy/var 包含可变数据文件，包括假脱机目录和文件，系统管理和登录数据，以及临时文件。 如果/var不能成为一个单独的分区，最好将/var移出/分区并移入/usr分区。（为了减小根分区大小或当根分区空间不足时）也可将/var链接到/usr/var。 The following dirs or symbolic-link to dirs are required in /var. 目录 描述 /var/cache 应用程序缓存数据 /var/lib 可变状态信息 /var/local /usr/local的可变数据 /var/lock 锁文件 /var/log 日志文件 /var/opt /opt的可变数据 /var/run 与运行进程相关的数据 /var/spool 应用程序队列数据 /var/tmp 为系统重启保留的临时文件 The following dirs or symbolic-link to dir must be in /var,if the corresponding subsystem is installed. 目录 描述 备注 /var/account 进程账户日志 可选 /var/crash 系统奔溃转储 可选 /var/games 可变游戏数据 可选 /var/mail 用户邮箱文件 可选 /var/yp 网络信息服务数据库文件 /var/account/var/account :该目录保存当前活动的进程记账日志和复合进程数据。 /var/cache/var/cache :保存应用程序缓存的数据。应用程序必须能够重新生成或回复数据。与/var/spool不同，删除了缓存文件不会丢失数据。数据必须在应用程序调用和系统重启间保持有效。缓存目录的数据格式没有其他要求。 对于缓存数据单独存在的目录，系统管理员可从/var下其他目录设备不同的磁盘和备份策略。 目录 描述 备注 /var/cache/fonts 本地生成的字体 可选 /var/cache/man 本地格式化的手册页 可选 /var/cache/www www代理或缓存数据 可选 /var/cache/&lt;package&gt; 特定包缓存数据 可选 /var/lib/var/lib :可变状态信息。目录保存于应用程序或系统有关的状态信息。状态信息(state infofmation)，是程序在运行时修改的数据，属于一个特定的主机。 应用程序必须为其数据使用/var/lib/&lt;subdir&gt;，有一个必须的子目录/var/lib/misc用于不需要子目录的状态文件。 /var/lock/var/lock :锁文件，锁文件应该存储在此目录中。锁文件锁定多个应用程序共享的设备和其他资源。 这种锁文件内容的格式必须是HDB UUCP锁文件格式。HDB格式是将进程标识符(PID)存储为ASCII十进制数，并带有换行符。 /var/log/var/log :日志文件和目录，大多数日志必须写入此目录或适当子目录。 The following file or symbolic-link to file must be in /var/log. 文件 描述 lastlog 每个用户上次登录信息的记录 message syslogd的系统信息 wtmp 所有登录和注销的记录 /var/mail邮件缓存区必须通过/var/mail访问，邮件缓冲区文件必须采用的形式。 /var/run/var/run :运行时变化数据，此目录包含系统信息数据，描述系统启动以来的情况。此目录下的文件必须在引导过程开始时被清除。进程标识符(PID)文件放置于此目录或下的子目录里面。 /var/spool/var/spool :应用程序队列数据。此目录包含正在等待某种稍后处理的数据，/var/spool中的数据表示工作将在将来执行(通过程序，用户或管理员)，数据通常会在工作处理后被删除。 The following dirs or symbolic-link to dirs must be in /var/spool,if the corresponding subsystem is installed. 目录 描述 备注 lpd 打印机队列目录 可选 mqueue 发送邮件队列 可选 news 新闻假脱机目录 可选 rwho rwhod文件 可选 uucp uucp的假脱机目录 可选 /var/tmp/var/tmp :在系统重启之间保存的临时文件。存储在/var/tmp的数据比/tmp中的数据更持久。 OS Specific Annex本节是针对仅适用于特定OS的其他建议和要求。 LinuxLinux操作系统的附件 / :根目录在Linux系统上，如果内核位于/，建议使用Linux内核源代码包中使用的名称vmlinux或vmlinuz。 我的CentOS7中，内核文件默认是/boot/vmlinuz-$kernel-version.$arch /bin :基本用户命令二进制文件(供多有用户使用) /dev :设备和特殊文件 /dev/null : 写入该设备的所有数据都被丢弃。从这个设备读取将返回一个EOF条件。 /dev/zero : 该设备是归零数据的来源，写入该设备的所有数据被丢弃。从这个设备读取将返回包含zero的请求的字节数。 /dev/tty : 该设备类似于进程控制终端。一旦这个设备被打开，所有读写操作就好像实际的控制终端以及被打开一样。 /etc :主机的特定系统配置Linux系统要将附件文件放置到/etc中。 /lib64 和 /lib32 :64/32位库(依赖于体系结构)64位体系结构PPC64,AMD64,x86_64必须将64位库放置于/lib64中，将32位库放置于/lib中；64位体系结构IA64必须将64位库放置于/lib中。 /proc :内核和进程信息虚拟文件系统PROC文件系统是用于处理进程和系统信息的标准Linux方法，而不是/dev/kmem和其它类似方法。强烈建议使用PROC文件系统获取 存储，进程，内存，内核等信息。 /sbin :基本系统二进制文件Linux系统将这些附加文件放置于/sbin中： 第二扩展文件系统命令（可选）： 123456badblocksdumpe2fse2fsckmke2fsmklost+foundtune2fs boot-loader 映射安装程序（可选）：lilo 静态二进制文件： 123ldconfigsln(static ln)ssync(static sync) 出现问题时，sln（静态ln）和ssync（静态同步）非常有用；idconfig程序可以作为升级知道的手段；sln的主要用途，修复不良协调升级后/lib中不正确的符号链接动态库。 对于/sbin, idconfig二进制文件是可选的。因为站点可能会在启动时选择运行idconfig而不是仅在升级共享库时。以下是一些常见问题： 我刚刚删除了/lib/； 我无法找到库的名称，因为ls是动态链接。我使用的shell没有内置ls，我也不知道使用echo *作为替换； 我有一个静态ln，但我不知道怎么称呼这个链接。 杂项： 12345#ctrl+alt+delctrlaltdel#keyboard ratekbdrate 为了应对某些键盘出现如此高的重复速率一致无法使用,kbdrate可以安装在某些系统上的/sbin中； 由于ctrl+alt+del组合键在内核中的默认操作是硬重启，因此通常建议在将根文件系统挂在到读写模式之前禁用该行为。这就可能需要ctrlaltdel程序，它可以安装在系统的/sbin中。 /usr/include :C程序包含的头文件如果安装了C或C++编译器，则只有非 基于glibc的系统才需要这些链接符号。 12/usr/include/asm -&gt; /usr/src/linux/include/asm-&lt;arch&gt;/usr/include/linux -&gt; /usr/src/linux/include/linux /usr/src :源代码对于基于glibc的系统，此目录没有具体指导。 对于glibc之前基于linux libc修订版的系统： /usr/src/linux是唯一放置Linux内核源代码的位置。 /usr/spool/cron :cron和jobs此目录包含了cron和程序的可变数据。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>FHS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zabbix]]></title>
    <url>%2F2017%2F11%2F14%2FZabbix%2F</url>
    <content type="text"><![CDATA[Zabbix简介参考： Zabbix官方网站Zabbix中文文档Zabbix-repo仓库: http://repo.zabbix.com , 阿里云镜像: https://mirrors.aliyun.com/zabbix/zabbix/ . 环境： CentOS7x86_64, Zabbix 3.4。 Zabbix （音同 zæbix），是由 Alexei Vladishev 开发的一种网络监视、管理系统，基于 Server-Client 架构。Zabbix 的授权是属于 GPLv2。Zabbix可用于监视各种网络服务、服务器和网络机器等状态。是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。Zabbix也可经由SNMP、TCP、ICMP、SSH等对目标进行监视。 Zabbix的系统构成Zabbix系统由以下各独立模块组成： Zabbix Server，服务端(以C开发)。Server端通过收集SNMP和Agent发送的数据，写入数据库，再通过PHP+Apache在Web端展示； Zabbix Agent，客户端(基本支持所有操作系统)，并将监控主机数据发送给Server； Zabbix Frontend，Web管理端(以PHP和JavaScript构成)； Zabbix Proxy(可选组件)。用于分布式监控。 Zabbix的特点Zabbix是一个高度集成的网络监控解决方案，一个简单的安装包中提供多样性功能。 数据收集； 灵活的阀值(触发器)定义； 高度可配置化的告警； 实现图表绘制； Web监控功能； 丰富的可视化选项； 历史数据存储； 配置简单； 使用模板； 网络发现； Zabbix API； 权限管理系统； 功能强大并易于扩展的监控代理。 定义Zabbix的常用术语含义。 主机(host)： 一台你想监控的网络设备，用IP或域名表示。主机名不能使用中文创建，会报错。 主机组(host group):主机的逻辑组，它包含主机和模板。组名可以使用中文。 监控项(item):你想要接收的主机的特定数据，一个度量数据。 触发器(trigger):一个被用于定义问题阀值和评估监控项接收到的数据的逻辑表达式。 事件(event):单次发生的需要注意的事情。 异常(problem):一个处在异常状态的触发器。 动作(action):一个对事件作出反应的预定义的操作。 升级(escalation):一个在动作内执行操作的自定义场景。 媒介(media):发送报警通知的手段。 通知(notification):利用已选择的媒体途径把事情相关信息发送给用户。 远程命令(remote command):预先定义好的，满足一定条件后，可在被监控主机上自动执行的命令。 模板(template):一组可以被应用到一个或多个主机上的实体的集合。 应用(application):一组监控项组成的逻辑分组。 Web场景(Web scenario):利用一个或多个HTTP请求来检查网站的可用性。 前端(frontend):Zabbix提供的Web界面。 Zabbix API:Zabbix API允许你使用JSON RPC协议来创建、更新和获取Zabbix对象信息或执行任何其他的自定义的任务。 Zabbix server:Zabbix软件监控的核心程序，主要功能是与Zabbix proxies和agent进行交互、触发器计算、发送告警通知，并将数据集中保存等。 Zabbix agent:部署在监控对象上，能够主动监控本地资源和应用。 Zabbix proxy:帮助Zabbix server收集数据，分担Zabbix server的负载。 Zabbix进程Agentzabbix agent部署在监控的目标上，主动监测本地的资源和应用（硬件驱动，内存，处理器统计等）。zabbix agent手机本地的操作信息并将数据报告给zabbix server用于进一步处理。 zabbix agent有被动(passive)和主动(active)两种检查方式。 Serverzabbix server是zabbix软件的核心程序。它通过轮询和捕获数据，计算是否满足触发器条件，向用户发送通知。它是zabbix监控代理和Proxy代理报告系统可用性和完整性数据的核心组件。zabbix server自身可以通过简单远程检查网络服务(如Web服务器和邮件服务器)。 server是一个包含了被存储了所有配置，统计方面的和可操作数据的中央仓库，它是监控系统问题升级以致于激活警告管理器的zabbix中的实体。 基本的zabbix server分三个不同的组件：zabbix server，web前端，数据库存储。zabbix的所有配置信息都存储在服务器和web前端进行交互的数据库中。 zabbix server进程是以守护进程（Daemon）运行的。 Proxyzabbix proxy是一个可以从一个或多个受监控的设备设备收集监控数据，并将信息发送到zabbix server的进程，基本上是代表server工作。所有收集的数据都在本地进行缓存，然后传送到proxy所属的zabbix server。 zabbix proxy是完成远程区域、分支机构、没有本地管理员的网络的集中监控的理想解决方案。 zabbix proxy需要使用独立的数据库，以守护进程的方式运行。 Java gatewayzabbix守护进程原生支持监控JMX程序，它被称为zabbix java gateway。zabbix gateway是用Java语言写成。 要查得一台主机特定的JMX计数器值，zabbix server向zabbix java gateway发送请求，后者使用JMX管理API去请求远程的有关应用。应用不许额外安装软件，只需要启动时在命令行指定 -Dcom.sun.management.jmxremote即可（是在java程序）。 每个zabbix server或zabbix agent只能配置一个java gateway。 Senderzabbix sender是一种命令行应用，它可以将性能数据发送到zabbix server进行处理。该应用通常用在长时间运行的用户脚本，用于定期发送可用性和性能数据。 123456zabbix_sender -z zabbix -s &quot;xxx&quot; -k db.connections -0 43-z :server主机-s :受监控主机的技术名称-k :监控项的键-o :要发送的值 Getzabbix get也是一种命令行应用，用于与zabbix agent进行通信，并从agent那里获取所需的信息。该应用通常被用于zabbix agent故障排除 12345678zabbix_get -s $host -p xxx -k system.cpu.load[all,avg15]-s --host-p --port-I --source-address-k --key-h --help-V --version 安装ZabbixZabbix安装要求硬件： 内存，最小128MB； 磁盘，最小256MB； CPU，可能需要大量CPU资源； SMS(短信)通知服务，串行通讯口(serial communication port)和串口GSM调制解调器(serial GSM modem)。可选项。 支持平台： Linux; IBM AIX; FreeBSD; NetBSD; OpenBSD; Mac OS X; Solaris; Windows(Only Agent). 软件：Zabbix基于Apache Web服务器、领先的数据库引擎和PHP脚本语言进行构建。 数据库管理系统： MySQL 5.0.3 及以上； Oracle 10g 及以上； PostgreSQL 8.1 及以上； SQLite 3.5及以上； IBM DB2 9.7 及以上。 前端： Apache 1.3.12 及以上； PHP 5.4.0及以上； PHP-Extension: 软件 版本 备注 gd 2.0及以上 PHP GD扩展包必须支持PNG图片 bcmatch php-bcmatch ctype php-ctype libXML 2.6.15及以上 php-xml xmlreader php-xmlreader xmlwrite php-xmlwriter session php-session sockets php-net-socket mbstring php-mbstring gettext php-gettext ldap php-ldap mysqli 使用MySQL作为Zabbix后端数据库所需的组件 pgsql 使用PostgreSQL作为Zabbix后端数据库所需的组件 sqlite3 使用SQLite作为Zabbix后端数据库所需的组件 客户端浏览器：必须启用Cookie和JavaScript功能。 服务器： 要求 描述 OpenlPMI 支持IPMI功能所需组件 libssh2 支持SSH功能 fping 支持ICMP ping功能 libcurl 支持Web监控，VMware监控及SMTP认证 libiksemel 支持Jabber功能 libxml2 支持VMware监控 net-snmp 支持SNMP监控 Java网关：Java gateway编译和运行在Java 1.6 及以上版本。 数据库容量：Zabbix配置数据需要使用固定的磁盘空间，而这个空间不会过多增长。 Zabbix数据库容量主要依赖于以下参数： 每秒处理值的数量(Number of processed values per second); 历史(History)数据的回收清理设置(Housekeeper); 趋势(Trends)数据的回收清理设置(Housekeeper); 事件(Events)数据的回收清理设置(Housekeeper)。 时钟同步：对于Zabbix稳定运行而言，服务获取精确的系统时间是非常重要的。对于所有运行Zabbix组件的系统，强烈建议这些系统的时间保持同步。ntpd是一个临幸的用于同步主机和其他服务器之间的时间的后台程序。 安装、启动、配置ZabbixZabbix-repo仓库：repo.zabbix.com该仓库服务器同时提供yum和apt源码库。 配置源码库1. 从官方下载源码库 1234567#rpm -ivh http://repo.zabbix.com/zabbix/$version/rhel/7/$arch/$zabbix-release.rpmrpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm#阿里云镜像#rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.noarch.rpm#镜像失效的话自己去官网找 2. 手动配置zabbix.repo 1234567vim /etc/yum.repos.d/zabbix.repo[zabbix]name=Zabbix-Repobaseurl=http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/gpgcheck=0enable=1 安装Zabbix部署包使用MySQL数据库安装Zabbix Server、Web前端：1yum install -y zabbix-server-mysql zabbix-get 注意：此处Zabbix数据库使用MySQL，请自行安装MySQL。 安装Zabbix Agent：1yum install -y zabbix-agent 安装初始化数据库查看刚刚安装的 zabbix-server-mysql：解压得到的sql脚本create.sql只会在对应的数据库中初始化zabbix所需要的数据库表，但是不会创建zabbix数据库。所以后面我们还需要手动创建zabbix数据库。1234567rpm -ql zabbix-server-mysqlcd /usr/share/doc/zabbix-server-mysql-3.x.xx/#有一个create.sql.gz的压缩文件gunzip create.sql.gz#得到create.sql 在MySQL中创建zabbix数据库：12345678910111213141516171819msyql -uxxx -pmysql&gt;CREATE DATABASE 'zabbix' DEFAULT CHARACTER SET 'utf8';mysql&gt;SHOW DATABASES;mysql&gt;GRANT ALL ON zabbix.* TO 'zabbix'@'localhost' identified by 'zabbix';mysql&gt;FLUSH PRIVILEGES;#导入sql脚本mysql -uroot -p -Dzabbix &lt; ./create.sqlUSE zabbix;SHOW TABLES;#mysql限制IPvim /etc/my.cnf[mysqld]bind-address=127.0.0.1 配置zabbix server并启动编辑zabbix server配置文件：123456789101112131415161718192021vim /etc/zabbix/zabbix_server.conf#常会修改的参数#数据库配置DBHost=localhostDBName=zabbixDBUser=zabbixDBPassword=zabbixDBPort=3306DBSocket=/var/lib/mysql/mysql.sock#服务监听端口ListenPort=10051#服务端源IPSourceIP=#日志记录方式，file使用指定文件作为日志文件，system将日志发往syslog，console将日志发送控制台LogType=fileLogFile=/var/log/zabbix/zabbix_server.log 启动zabbix服务端：1234567891011121314systemctl start zabbix-server#此处可能由于没有关闭SELinux而报错tail /var/log/zabbix/zabbix_server.logcannot set resource limit: [13] Permission denied#关闭SELinuxsetenforce=0vim /etc/selinux/configSELINUX=disabled#查看zabbix-server默认监听的10051端口netstat -nltp 安装zabbix webzabbix web可以安装在单独的主机上，只要能连接到zabbix database所在数据库就行。但为了方便，都安装在了server上。 zabbix web需要LAMP环境：12345#可能需要自己配置PHP remi源，注意PHP及扩展版本问题yum install -y httpd php php-mysql php-mbstring php-gd php-bcmatch php-ldap php-xml#指定php版本#yum --enablerepo=remi-php56 install php-mysql php-mbstring php-gd php-bcmatch php-ldap php-xml 安装zabbix web所需的两个包：123456789yum install -y zabbix-web zabbix-web-mysql#此处默认使用php5.4#因为我的环境是php5.6,会报错#此时就需要指定php版本来安装yum --enablerepo=remi-php56 install zabbix-web zabbix-web-mysqlrpm -ql zabbix-web#zabbix-web位于/usr/share/zabbix/ 编辑zabbix的前端Apach-PHP配置文件zabbix前端的Apache配置文件位于 /etc/httpd/conf.d/zabbix.conf:1234567891011121314151617181920212223242526272829vim /etc/httpd/conf.d/zabbix.conf#需修改时区php_value max_execution_time 300php_value memory_limit 128Mphp_value post_max_size 16Mphp_value upload_max_filesize 2Mphp_value max_input_time 300php_value always_populate_raw_post_data -1php_value date.timezone Asia/Shanghai#建议顺便修改/etc/php.ini的时区vim /etc/php.inidate.timezone = Asia/Shanghai#添加httpd的虚拟主机访问zabbix web&lt;VirtualHost IP:80&gt;servername zabbix.medocumentroot /usr/share/zabbix默认数据&lt;/VirtualHost&gt;#开启httpd服务systemctl start httpd 添加hosts后就可以利用域名访问zabbix-web端了。 1echo -e "192.168.1.9 \t zabbix.me" &gt;&gt; /etc/hosts 在web端配置zabbix在浏览器访问 http://zabbix.me 初始化zabbix配置。配置好后就需要用账号密码进行登录zabbix-web端dashboard。 默认用户名是：admin，密码是配置文件里面设置的。 登录进Dashboard后，可修改语言为中文。 如果你的Zabbix无法看到中文选项，那么可能需要如下操作：1234vim /usr/share/zabbix/include/locales.inc.php#修改'zh_CN' =&gt; ['name' =&gt; _('Chinese (zh_CN)'), 'display' =&gt; true], 如果又遇到中文乱码的问题，则可以从windows中挑选一些好看的中文字体，将对应字体文件放置到zabbix web的字体目录中。windows中字体后缀.TTF，Linux中为.ttf。注意修改大小写。123456789101112131415cd /usr/share/zabbix/fonts#只有一个默认字体 graphfont.ttf#将新字体放置到此目录下#修改配置文件中对应字体名称vim /usr/share/zabbix/include/define.inc.php#将默认字体名字修改为字体目录下 你需要的字体名define('ZBX_FONT_NAME', 'graphfont');define('ZBX_GRAPH_FONT_NAME', 'graphfont'); // font file name#栗子，如perpetua字图PER.ttfdefine('ZBX_FONT_NAME', 'PER');define('ZBX_GRAPH_FONT_NAME', 'PER'); // font file name 图形显示乱码，同样是用以上方法。在windowss上找一个中文字体上传到zabbix字体目录，并修改配置文件就可以了。 Zabbix Web界面菜单： 管理菜单，用于管理zabbix自身及zabbix相关设置； 配置菜单，用于配置监控相关设置； 报表菜单，为管理员生成一段时间内的监控统计信息； 检测中菜单，用于查看被监控的相关数据； 资产记录菜单，查看被监控的主机有哪些，以及相关的资产信息。 安装zabbix agentAgent端安装也非常方便，直接在Client上安装两个包即可。 123456789101112#配置zabbix源rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpm#aliyun镜像#rpm -ivh http://mirrors.aliyun.com/zabbix/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.noarch.rpm#安装yum install -y zabbix-agent zabbix-senderrpm -ql zabbix-agent#/etc/zabbix/zabbix_agentd.conf zabbix的“主动模式”与“被动模式”都在/etc/zabbix/zabbix_agentd.conf中定义。配置最常用的agent端：12345678910111213141516171819202122232425262728293031vim /etc/zabbix/zabbix_agentd.conf####GENERAL PARAMETERS 通用配置PidFile=LogFile=####Passive checks related 被动模式配置#指定允许哪台服务器拉取本机数据Server=#指定agent端工作于被动模式时监听的端口号ListenPort=10050(默认)#指定agent端工作与被动模式时所监听的IP地址ListenIP=0.0.0.0(默认)#指定预生成的agent进程数量StartAgents=####Active checks related#agent工作于主动模式时，将消息推送到哪台Server上ServerActive=IP1,IP2...#指定当前主机主机名，Server端通过对应的主机名识别主机Hostname=#指明agent端每隔多少秒将采集的数据发往Server端RefreshActiveChecks=#栗子Server=192.168.1.9ServerActive=192.168.1.9Hostname=zabbix.me 启动zabbix-agent1234systemctl zabbix-agent start#查看状态,默认端口10050netstat -nltp 快速开始zabbix-web菜单zabbix-web界面中包含有监测中、资产记录、报表、配置、管理五项菜单。 登录和配置用户在浏览器输入 zabbix.me (修改hosts)，登录zabbix-web后台。 默认用户名：Admin，密码：zabbix。它是超级管理员。 为了防止暴力破解和词典攻击，连续尝试五次登录失败，zabbix界面将暂停30秒。 可以通过管理(Management)菜单下的用户(User)，新建、查看、管理用户信息。 zabbix在安装后自定义了两个用户： Admin用户是zabbix的超级管理员，拥有所有权限； Guest用户是一个特殊的默认用户。如果你没有登录，你访问zabbix的时候其实就是“guest”权限。guest默认没有任何权限。 你可以创建一个用户(user)并将其加入特定的用户组(Group)以提升用户权限。 可以仅用用户信息里面-报警媒介里面，自定义严重性的报警。只有勾选部分的报警信息才会发送过来。这也很棒！ 如果存在严重性则使用： Not classified Information Warning Average High Disaster 新建主机zabbix中的主机(host)是一个你想要监控的网络实体(物理的、虚拟的)。对于主机的定义非常灵活。它可以是一台物理服务器，一个网络交换机，一个虚拟机或一些应用。 可以通过配置(Configuration)菜单下的主机(Host)，查看已配置主机相关信息。默认有一个“Zabbix Server”的定义好的主机。 点击创建主机(Create host)后，填写对应的主机名称、添加对应的主机群组，zabbix-agent的IP地址和端口，以及其它信息。 新建监控项监控项是zabbix中获得数据的基础。没有监控项，就没有数据。因为一个主机中只有监控项定义了”单一的指标“或者”需要获得的数据“。 可以通过配置(Configuration)菜单下的主机(Item)，找到需要配置监控项(Item)的主机，然后创建监控项。主机默认是没有定义任何监控项的。 填写对应的监控名称、类型、键值、主机接口、信息类型等等信息。 可在监控(Monitoring)菜单中最新数据(Latest data)查看之前定义的监控项和获得的值。还可选择以图形(Graph)或值来查看监控项的相关信息。 同样也还以在Zabbix-Server端获得数据信息：12#zabbix_get -s $ip -k $valuezabbix_get -s 192.168.1.9 -k system.cpu.load 新建触发器监控项只用于收集数据。如果要自动评估收到的数据，我们则需要定义触发器(trigger)。触发器包含了一个表达式，这个表达式定义了数据的可接受的阈值级别。 如果收到的数据超过了定义好的级别，触发器将被触发，或者进入异常状态(problem)。从而引起我们的注意，让我们知道有问题发生。如果数据再次恢复到合理范围，触发器将会转到正常状态(OK)。 可以通过配置(Configuration)菜单下的主机(Hosts)选项，找到某主机的触发器(Triggers)创建触发器。 填写对应的触发器名称、表达式、描述等信息。 获取问题通知当监控项收集了数据后，触发器会根据异常状态触发报警。根据一些报警机制，它也会通知我们一些重要的事情，而不是直接在zabbix-web端进行查看。这就是通知(Notification)的功能。E-mail是最常用的异常通知发送方式。当然还有SMS（短信），脚本等媒体类型。 可以通过管理(Administration)菜单中的报警媒体类型(Media types)，点击预定义媒体类型列表中的Email，来配置Email。 为了建立一个通知，我们需要在配置菜单下动作中，创建动作(Create action)。 一旦满足了触发器的条件，变回触发执行动作。如收到E-mail等… 新建模板如果我们配置上前台主机，一些自动化操作会带来更多便利性。没错，模板(templates)功能就可以实现。模板允许对有用的监控项、触发器和其他对象进行分组，只需要一步就可以对监控主机应用模板，已达到反复重用的目的。 当一个模板链接到一个主机后，主机会继承这个模板中的所有对象。简单而言，一组预先定义好的检查会被快速应用到主机上。 Zabbix为各种操作系统、设备以及应用准备好了一些预定义的模板。你可以快速部署使用他们。但是请注意，一些模板需要根据你的实际情况和使用环境进行适当俄调整。 比如，一些检查项是不需要的，一些轮询周期过于频繁等。 在配置菜单下的模板(Templates)下，点击创建模板(Create template)。填写对应的模板名称，群组等信息。 创建模板完毕后，可将模板链接到主机。之后，模板及其所有对象被添加到了主机。 配置(Configuration) 主机和主机组(Hosts and groups)一般来讲，zabbix主机是指你希望监控的那些设备。如服务器、工作站、交换机等。创建主机是使用zabbix过程的首要任务。 我们可以把主机组想象成项目组。根据不同的功能将主机划分到主机组是非常重要的，这样可以对以后创建的用户和用户组在定义权限的时候，不用给他们zabbix admin权限，而只需要根据主机组(项目组)给予用户和用户组对应项目(主机组)的权限即可。这样很大程度上方便了Zabbix监控多个项目，也利于管理。同样，报警的时候也只会收到权限内的相关报警信息。 配置一台主机配置–主机–创建主机–填写相关参数信息。 可以在已经存在的主机上使用 Clone或Full Clone创建一个新主机。 Clone将保留所有的主机参数和模板链接；Full Clone将额外保留指数实体(应用集、监控项、触发器、视图、规则、Web场景)。 新建主机下： 主机(Host)：包含了通用的主机属性； 模板(Template)：允许将模板链接诶到主机，所有实体将从模板继承； IPMI：包含IPMI管理属性； 宏(Macros)：允许定义主机级别的用户宏； 主机资产记录(Host inventory)：允许为主机收工输入库存信息； 允许你请求与主机的加密的连接。 资产管理(Inventory)你可以将联网设备的资产信息保存在zabbix里。资产信息实在配置主机时人工录入建立的资产信息数据，或者通过使用某些自动填充选项完成的录入。 构建资产库： 手动模式： 在配置一台主机的时候，手动输入资产信息； 自动模式： 在配置主机的时候，选择自动。 之后便可以在资产记录菜单中的概述，主机项中查看相关信息。 批量更新(Mass update)有时候可能需要一次更改多个主机的某些属性，使用批量更新(mass update)功能来代替打开每个主机进行编辑。 可批量处理主机、模板、IPMI、资产、加密相关信息。 监控项(Items)监控项是从主机收集的数据信息。配置主机后，需要添加一些监控项以开始获取数据。快速添加多个监控项的一种方法是将预定义的模板附加到主机。 在单个监控项中，可指定从主机收集哪些数据信息。为此，可使用监控项key。 如system.cpu.load将收集处理器负载的数据。要给 key 指定更过参数，请在后面添加方括号[]。 如system.cpu.load[avg5]， 返回最近5分钟的CPU负载平均值。 创建一个监控项可在主机中新建一个监控项。不支持的监控项：如果由于某种原因无法检索该值，则该监控项可能不被支持。这些监控项仍然以固定的间隔重新检查。 监控项的key: key名称允许使用字符： 0-9a-zA-Z_-. key参数，用 逗,号 分隔： xxx[par1,par2…] key参数也可以为空，此时使用默认值： key key参数带引号，则允许任何Unicode字符，如果包含双引号则需要 \反斜杠 转义 key参数是一个数组，它需要包含在方括号中 自定义间隔(Custom intervals) 创建关于监控项的自定义时间规则。灵活间隔被设计为重新定义默认监控项的的更新间隔，但调度间隔用于指定独立执行的检查计划。 灵活的间隔(Flexible intervals)：允许重定义特定时间段的默认间隔。 间隔(Interval)： 指定时间段的更新间隔； 期间(Period)： 灵活间隔有效的时间段； 举个栗子： 60(interval), 1-7,00-24(period)。监控项每隔60s检查一次。 调度间隔(Scheduling intervals)：用于在特定时间检查监控项。 调度间隔定义为， md&lt;filter&gt;wd&lt;filter&gt;h&lt;filter&gt;m&lt;filter&gt;s&lt;filter&gt;。 md: month days(1-31) wd: week days(1-7) h: hours(0-23) m: minutes(0-59) s: seconds(0-58) : 指定其前缀的值—-[from-to/step]。 其实类似于Linux中定时任务的写法，只不过这里把单位(md,wd,h,m,s)写在了数值的前面。 举个栗子： 123456789101112md1-15 #1-15号wd3 #星期三h0-12 #上半天m1,3,5,7,9 #每个1,3,5,7,9分钟s/10 #每个10s#组合体wd1-5h9-18m/10 #每个工作日的上班时间每个10分钟 监控项类型(Items type)监控项类型包含从系统获取数据的多种方式。每个监控项类型都有一组自己支持的监控项key和所需的参数。 zabbix提供的监控项类型： zabbix代理检查(agent checks) SNMP代理检查 SNMP traps IPMI检查 简单检查(simple checks) VMware监控(monitoring) 日志文件监控 计算监控项(Calculated items) zabbix内部检查(internal checks) SSH检查 Telnet检查 外部检查(External checks) 汇总检查(Aggregate checks) 捕捉器监控项(Trapper items) JMX监控 ODBC监控 zabbix代理(zabbix agent)：这些检查与zabbix代理进行通信实现数据的采集。 zabbix agent-passive： 被动模式，Server向Agent索要数据； zabbix agent-active： 主动模式，Agent主动上报数据给Server。 可支持的监控项，可在新建监控项是在键值里面查看。 SNMP代理(SNMP agent)： 在启用SNMP的设备(如打印机，交换机，路由器…)上使用SNMP监控，为了能够监控SNMP代理在这些设备上提供的数据，zabbix服务器初始化配置时必须具有SNMP支持。仅通过UDP协议执行SNMP检查。 配置SNMP监控： 使用SNMP接口为设备创建一个主机； 找出要监控项目的SNMP字符串； 创建一个监控项。 IPMI检查： 你可以在zabbix中监控 智能平台管理接口(IPMI) 设备的运行状况和可用性。要执行IPMI检查，zabbix服务器必须首先配置IPMI支持。 简单检查： 简单检查通常用于远程无代理监控服务。 日志文件监控： zabbix可用于集中监控和分析 具有/不具有 日志转动能力的日志文件。当日志文件包含某些字符串或字符串模式时，通知信息可用于警告用户。 计算监控项： 计算监控项是创建虚拟数据源的一种方式。这些值将根据算术表达式定期计算。所有计算都由Server完成。 内部检查：内部检查可以监控zabbix的内部检查。即Server或Agent Server的运行情况。 SSH检查： 运行SSH检查是作为无代理监控的，SSH检查不需要zabbix代理。执行SSH检查zabbix服务器必须初始化配置为SSH2支持。 SSH检查提供两种身份验证方法，一种是用户/密码，另一种是基于密钥文件。 zabbix SSH 密钥配置: 1234567891011vim /etc/zabbix/zabbix_server.conf#SSHKeyLocation=SSHKeyLocation=/home/zabbix/.sshusermod -m -d /home/zabbix zabbixchown zabbix:zabbix /home/zabbixchmod 700 /home/zabbixcd /home/zabbix &amp;&amp; su zabbixssh-keygen -t rsa 外部检查： 外部检查是由zabbix Server通过运行shell脚本或二进制的检查。外部检查不需要再被监控的主机上运行任何代理。 汇总检查： 在汇总检查中，zabbix通过直接从数据库中查询监控信息，然后进行信息聚合。聚合检查不需要再被监控的主机上运行任何代理。 捕捉器监控项： 捕捉器监控项接收传入的数据，而不是查询它。对于想要推送到zabbix的任何数据都是适用的。 要使用捕捉器监控项，需要在zabbix中建立一个捕捉器监控项，将数据送给zabbix。 JMX监控项： JMX监控可用于监视Java应用程序的JMX计数器。JMX监视器以zabbix守护进程方式运行，名为zabbix java gateway。 ODBC监控： ODBC监控对应于zabbix web管理端中的数据库监控器监控项类型。ODBC是用于访问 数据库管理系统(DBMS) 的C语言中间件API。 zabbix可以查询ODBC支持的任何数据库。为了实现监控，zabbix不直接连接到数据库，而是使用ODBC中设置的ODBC接口和驱动。该功能允许为多个目的更加有效地监控不同的数据库。 历史与趋势(history and trends)历史与趋势是zabbix中存储数据的两种方式。历史保持每个收集的值，而趋势是每小时的平均信息。 建议保持的历史数据尽可能少，但可以保留更多的趋势数据。 用户自定义参数(user parameter)有时你想运行一个代理检查，但它不是zabbix预定义的。这时就能用到用户参数。用户参数是由zabbix代理之星的命令，最多可以返回512KB的数据。key 是唯一的。 用户参数用法： 12345678910111213141516171819202122UserParameter=&lt;key&gt;,&lt;command&gt;#栗子UserParameter=ping,echo 1#使用ping键为一个监控项返回 1#复杂栗子UserParameter=mysql.ping,mysqladmin -uroot -ppwd ping | grep -c 'alive'#mysqld状态为alive返回1，否则0#灵活的用户参数UserParameter=key[*],command#[*]定义该key接受括号内的参数#栗子UserParameter=ping[*],echo $1UserParameter=mysql.ping[*],mysqladmin -u$1 -p$2 ping | grep -c 'alive'#mysql.ping[zabbix,passwd]UserParameter=wc[*],grep -c "$2" $1#wc[/etc/passwd,root] 用户自定义参数扩展zabbix代理：是将key添加到被监控的主机哦！123456789101112131415161718#编写命令--SQL查询总数mysqladmin -uxxx -pxxx status | cut -f4 -d":" | cut -f1 -d"S"#将命令添加到zabbix_agentd.confvim /etc/zabbix/zabbix_agentd.conf#找到如下字段### Option: UserParameterUserParameter=mysql.totalquery,mysqladmin -uroot -pxxx status | cut -f4 -d":" | cut -f1 -d"S"#mysql.totalquery这个key是唯一的标识符#测试此参数##测试参数可用与否很重要哈zabbix_agentd -t mysql.totalquery#重启zabbix-agent，将重新加载配置zabbix_get -s $host -k mysql.totalquery 可加载模块(loadable modules)可加载模块提供了一种关于zabbix性能扩展的选项。 可加载模块基本上只zabbix守护程序使用的共享库，并在启动时加载。可加载模块具有很多优点，卓越的性能和可实现任何逻辑的能力，更重要的是使用和共享了zabbix模块的开发能力。 windows性能计数器(windows perfomance counter)使用perf_counter[]key有效的监控windows性能计数器 批量更新(mass update)使用批量更新功能，可一次更改多个监控属性。 值映射(value mapping)对于接收值更人性化的表示，可以使用包含数值和字符串之间的映射的值映射。 如： 0 —&gt; error 1 —&gt; true F —&gt; Full D —&gt; Differential I —&gt; Incremental … 应用集(Application)应用集对逻辑组中的监控项进行分组。 如，对MongoDB的可用性，空间，负载，慢查询，执行命令…，可归于 MongoDB应用于中。 队列(queue)队列显示正在等待刷新的监控项。队列只是一个逻辑表达的数据。 队列显示的统计信息是zabbix服务器性能是否健康的指标。在 管理–队列 下对去队列。 值缓存(value cache)为了计算触发表达式，以及让计算/聚合监控项和一些宏更快，zabbix服务器支持值的缓存选项。 在内存中的缓存可用于访问历史数据，而不用之间调用数据库。如果缓存中不存在历史值，则从数据库请求缺少的值，并相应地跟新缓存。 要启用值缓存功能，修改zabbix_server.conf中可选的ValueCacheSize参数。 触发器(Trigger)触发器是评估有项目采集的数据并表示当前系统状况的逻辑表达式。触发器表达式允许定义一个什么状况的数据是“可接受”的阈值。如果超过了可接受状态，则触发器会被触发。 配置一个触发器(configuring a trigger)在主机里面配置触发器。 触发器表达式(trigger expression)一个简单有效的表达式看起来像： 1234&#123;&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)&#125;&lt;operator&gt;&lt;constant&gt;#如&#123;192.168.1.7:agent.ping.time()&#125;=0 函数参数(function parameters)： 大多数数字型的函数接受秒数来作为参数。 1234567891011121314#600s内所有值的总和sum(600)#随后5个值总和sum(#5)avg()count()last()min()max()#5m 可被 300s 代替#1k 代表 1024bytes 运算符(operators)： 优先级 运算符 定义 1 - 负号(minus) 2 not 逻辑非(NOT) 3 *, / 乘，除 4 +, - 加，减 5 &lt;, &lt;=, &gt;, &gt;= - 6 =, &lt;&gt; 相等，不等于 7 and 逻辑与 8 or 逻辑或 触发器示例： 12345678910111213&#123;www.zabbix.com:system.cpu.load[all,avg1].last()&#125;&gt;5&#123;www.zabbix.com:system.cpu.load[all,avg1].last()&#125;&gt;5 or &#123;www.zabbix.com:system.cpu.load[all,avg1].min(10m)&#125;&gt;2&#123;www.zabbix.com:net.if.in[eth0,bytes].min(5m)&#125;&gt;100k&#123;$url1:net.tcp.service[smtp].last()&#125;=0 and &#123;$url2:net.tcp.service[smtp].last()&#125;=0&#123;$host:icmpping.count(30m,0)&#125;&gt;5&#123;$host:system.cpu.load[all,avg1].min(5m)&#125;&gt;2 and &#123;$hsot:system.cpu.load[all,avg1].time()&#125;&gt;000000 and &#123;$host:system.cpu.load[all,avg1].time)()&#125;&lt;060000... 滞后(Hysteresis): 有时候需要一个触发器状态OK和PROBLEM之间的间隔，而不是简单的阈值。 要做到这一点，我们首先定义一个PROBLEM事件的触发器表达式，然后为OK选择 ‘Recovery expression’，并未OK事件书如不同的表达式 如： 1234567#Problem expression&#123;server:temp.last()&#125;&gt;20#Recovery expression&#123;server:temp.last()&#125;&lt;=15#两者之间便有了几个滞后值 触发器依赖(trigger dependency)有时候，一台主机的可用性取决于另一台主机。如一台路由器后的上网设备。这就是主机之间某些依赖关系可能有用的地方，依赖关系设置的通知可能会被抑制，而只发送根本问题的通知。 zabbix中触发器的依赖，一个触发器可能有多个依赖于它的触发器。 路由器和路由器后的Server同时宕机，如果有依赖关系，则zabbix不会执行服务器的触发动作。值得注意的是，如果触发器所依赖的触发器被禁用，则次触发器的事件和动作将不会被抑制。 批量更新使用批量更新，可一次更改一些触发器的某些属性。 触发器严重性(trigger severity)触发器严重性定义了触发器的重要程度: 未分类(not classified), 灰色 信息(information), 淡蓝 警告(warning), 黄色 一般严重(average), 橙色 严重(High), 淡红 灾难(disaster), 红色 自定义触发器严重性(customising trigger)在 管理 – 一般 – 触发器严重性，里面自定义触发器严重性。 预测触发功能(predictive trigger function)有时候有即将到来的问题的迹象。可以发现这些迹象，以便提前采取行动，以减小影响。 zabbix具有基于历史数据预测受监视系统的未来行为的工具，这些工具通过预测触发功能实现。 事件标签(event tag)在zabbix中可以自定义事件标签，在触发器级别上定义事件标签。在事件标签定以后，相应的新事件被标记为时间标签数据。在拥有自定义时间标签的情况下，可以变得更加灵活。 例如： 识别日志文件中的问题并单独关闭他们； 用它来过滤通知； 查看前端的事件标签信息； 从项目值中提取的信息作为标签值； 在通知中更好地识别问题； 通过使用模板级别的标签来建华配置任务； 使用低级别发现的标签创建触发器。 事件(Events)zabbix可以生成一下几种类型的事件： trigger events-触发器事件； discovery events-发现事件； auto registration events-自动注册事件； internal events-内部事件； 事件以时间戳，并可以发送Email等基础动作。在 监控-问题 里面查看信息信息。 触发器事件生成(trigger events generation)触发器状态的变化是事件最常见和最重要的来源。每次触发器的状态改变时，都会生成一个事件。改时间包含了触发器状态变更的详细信息、发生时间以及信息的状态。 触发器会创建两种类型的事件：问题(problem)和正常(OK) 手动关闭问题事件(manual closing of problems)当触发器状态从“问题(problem)”变成“正常(OK)”时，很难判断是通过触发器表达式的方式解决。这时就需要手动解决。 只有在触发器中启用 “允许手动关闭” 选项，问题事件才可以被手动关闭。 其他事件来源(other event source)zabbix定期扫描网络发现规则中定义的IP范围，可以为每个规则单独配置检查频率。一旦发现主机或服务，就会生成一个发现事件。 zabbix可以生成以下事件： 1234Service Up/DownHost Up/DownService Discovered/LostHost Discovered/Lost 事件关联(event correlation)通常，在zabbix中正常事件会关闭所有的问题事件，但在某些情况下需要更细致的方法。可以根据事件标签关联问题事件。如，当监控日志文件时，在日志文件中想要发现某些问题，并将它们单独关闭，而不是一起关闭。 可视化(visualisation)图形(graphs)大量的监控数据被采集到zabbix中，如果能用可视化的表现形式来查看，那就直观和容易多了。 zabbix为用户提供了如下图形： 监控项数据的内置简单图形 “simple graphs”； 创建更复杂的自定义图形 “customised graphs”； 特定图形 “ad-hosc graphs”快速访问几个监控项的数据比较。 简单图形(simple graphs)：zabbix提供的简单图形，用来可视化显示监控项采集到的数据。并不需要配置就可以查看。 通过 监控-最新数据-图形 来展示图形。 自定义图形(customised graphs)：自定义图形，提供定制功能。这就有点厉害了。这个是手动配置的。可以为单个主机、多个主机、单个模板、多个模板创建自定义图形。 在 配置-主机-图形-创建图形 里编辑图形属性；图形编辑后可点击预览。 特设图形(ad-hoc graphs)：简单图形和自定义图形都不允许快速创建多个监控项目数据的比较图形，工作量小且没有维护。 在 检测-最新数据-旋转监控项前复选框-显示数据图(显示堆叠数据图) 下， 里面也包含了 正常和层积 的图形风格。 拓扑图(networking maps)运维人员如果想要了解网络环境的基础设施状况，可以在zabbix中创建网络拓扑图。 配置拓扑图(configurating network maps): 在 监控-拓扑图 下，可以创建拓扑图。点击拓扑图中的 构造函数 选项，来打开编辑区域。然后在编辑区域中添加元素和链接元素。 链接指示器(link indicators):可以为网络拓扑图中的元素之间的链接分配一些触发器，当这些触发器状况为“Problem”时，可以在链接上体现出来。如果多个触发器进入”Problem”状态，则严重程度最高的将决定链接的颜色和样式。 聚合图形(screen)在zabbix的聚合图形页面上，你可把各种来源的信息聚集到一起，一边在单个屏幕上快速查看。在 监测-图形聚合 下，对其进行创建、配置、管理和查看。 基本上，聚合图形是一个表格，你选择把每个表格有多少单元格以及其中要显示的元素。元素如下： 简单图形； 简单图形原型； 用户自定义图形； 自定义图形原型； 拓扑图； 其他聚合图形； 纯文本信息； 服务器信息； 触发器信息； 主机/主机组信息； 系统状态； 数据概述； 时钟； 事件历史； 动作历史； URL。 幻灯片演示(slide shows)在幻灯片演示中，可以配置多个聚合图形以设定的间隔逐个显示。在 监测-聚合图形-幻灯片演示 下。 模板(template)模板是可以方便地应用于多个主机的一组实体。 配置模板(configuring a template)：配置模板需要首先通过定义一些参数来创建模板，然后添加实例。在 配置-模板-创建模板 链接模板(linking)：链接是将模板应用于主机的过程，之后主机将拥有模板的所有实体。 嵌套(nesting)：嵌套是一种包含一个或多个其它模板的模板方式。可以在一个嵌套模板中奖一些模板链接在一起。 嵌套的好处在于，您只需要讲一个模板链接到主机，并且主机会自动继承链接的模板的所有实体。 事件通知(notifications upon events)当配置了一些项目和触发器，并且由于触发器改变状态，现在正在发生一些事件，之后就要考虑 action。发送通知是zabbix提供的主要操作之一。 为了能够发送和接收通知，必须： 定义一些media； 配置action，向指定的media发送消息。 action由condition和operation组成。当条件满足是，执行操作。操作主要是 发送消息和执行远程命令。 media类型媒体是zabbix中发送通知和警报的传送通道。 E-mail: 在 管理-媒体类型 下，配置Email。 SMS： zabbix支持使用连接到zabbix-server的串行端口的串行GSM调制解调器发送SMS消息。 确保： 串行设备的速度(在Linux下通常为/dev/ttyS0) 与 GSM调制解调器的速度相匹配。zabbix没有设置串行链路的速度，它使用默认设置。 zabbix用户对串行设备有读写访问权限。 GSM调制解调器输入PIN码，并在电源复位后保留PIN码。或者在SIM卡上禁用PIN。 管理-媒体类型下要为用户分配电话号码：管理-用户-报警媒介，添加报警媒介(如电话号码等) Jabber： zabbix支持发送jabber消息。 Ez Texting： 可以使用 zabbix技术合作伙伴 Ez Texting发送信息。 脚本： 警报脚本在zabbix服务器上执行，这些脚本位于服务器配置文件中定义的目录中(AlertScriptsPath)。123456789101112131415161718cat /etc/zabbix/zabbix_server.confAlertScriptsPath=/usr/lib/zabbix/alertscripts#创建报警脚本vim /usr/lib/zabbix/alertscripts/zabbix_test.sh#!/bin/bashto=$1subject=$2body=$3#可以同时给多个用户发送，用空格隔开cat &lt;&lt;EOF | mail -s &quot;$subject&quot; &quot;to&quot;$bodyEOF 然后我们在创建脚本媒体的时候，写入相关参数。 actions可以根据所有支持的类型的时间定义操作： 触发事件：当trigger的状态从OK转到Problem或回转时； 发现事件； 自动注册事件； 内部事件； 配置-动作-创建动作 条件(condition)只有在事件与定义的条件匹配的情况下才执行操作。 注意运算类型：似与非似 操作(operation)操作：发送信息，执行远程命令。 发送消息远程命令(不支持在zabbix-agent上执行远程命令，需要在zabbix-server到代理的命令才能直接连接。远程命令限制255字符，可以将过个命令放置于新行上来执行过个命令。及时目标主机处于维护状态，也会执行远程命令). 配置-动作-操作，在操作细节中修改操作类型为远程命令。 在Zabbix代理（自定义脚本）上执行的那些远程命令必须首先在相应的命令中启用 zabbix_agentd.conf.确保 EnableRemoteCommands 参数设置为 1 并取消注释。 如果更改此参数，请重新启动代理守护程序。 123456789101112vim /etc/zabbix/zabbix_agentd.confEnableRemoteCommands=1cd /usr/lib/zabbix/alertscripts#或修改zabbix-server.conf中的文件位置vi sendmail.shchown zabbix.zabbix ./sendmail.sh &amp;&amp; chmod a+x ./sendmail.sh 接下来在动作中选择为执行远程命令，并在相应位置输入命令。 支持自定义脚本、SSH、Telnet等方式。 在信息中使用宏(using macros in messages)：在消息主题和消息文本中，可使用宏来更有效的问题报告。 恢复操作(recovery operation):恢复操作允许在问题解决时通知我们。恢复操作支持消息和远程命令。 宏(macros)官方支持的宏的完整列表：https://www.zabbix.com/documentation/3.4/manual/appendix/macros/supported_by_location zabbix支持许多在多种情况下使用的宏。宏是一个变量，由如下特殊语法标识。 宏类似于全局变量，宏是特别有用的，特别是在报警动作中。对于不同的细节加上特定的宏，能够使报警信息更加详细。 {MACRO} 根据在上下文汇总，宏解析为一个特殊的值。有效地使用宏可以节省时间，并使zabbix更加高效。 宏可以在监控项键值参数中使用。宏只能用在监控项键值参数的一部分中。如item.key[server_{HOST.HOST}_local] 。 宏函数(macro function)宏函数能提供自定义宏值的功能。 宏函数语法：12345678&#123;&lt;macro&gt;.&lt;func&gt;(&lt;params&gt;)&#125;#&lt;macro&gt;, 要定义的宏#&lt;func&gt;, 要应用的函数#&lt;params&gt;, 以逗号分隔的函数参数列表#栗子&#123;&#123;ITEM.VALUE&#125;.regsub&#123;pattern, output&#125;&#125; 用户宏(user macro)除了支持开箱即用的宏之外，zabbix还支持更灵活的用户宏。 用户宏可在全局、模板和主机级别进行定义。有一个特殊语法：1&#123;$MACRO&#125; 用户宏可用于： 监控项名称； 监控项键值参数； 触发器名称和描述； 触发器表达式参数和常量； 许多其他位置。 自动发现宏(LLD)有一种自动发现(LLD)函数中使用的宏类型，可用于创建监控项、触发器和图形原型。然后，当发现真实的文件系统、网络接口等，这些宏将替换为真实的值，并且以这些值来创建真实的监控项、触发器和图形。1&#123;#MACRO&#125; 用户和用户组(user and group)zabbix中所有用户都通过web前端去访问zabbix应用程序。并为每一个用户分配唯一的登录名和密码，被加密储存于zabbix数据库中。 配置用户(configuring user)管理-用户，创建和管理用户。 权限(permission)可定义相应的用户类型，如用户，管理员和超级管理员。 用户组(groups)管理-用户组，创建和配置用户组。 服务监控(service monitoring)服务监控，旨在帮助那些想要高级业务监控的人。在很多情况下，我们关注的不是底层细节，而是提供的可用性服务。 服务是分层表示监控数据。 IT Workstations workstation1workstation2 Services 配置-服务，最高节点的服务是’root’。你可以通过添加低级服务节点和各个节点服务创建下层层次结构。 Web监控(web monitoring)配置-主机-web监测，创建或修改web监测信息。可使用zabbix检查几个网站可用性方面。(zabbix中包含libcurl库才行) 要使用web监控，需要定义web场景。包括一个或多个HTTP请求或步骤。Zabbix-Server根据预定义的命令周期性的执行这些步骤。 Web监测中的要求的字段(required string)支持正则表达式，所以这对于检索页面信息很有用。这个真的很有用！ 所有web场景会收集下列数据： 整个场景中所有步骤的平均下载速度； 失败的步骤数量； 最后一次错误信息 web场景的所有步骤，都会收集下列数据： 平均下载速度； 响应时间 HTTP状态吗 Web监控项(web monitoring items)在创建web场景时，会自动添加一些新监控项进行监控。 创建场景后，zabbix会自动添加以下监控项进行监控，将它们链接到所选的应用程序。 场景的下载速度； 场景的失败步骤； 场景的最后一个错误消息； 举个栗子：1234567891011121314151617181920212223242526272829303132333435363738394041##创建Web监测#配置-主机-Web监测-创建web监测URL：web.zabbix.me/monitor.php要求的状态码：200超时：20s##创建web监测触发器#配置-主机-触发器-创建触发器严重性：一般严重#触发条件：状态码!=200表达式：N&lt;&gt;200##创建触发报警对应的动作#配置-动作-创建动作#触发条件触发器示警度=一般严重 or 触发器=web.zabbix.me#操作：发送Email发送给zabbix administrator用户群组仅送到Email默认信息/自定义信息##在媒体类型中定义Email相关信息#管理-报警媒体类型-EmailSMTP服务器：smtp.xxx.comsmtp端口：465SMTP电邮：发件人Email安全链接：SSL/TLS认证：Usernameand passwd用户名：xxx密码： xxx##接下来就可以测试接收报警Email了 虚拟机监控(VM monitoring)zabbix支持对VMware的监控，使用low-levle-discovery(LLD)自动发现VMware hypervisors和虚拟机，并根据事先定义的主机原型，为这些虚拟机建立主机，添加监控。 zabbix中提供了几个模板，可以直接用来解控VMware vCenter 或 ESX hypervisor。 虚拟机监控分为两个步骤： 首先，zabbix是通过VMware collector进程来监控虚拟机。这些进程通过SOAP协议从VMware服务获取必要的信息，对其进行预处理并储存到zabbix-server共享内存中； 然后，zabbix-pollers通过zabbix简单检查VMware keys来检索这些数据。 要使虚拟机监控正常工作，需要libxml2库和libcurl库的支持。 配置-自动发现-创建自动发现配置-主机-自动发现 维护(maintenance)可在zabbix中为主机和主机组定义维护周期。有两种维护类型：“继续对目标进行监控数据的收集” 和 “停止对目标进行监控数据的收集” 要在维护期间正常接收问题通知，必须在动作配置中的选项中取消选择暂停操作。为了确保定期维护按照预期的时间进行，需要对zabbix的所有部分使用通用时区。 配置-维护-创建维护期 维护期的主机显示的是橙色背景！ 事件确认(event acknowledgment)zabbix中的问题事件可以由用户确认。 如果用户获得了有关问题时间的通知，可以访问zabbix前端，从时间导航到确认屏幕并确认问题。当他们确认时，可输入评论或其他一些相关描述。这样其他系统用户同样的问题，他们便会立即看到是否已被解决和目前的评论。 以这种方式，可以更协调的进行解决多个系统用户的问题的工作流程。 要确认事件，用户必须至少要有对相应触发器的读取权限。 在Dashboard下，在出现的问题里，点击确认，进入确认事件。也可在监控-问题下查看问题详细信息。 配置导出/导入(Configuration export/import)zabbix导入/导出功能，使得可以在一个zabbix系统与另一个zabbix系统之间交换各种配置实体。类似于数据库的导入导出。即也可以对zabbix做备份。 可导出/导入的对象有：主机组； 模板； 主机； 拓扑； 图片； 聚合图形； 值映射。 数据也可导出： XML - 在前端 XML or JSON - 在zabbix API 导出的详细信息： 所有支持的元素都导出到一个文件中； 不导出从连链接模板继承的主机和模板实体； 由低级别发现创建的实体依赖于他们的任何实体不会导出。 导入详细信息： 第一次遇到错误停止导入； 导入支持XML和JSON文件； 使用“删除缺失”选项导入主机/模板时，导入的XML文件中不存在主机/模板宏也将被删除。 将Zabbix展现在Nginx上毕竟现在Nginx用的多，那就把Apache换成Nginx吧！ Nginx仓库:http://nginx.org/packages/ 自己安装Nginx: 下载nginx-release-xx.rmp仓库源来安装； 手动创建/etc/yum.repo.d/nginx.repo； 直接下载ngix.rpm来安装； 直接下载源码来安装。 相较于Apache，Nginx也只是配置个server就行了。优化什么的自己弄。12345678910111213141516171819202122232425262728293031vim /etc/nginx/conf.d/zabbix.confserver &#123; listen 80; server_name zabbix.me; root /usr/share/zabbix; access_log /var/log/nginx/zabbix.access.log main; allow 127.0.0.1; allow Your-IP; deny all; location / &#123; if (!-f $request_filename) &#123; rewrite ^([^\?]+)$ /index.php?1=$1 last; &#125; &#125; location ~ \.php$ &#123; root /usr/share/zabbix; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125;&#125;nginx -tsystemctl start nginx 下载就可以正常访问zabbix-web端了! Zabbix监控Zabbix自带的templates基本涵盖了大部分监控信息。 大部分操作系统： OS Linux; OS AIx; OS FreeBSD; OS Solaris; OS Windows; … 大部分服务： CPU; Filesystems; HTTP/HTTPS service; Memory; Network interfaces; Processes; Secutity; Zabbix server/agent/Proxy; SMTP,POP,SSH,NTP, service; ICMP Ping; SNMP; … 虚拟机： VM VMware; VM WMware Hypervisor; … 网络设备： Cisco; Huawei; TPLink; HP; … 除了Zabbix自带的templates，你还可以下载templates并导入zabbix-server。 例如PHP-FPM, MongoDB, Apache, Nginx, Redis等额外软件的监控就需要下载额外templates。 监控MySQL使用Zabbix自带模板监控MySQLZabbix默认带有MySQL的监控和模板，所以无需再去下载。不过需要配置用户，密码，主机，端口等信息。 123456789101112131415161718192021222324vim /etc/zabbix/zabbix-agentd.d/userparameter_mysql.conf#For all the following commands HOME should be set to the directory that has .my.cnf file with password information.#这句话叫我们新建一个带有mysql密码信息的.my.cnf文件#并把此配置文件里面的HOME改为.my.cnf所的在目录#.my.cnf文件里面的用户要对MySQL数据库有权限才行，没有权限请记得加[mysql]host=localhostuser=zabbixpassword=zabiixsocket=/var/lib/mysql/mysql.sock[mysqladmin]host=localhostuser=rootpassword=passwordsocket=/var/lib/mysql/mysql.sock#测试zabbix_get -s 127.0.0.1 -k mysql.ping#1 使用Percona插件监控MySQLZabbix默认带有MySQL的监控和模板，所以无需再去下载。不过需要配置用户，密码，主机，端口等信息。但是Zabbix自带的MySQL监控太简陋了。所以使用Percona提供的模板及监控。 Percona Monitoring Plugins-URL: https://www.percona.com/downloads/percona-monitoring-plugins/LATEST/Percona Monitoring Plugins for Zabbix- Instructions: https://www.percona.com/doc/percona-monitoring-plugins/LATEST/zabbix/index.html 此插件地址需要我们选择Percona-Version和Software平台。 选择平台后，我们只需安装zabbix的rpm包就好： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#安装rpm包yum install -y https://www.percona.com/downloads/percona-monitoring-plugins/percona-monitoring-plugins-1.1.7/binary/redhat/7/x86_64/percona-zabbix-templates-1.1.7-2.noarch.rpm#安装软件#注意php版本问题yum install -y percona-zabbix-templatesls /var/lib/zabbix/percona#scripts目录有.sh脚本文件#templates目录有配置文件和模板文件#复制配置文件cp /var/lib/zabbix/percona/templates/userparameter_percona_mysql.conf /etc/zabbix/zabbix_agentd.d/#我看了一下，这个配置文件和zabbix自带的MySQL配置文件一样#添加MySQL的相关信息vim /var/lib/zabbix/percona/scripts/ss_get_mysql_stats.php$mysql_user = 'root';$mysql_pass = 'password';$mysql_port = 3306;$mysql_socket = '/var/lib/mysql/mysql.sock';$mysql_flags = 0;#测试脚本/var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh gg#10#创建.my.cnf文件vim /etc/zabbix/zabbix_agentd.d/.my.cnf[mysql]host=localhostuser=rootpassword=passwordsocket=/var/lib/mysql/mysql.sock[mysqladmin]host=localhostuser=rootpassword=passwordsocket=/var/lib/mysql/mysql.sock[client]host=localhostuser=rootpassword=passwordsocker=/var/lib/mysql/mysql.sock#重启服务systemctl restart zabbix-agent#测试sudo -u zabbix -H /var/lib/zabbix/percona/scripts/get_mysql_stats_wrapper.sh running-slave#0/1 导入模板，模板文件位于：/var/lib/zabbix/percona/templates/zabbix_agent_template_percona_mysql_server_ht_2.0.9-sver1.1.7.xml 但我直接导入模板时报错——标签无效 “/zabbix_export/date”: “YYYY-MM-DDThh:mm:ssZ” 预计。此模板需要先导入Zabbix2.4后再导出，然后再导入到Zabbix3.4。太麻烦。 所以需要下载修改过的模板： http://jaminzhang.github.io/soft-conf/Zabbix/zbx_percona_mysql_template.xml 1wget http://jaminzhang.github.io/soft-conf/Zabbix/zbx_percona_mysql_template.xml 下载之后导入模板，然后链接主机。链接之后可以部分监控可能显示不支持的。 如：Received value [rm: 无法删除”/tmp/localhost-mysql_cacti_stats.txt”: 不允许的操作0] is not suitable for value type [Numeric (float)]没有权限。 解决办法： 1234cd /tmpchown -R zabbix:zabbix localhost-mysql_cacti_stats.txtsystemcet restart zabbix-agent 监控MongoDB感谢大神： MongoDB-templates: https://share.zabbix.com/databases/mongodb/mongodb-for-zabbix-3-2 ; GitHub: https://github.com/oscm/zabbix/tree/master/mongodb 此github-repo中还包含了Oracle, php-fpm, postfix, redis, Nginx。可参看README.md来配置zabbix对它们的监控。 安装步骤1. 在zabbix-agent安装jqjq - Command-line JSON processor; 1yum install -y jq 2. 在zabbix-agent的MongoDB中创建用于监控的账号创建用于读取MongoDB相关信息的账户及其权限。 12345678910111213mongo&gt;use admin&gt;db.createUser( &#123; user:'zabbix', pwd:'zabbix', roles:[&#123; role:'clusterMonitor', db:'admin'&#125;] &#125;) 3. 在agent下载github仓库的MongoDB模板等文件 12345678910111213wget https://codeload.github.com/oscm/zabbix/zip/master -O master.zip#这里面不仅仅有mongodb，还有redis,php等。#我们只需要进入mongodb目录就好unzip master.zipcd ./zabbix-master/mongodbls#mongodb.sh , 执行脚本#userparameter_mongodb.conf ，配置脚本#zbx_export_templates.xml，zabbix模板文件 4. 移动并配置mongodb.sh 123456789101112cp ./mongodb.sh /etc/zabbixchmod a+x /etc/zabbix/mongodb.shvi mongodb.sh#如果HOST,PORT不是默认，请修改DB_HOST=127.0.0.1DB_PORT=27017DB_USERNAME=zabbixDB_PASSWORD=zabbix 5. 移动并修改userparameter_mongodb.conf 1234567cp ./zabbix-master/userparameter_mongodb.conf /etc/zabbix/zabbix_agentd.dvi ./userparameter_mongodb.confUserParameter=mongodb.status[*],/etc/zabbix/mongodb.sh $1 $2 $3 $4 $5#修改为mongdb.sh真实位置#这个是用户自定义的参数，可以之间写入到zabbix_agent.conf里面 6. 重启zabbix-agent 1systemctl restart zabbix-agent 7. 在zabbix-web导入mongodb模板 配置-模板-导入模板； 选择./master/mongodb/zbx_export_templates.xml模板文件，并导入； 接下来便可以在 templates中看到”Template App MongoDB”这个模板； 可将此模板链接到某个主机上监控，并到最新数据里查看相关MongoDB信息； 如果相对此模板就行修改，可编辑zbx_export_templates.xml文件。 监控一台主机上的额外mongod实例由于可能一台主机上运行的mongod实例不止一个，所以我们需要修改一下前面下载的配置文件，用以监控其它端口的mongod实例。 此处假设默认的mongod实例运行在27017端口上 另外还有一个mongod实例运行在27018端口上 此处假设我们已经完成了前面对27017mongodb的监控了 操作： 12345678910111213141516171819202122232425262728cd /etc/zabbixcp mongodb.sh mongodb_27018.shvim ./mongodb_27018.sh#配置监控的mongodb账号和端口DB_HOST=127.0.0.1DB_PORT=27018DB_USERNAME=zabbixDB_PASSWORD=zabbix#现在就有了提取27017/27018两个mongodb实例的脚本#mongodb.sh#mongodb_27018.shcd ./zabbxi-agentd.dvim userparameter_mongodb.conf#在默认的27017下面添加一行提取mongodb_27018信息的脚本UserParameter=mongodb.status[*],/etc/zabbix/mongodb.sh $1 $2 $3 $4 $5UserParameter=mongodb_27018.status[*],/etc/zabbix/mongodb_27018.sh $1 $2 $3 $4 $5#现在zabbix-server端就可以同时获取27017/27018两个mongodb实例的信息#但是Web界面还不能直接显示出来，因为27018的键值和默认不相同#没错，就是上面我们修改的 mongodb_27018.status[*] 接下来要在Zabbix-Web端配置监控项用以提取信息 我们先找到一个默认的MongoDB自带的配置模板，如MongoDB Connections current，点进去查看它的键值对为mongodb.status[connections,current] 因此我们只需要修改为我们配置文件里面的mongodb_27018.status[*]就可以了。 其余个监控项以此类推，我觉得其他服务也应该可以如此。 你也可以对此建立一个单独的模板，如MongoDB_27108 templates。在此监控模板下创建上面的监控项。这样就可以对所有主机生效了。也可以批量化操作，更方便一些。 下面是我的参考Template App MongoDB模板建立的Template App MongoDB_27018 监控PHP-FPM同样使用上面大神的模板。 步骤和监控MongoDB类似： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#进入下载的文件目录cd ./zabbix-master/php-fpmcp ./php-fpm.xml.sh /etc/zabbixchmod a+x /etc/zabbix/php-fpm.xml.shvim /etc/zabbix/php-fpm.xml.sh#如果这三个参数修改了，请修改#因为是使用culr，所以请允许此IP能够访问此页面#另外还要Nginx允许Server-IP访问哦，不然无法读取数据#我测试的时候用IP无法获取数据，所以用的域名#如果没做域名解析，请加本地hosts#php-fpm_status使用我修改的HOST="localhost"PORT="80"#status="status"status="php-fpm_status"cp ./userparameter_php-fpm.conf /etc/zabbix/zabbix_agent.d/#当然也可以把这个用户自定义参数写入zabbix_agent.conf#修改自定义参数里面的文件位置vim /etc/zabbix/zabbix_agent.d/userparameter_php-fpm.confUserParameter=php-fpm.status[*],/etc/zabbix/php-fpm.xml.sh $1#php-fpm，nginx的状态必须用Nginx展现，Zabbix-Server是使用curl提取状态页面的信息vim /etc/nginx/conf.d/zabbix.confserver &#123; listen 80; server_name zabbix.me localhost;#如果localhost与其他配置文件冲突，那就用IP#server_name zabbix.me 127.0.0.1 Private-IP Public-IP; root /usr/share/zabbix; access_log /var/log/nginx/zabbix.access.log main;#allow无法使用localhost，所有内外网要分开写 allow 127.0.0.1; allow Private-IP; allow Public-IP; allow Zabbix-Server-IP; allow Remote-View-IP; deny all; location / &#123; if (!-f $request_filename) &#123; rewrite ^([^\?]+)$ /index.php?1=$1 last; &#125; &#125;#Nignx_Status location /nginx_status &#123; stub_status on; #开启nginx自带的状态检查功能 access_log off; &#125;#php-fpm_Status#php-fpm的默认状态页面是/status,/ping。我修改了一下。 location ~ ^/php-fpm_(status|ping)$ &#123; access_log off; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125; location ~ \.php$ &#123; root /usr/share/zabbix; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125;&#125; php-fpm状态页面的配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163vim /etc/php-fpm.d/www.conf#说明和用法如下，我做简单修改#修改默认值;pm.status_path = /statuspm.status_path = /php-fpm_status;ping.path = /pingping.path = /php-fpm_ping;ping.response = pongping.response = 200#用法zabbix.me/php-fpm_statuszabbix.me/php-fpm_ping#配置文件提供了格式化输出zabbix.me/php-fpm_status?htmlzabbix.me/php-fpm_status?html&amp;full; output syntax. Example:; http://www.foo.bar/status; http://www.foo.bar/status?json; http://www.foo.bar/status?html; http://www.foo.bar/status?xml; http://www.foo.bar/status?full; http://www.foo.bar/status?json&amp;full; http://www.foo.bar/status?html&amp;full; http://www.foo.bar/status?xml&amp;full#修改完毕后重启服务systemctl restart php-fpm nginx#具体看下面描述##这下面是说明; The URI to view the FPM status page. If this value is not set, no URI will be; recognized as a status page. It shows the following informations:; pool - the name of the pool;; process manager - static, dynamic or ondemand;; start time - the date and time FPM has started;; start since - number of seconds since FPM has started;; accepted conn - the number of request accepted by the pool;; listen queue - the number of request in the queue of pending; connections (see backlog in listen(2));; max listen queue - the maximum number of requests in the queue; of pending connections since FPM has started;; listen queue len - the size of the socket queue of pending connections;; idle processes - the number of idle processes;; active processes - the number of active processes;; total processes - the number of idle + active processes;; max active processes - the maximum number of active processes since FPM; has started;; max children reached - number of times, the process limit has been reached,; when pm tries to start more children (works only for; pm 'dynamic' and 'ondemand');; Value are updated in real time.; Example output:; pool: www; process manager: static; start time: 01/Jul/2011:17:53:49 +0200; start since: 62636; accepted conn: 190460; listen queue: 0; max listen queue: 1; listen queue len: 42; idle processes: 4; active processes: 11; total processes: 15; max active processes: 12; max children reached: 0;; By default the status page output is formatted as text/plain. Passing either; 'html', 'xml' or 'json' in the query string will return the corresponding; output syntax. Example:; http://www.foo.bar/status; http://www.foo.bar/status?json; http://www.foo.bar/status?html; http://www.foo.bar/status?xml;; By default the status page only outputs short status. Passing 'full' in the; query string will also return status for each pool process.; Example:; http://www.foo.bar/status?full; http://www.foo.bar/status?json&amp;full; http://www.foo.bar/status?html&amp;full; http://www.foo.bar/status?xml&amp;full; The Full status returns for each process:; pid - the PID of the process;; state - the state of the process (Idle, Running, ...);; start time - the date and time the process has started;; start since - the number of seconds since the process has started;; requests - the number of requests the process has served;; request duration - the duration in µs of the requests;; request method - the request method (GET, POST, ...);; request URI - the request URI with the query string;; content length - the content length of the request (only with POST);; user - the user (PHP_AUTH_USER) (or '-' if not set);; script - the main script called (or '-' if not set);; last request cpu - the %cpu the last request consumed; it's always 0 if the process is not in Idle state; because CPU calculation is done when the request; processing has terminated;; last request memory - the max amount of memory the last request consumed; it's always 0 if the process is not in Idle state; because memory calculation is done when the request; processing has terminated;; If the process is in Idle state, then informations are related to the; last request the process has served. Otherwise informations are related to; the current request being served.; Example output:; ************************; pid: 31330; state: Running; start time: 01/Jul/2011:17:53:49 +0200; start since: 63087; requests: 12808; request duration: 1250261; request method: GET; request URI: /test_mem.php?N=10000; content length: 0; user: -; script: /home/fat/web/docs/php/test_mem.php; last request cpu: 0.00; last request memory: 0;; Note: There is a real-time FPM status monitoring sample web page available; It's available in: @EXPANDED_DATADIR@/fpm/status.html;; Note: The value must start with a leading slash (/). The value can be; anything, but it may not be a good idea to use the .php extension or it; may conflict with a real PHP file.; Default Value: not set;pm.status_path = /statuspm.status_path = /php-fpm_status; The ping URI to call the monitoring page of FPM. If this value is not set, no; URI will be recognized as a ping page. This could be used to test from outside; that FPM is alive and responding, or to; - create a graph of FPM availability (rrd or such);; - remove a server from a group if it is not responding (load balancing);; - trigger alerts for the operating team (24/7).; Note: The value must start with a leading slash (/). The value can be; anything, but it may not be a good idea to use the .php extension or it; may conflict with a real PHP file.; Default Value: not set;ping.path = /pingping.path = /php-fpm_ping; This directive may be used to customize the response of a ping request. The; response is formatted as text/plain with a 200 response code.; Default Value: pong;ping.response = pongping.response = 200 效果图： 展现的话是在Agent端的Nginx上，这个更直观一些。而Zabbix-Server就是通过curl -s zabbix.me来获取数据的，并通过对数据的提取来返回给Zabbix-Server。所以收集php-fpm，nginx的信息状态，都是基于这个页面的。 现在导入PHP-FPM模板，导入操作同MongoDB。 12#就是这个文件zbx_export_templates.xml 导入模板后，直接链接模板就可以啦。然后就可以使用了。 监控NginxZabbix是通过stub_status模块实现对Nginx的监控。Nginx的ngx_http_stub_status_module模块提供了基本的Nginx状态信息，源码安装的话需要加上–with-http_stub_status_module编译参数，如果是epel源yum安装的话，已经默认启用该模块。 在Nginx配置文件中加入如下配置： 1234567891011121314location /nginx_status &#123; allow IP; deny all; stub_status on; access_log off;&#125;#栗子Active connections: 14server accepts handled requests 22889 22889 72510Reading: 0 Writing: 2 Waiting: 12 以上数据是通过Web端查看。但，我们需要把数据收集到Zabbix-Server。还需要使用之前下载同MongoDB，php-fpm一起的那个包。 操作，基本还是类似MongoDB，php-fpm。只是个别参数需要修改一下。 1234567891011121314151617181920cd ./zabbix-master/nginx/cp ./nginx.sh /etc/zabbix/chmod a+x /etc/zabbix/nginx.shcp ./userparameter_nginx.conf /etc/zabbix/zabbix_agentd.dvim /etc/zabbix/nginx.sh#HOST=&quot;localhost&quot;PORT=&quot;80&quot;#stub_status=stub_statusstub_status=nginx_statusvim /etc/zabbix/zabbix_agentd.d/userparameter_nginx.conf#修改成脚本对应的位置UserParameter=nginx.status[*],/etc/zabbix/nginx.sh $1 现在想以前一样导入模板。然后在链接模板就可以了。 监控Redis监控Redis，也是把包里面对应的文件复制过去就行。 123456789101112131415cd ./zabbix-master/rediscp ./userparameter_redis.conf /etc/zabbix/zabbix_agentd.d/#如果redis设置有密码，请加上密码#如果是不同的端口，请修改UserParameter=redis.local[*],redis-cli -h 127.0.0.1 -p 6379 info|grep $1|grep -v _human|cut -d : -f2#UserParameter=redis.local[*],redis-cli -h 127.0.0.1 -p 6379 -a Password info|grep $1|grep -v _human|cut -d : -f2UserParameter=redis.status[*],redis-cli -h $1 -p $2 -a Password info|grep $3|grep -v _human|cut -d : -f2UserParameter=redis.proc,pidof redis-server | wc -l#重启服务systemctl restart redis 导入模板，链接主机，OK。 系统监控 CPUCPU的性能状态信息： 简写 描述 说明 us user cpu tim 用户使用CPU时间 sy system cpu time 系统使用CPU时间 id idle cpu time CPU的空闲时间 wa io wait cpu time CPU等待IO时间 ni user nice cpu time 用nice调整进程优先级的CPU时间 st steal time 虚拟机偷取的CPU时间比，被强制等待虚拟CPU的时间 si softirq time 系统处理软件中断所花费的CPU时间 hi hard time 系统硬中断所花费的CPU时间 interrupt 中断 被处理过的中断数 cs Context switches 上下文切换 ql processor queue length 队列长度 processor load processor load 处理器负载，几核乘以几 Tips 当我们要监控并报警CPU使用率时，我们可以反过来用CPU空闲时间来定义 cpu idle tiem% + cpu usage time% = 1 (CPU usage time% gt 80%) == (CPU idel time% lt 20%) (CPU usage time% gt 90%) == (CPU idel time% &lt; 10%) 所以监控CPU使用率就可以监控CPU空闲时间，并依据这个报警 内存Zabbix中自带的Linux OS模板提供了Total memory和Available memory选项，这两者直接用模板就可以了。但没有提供内存使用率的选项，因此需要我们自定义。 内存使用率 = 可用内存 / 总内存 ast(vm.memory.size[available])/last(vm.memory.size[total]) 自定义内存使用率我们只需要在Linux OS模板下配置内存使用率，就可以一劳永逸。 配置(Configuration) 模板(Templates) OS Linux模板的监控项(Items) 创建监控项 监控项名称: Available memory percent 类型： 可计算的 键值： vm.memory.size[percent] 公式： 100*last(vm.memory.size[available])/last(vm.memory.size[total]) 记得将其加入Memory应用集，这样便于查找和管理 可加入单位： % 添加触发器 配置 模板 OS Linux模板 触发器 创建触发器，当可用内存率在三分钟内的平均值小于20%时报警 名字：Available memory percent lt 20% on {HOST.NAME} 严重性：一般严重 表达式： {Template OS Linux:vm.memory.size[percent].avg(3)}&lt;20 磁盘由于Zabbix-Server自带的Linux OS模板中的filesystem的监控是一个自动发现规则，而在应用集中的filesystem是没有监控项的。所有对于磁盘的监控和触发要在自动发现规则中去定义。 进程和端口Zabbix-Server自带有检测进程和端口的键值对。 检测进程数proc.num[&lt;name&gt;,&lt;user&gt;,&lt;state&gt;,&lt;cmdline&gt;] name: 进程名； user: 运行该进程的用户； state: run sleep zomb cmdline: ps -ef的最后那项，如/usr/bin/mongod -f /etc/mongod.conf 现在Zabbix-Server端测试： 1234567891011#zabbix-get --host hostname --key proc.num[&lt;name&gt;,&lt;user&gt;,&lt;state&gt;,&lt;cmdline&gt;]#检测mongd进程数量zabbix-get --host 192.168.1.11 --key proc.num[mongod,,,]#2，因为我开了两个mongd实例zabbix-get --host 192.168.1.11 --key proc.num[mongod,root,,]#1，只有一个是以root运行的，有一个是以mongod运行的 由于我们上面使用的MongoDB监控模板没有判断mongod进程存活与否的判断，此处我们在MongoDB模板中增加一个检查mongod进程的监控项，并创建对应的触发器。 端口net.tcp.listen[port] 检查 TCP 端口 是否处于侦听状态。返回 0 - 未侦听；1 - 正在侦听 此处我也用Mongod举例。我的两个mongod实例分别监听在27017,27018/tcp。 在Zabbix-Server端先测试： 123456#net.tcp.listen[port]zabbix-get --host 192.168.1.11 --key net.tcp.listen[27017]#1zabbix-get --host 192.168.1.11 --key net.tcp.listen[27018]#1 在Web端创建监控项和触发器与上面类似。 用户自定义参数(user parameter)我也是参考了上述大神的脚本，进行参考而来。 由于公司需要监控大量的Web页面和API接口的状态，并通过页面判断相关key-value的正确性，用以判断状态。此处可能由模拟登录等操作，Zabbix自带的Web监控不太够用，所以此处自定义用户参数来实现。 此处，我叫公司开发人员帮忙将全部接口以及Web页面内容都生成到一个json文件里，如 http://zhang21.cn/test.json。然后用jq命令解析json文件，里面key一一对应value，这样取值就很方便了。 jqjq 是一款命令行下处理 JSON 数据的工具。真的很好用！ jq官网：https://stedolan.github.io/jq/GitHub: https://github.com/stedolan/jq 安装jq 1yum install -y jq 使用jq 123456789101112131415161718192021222324252627282930313233jq --help#查看所有键键值curl --silent http://zhang21.cn/test.json | jq .###栗子&#123; "collapsectimes": 130, "collapsectimes": 0, "bootfailtimes": 23, "failrate": 0.3623, "bootrate": 0.3324, "time": "2018-01-25 15:03:30", "db_error": false&#125;#查看某个键值curl --silent http://zhang21.cn/test.json | jq '.time'curl --silent http://zhang21.cn/test.json | jq '.bootrate'###2018-01-25 15:03:300.3324#查看某个不存在的值，会返回nullcurl --silent http://zhang21.cn/test.json | jq '.zhang'###null json嵌套解析 1234cat test.json | jq '.location.city'###"Chengdu" json解析数组 1234cat test.json | jq '.array[1].name'###"Zhang" 内建函数 jq还有一些内建函数，如key,hss。 key用来获取json中的key元素： 123456789101112curl --silent http://zhang21.cn/test.json | jq 'keys'###[ collapsectimes, collapsectimes, bootfailtimes, failrate, bootrate, time, db_error] has用来判断是否存在某个key: 1234curl --silent http://zhang21.cn/test.json | jq 'has("time")'###true jq的select语句使用select函数来完成jq的过滤操作。jq的select语句太好了! select 接受一个条件表达式作为参数。其输入可以是迭代器，或者和 map 函数配合使用来处理数组。当输入中的某个元素使 select 参数中的条件表达式结果为真时，则在结果中保留该元素，否则不保留该元素。 对json文件的值是数组的，根据数据里面的key在取值，厉害厉害。 123456789101112131415161718192021222324252627cat zhang.json"array": [&#123; "ip": "192.168.1.11", "loads": 1234&#125;,&#123; "ip": "192.168.1.22", "loads": 567&#125;]####栗子cat /etc/zabbix/zhang.json | jq ".array[] | select(.ip == \"192.168.1.11\")"&#123; "ip": "192.168.1.11", "loads": 1234&#125;cat /etc/zabbix/zhang.json | jq ".array[] | select(.ip == \"192.168.1.11\").loads"1234我们在自定义用户参数的时候便可以将ip作为参数传入cat /etc/zabbix/zhang.json | jq ".array[] | select(.ip == \"$1\").loads" 编写自定义参数和脚本将脚本放置于/etc/zabbix，可将自定义参数写入zabbix-agentd.conf文件，也可单独写入/etc/zabbix/zabbix_agentd.d/(推荐)，这样修改更方便。 编写脚本文件123456789101112131415161718192021222324252627282930cd /etc/zabbixvim xbreport.sh########### Zabbix3.4# Zhang21# Thu Jan 25 15:20:44 CST 2018###########url="http://zhang21.cn/test.json"JQ=`which jq`CURL=`which curl`function XBREPORT() &#123; $CURL --silent $url | $JQ ".$1"&#125;if [ $# == 0 ]; then echo $"Usage $0 &#123;browsercollapsectimes|servercollapsectimes|xiaobaibootfailtimes|terminaldesktopfailrate|competebootrate|db_error&#125;" exitelse XBREPORT "$1"fi 编写自定义参数文件123456789101112cd /etc/zabbix/zabbix_agentd.dvim userparameter_XBreport.conf########### Zabbix3.4# Zhang21# Thu Jan 25 15:45:19 CST 2018##########UserParameter=XBreport[*],/etc/zabbix/xbreport.sh $1 测试自定义参数1234zabbix_get --host host --key XBreport[time]###"2018-01-25 17:03:18" 自定义用户参数额外由于我的json文件key对应的value中内嵌有数组，所以我需要再提取数组内的值。 12345678910111213141516171819curl http://zhang21.cn/test.json | jq &apos;.array&apos;###[ &#123; &quot;ip&quot;: &quot;1.1.1.1&quot;, &quot;loads&quot;: 1051 &#125;, &#123; &quot;ip&quot;: &quot;2.2.2.2&quot;, &quot;loads&quot;: 356 &#125;]#array[],array[1],array[2],array[n]#array[].ip, array[1].ip#array[].loads, array[2].loads 上面的数据中包含有zabbix无法解析的特殊符号，所以需要改变策略。 由于zabbix对UserParameter中包含\’”`*?[]{}~$?&amp;;()&lt;&gt;|#@这些特殊字符无法进行处理，此处有两种方法来解决。 在zabbix_agentd.conf中开启参数UnsafeUserParameters，将其值设置为1 或者，使用多个变量$1 $2 $3...来解决我这个数组值的问题 我是使用多个变量来解决我这个情况的。看下脚本。 1234567891011121314151617181920212223242526272829303132cd /etc/zabbixvim ./zhang.shurl=&apos;http://www.zhang21.cn/test.json&apos;JQ=`which jq`CURL=`which curl`function ZHANG() &#123; $CURL --silent $url | $JQ &quot;.$1&quot;&#125;if [ $# == 0 ]; then echo $&quot;Usage $0 &#123;aaa|bbb|ccc|...&#125;&quot; exitelif [ $# ==1 ]; then ZHANG &quot;$1&quot;elif [ $# == 2 ]; then ZHANG &quot;$1[$2]&quot;else ZHANG &quot;$1[$2].$3&quot;ficd /etc/zabbix/zabbix_agentd.d/userparameter_Zhang.confUserParameter=Zhangxx[*],/etc/zabbix/zhang.sh $1 $2 $3 测试： 123456systemctl restart zabbix-agentdzabbix_get --host host --key Zhangxx[array]zabbix_get --host host --key Zhangxx[array,0]zabbix_get --host host --key Zhangxx[array,0,loads]zabbix_get --host host --key Zhangxx[array,1,ip] 测试正确能取到值的话，在Web端设置相对应的监控项。注意自己定义的key不要写错了。 数组的key栗子： Zhangxx[array] Zhangxx[array,0]或Zhangxx[array,1] Zhangxx[array,0,ip], Zhangxx[array,0,loads] 在Web端添加监控项由于这个参数是我们自定义的，所以在填写监控项key的时候需要我们手动填写自己定义的参数。注意监控项的参数和信息类型。 这里我遇到一个问题，我自定义key的执行脚本在Web端报超时问题，无法取值。这是由于zabbix默认的脚本执行超时时间为3s，所以我们需要修改超时时间30s(最大值)。 12vim /etc/zabbix/zabbix_server.confvim /etc/zabbix/zabbix_agentd.conf 设置触发器和报警这个就根据你个人项目实际情况设置对于的触发器和报警。 短信报警腾讯短信服务由于公司使用的是腾讯企业邮箱，可以将邮箱直接与微信绑定，从而在微信中实时显示邮件消息，所以不用微信报警！ 此处使用的腾讯短信SMS服务： https://cloud.tencent.com/product/sms 短信文档： https://cloud.tencent.com/document/product/382/13445 API文档： https://cloud.tencent.com/document/product/382/13297 SDK文档： https://cloud.tencent.com/document/product/382/5804 Python SDK: https://cloud.tencent.com/document/product/382/11672 由于腾讯提供了程序SDK，所以我选择了linux自带的Python SDK。这里面有详细的Python使用方法，做一些小修改就可以使用了。 配置获取Python SDK获取Python SDK 申请SDK AppID和App Key申请SDK AppID以及APP Key。 申请完毕后，效果如下： 申请短信签名下发短信必须携带短信签名。短信签名需要上传公司证件进行认证，大概十分钟左右！ 效果如下： 申请短信模板下发短信内容必须经过审核。在此短信模板中，我们必须要定义相关变量{n}，其他都是不会变化的常量。此处我定义了五个变量，分别为了带入Zabbix中的宏： 问题名，{TRIGGER.NAME} 主机名，{HOST.NAME} 事件事件，{EVENT.TIME} 事件日期，{EVENT.DATE} URL，{TRIGGER.URL} SDK配置1234yum install -y eple-realseyum install -y python-pippip install qcloudsms Python代码配置腾讯文档：https://cloud.tencent.com/document/product/382/11672 由于我是向多人发送短信，所以做了小修改： 12345678910111213141516171819202122232425262728293031323334#zabbix-servercd /usr/lib/zabbis/alartscriptsvim sendSms.py#!/bin/python#coding: utf-8from qcloudsms_py import SmsSingleSenderfrom qcloudsms_py.httpclient import HTTPErrorimport sysappid = App IDappkey = App Keyphone_numbers = ["12345", "1234567"]#params = ["Problem", "Hostname", "Time", "Date","Url"]params = [sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5]]msender = SmsSingleSender(appid, appkey)for i in phone_numbers: try: result = msender.send_with_param(86, i, 短信内容模板ID, params) except HTTPError as e: print(e) except Exception as e: print(e) print(result) 要给sendSms.py加上可执行权限哈！chmod a+x ./sendSms.py。 sys.argv变量是一个字符串的列表。特别地，sys.argv包含了命令行参数 的列表，即使用命令行传递给你的程序的参数。使用Python的sys.argv[n]可以像shell一样将放在文件后的变量传入文件执行。此处对于在Zabbix-Web端将宏放在脚本后，作为变量传入，非常重要。 sys.argv[0]代表sendSms.py文件 sys.argv[1]才代表第一个参数。 Zabbix Web端配置配置-动作-创建动作-操作 注意事项： 建议针对触发器示警度最高就行短信报警，其它交给Email 操作类型，选择远程命令 目标列表，选择当前主机 类型，自定义脚本 执行在，这个一定是放在Zabbix-Server上来执行哈 命令，文件名SendSms.py后面接的宏一定要加上双引号(“”) 最后根据不同的内容，设置不同的报警机制。后台的脚本也修改为对应的名称，修改里面对应的手机号码。 首先根据不同报警设置不同的触发条件 运维组，SendSms_dev.py，修改运维对应的号码 开发组，SendSms_develop.py，修改对于的号码 其实这个发送短信，就是在执行远程命令。 你命令里是发送短信就发送短信，你命令里是发送邮件就发送邮件。这个还是挺不错的。 针对不同业务向不同人员报警有时候我们只需要关心我们自己那部分就可以了，没必要所有报警都发送给所有人，这样很不方便。 所以，我们可以根据业务相关，组别权限等，分别向不同的人报警不同的信息。 如下我的一个栗子图：]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Love at First Sight]]></title>
    <url>%2F2017%2F11%2F06%2FLove-at-First-Sight%2F</url>
    <content type="text"><![CDATA[——波兰诗人维斯拉瓦·辛波丝卡(Wislawa Szymborska) They’re both convincedthat a sudden passion joined them. Such certainty is beautiful,but uncertainty is more beautiful still. Since they’d never met before,they’re sure that there’d been nothing between them. But what’s the word from the streets, staircases, hallways –perhaps they’ve passed each other a million times? I want to ask themif they don’t remembera moment face to facein some revolving door?perhaps a “sorry” muttered in a crowd?a curt “wrong number” caught in the receiver?but I know the answer. No, they don’t rememberThey’d be amazed to hearthat Chance has been toying with themnow for years. Not quite ready yetto become their Destiny,it pushed them close, drove them apart,it barred their path, stifling a laugh,and then leaped aside. There were signs and signals,even if they couldn’t read them yet. Perhaps three years agoor just last Tuesdaya certain leaf flutteredfrom one shoulder to another? Something was dropped and then picked up.Who knows, maybe the ball that vanished into childhood’s thicket? There were doorknobs and doorbellswhere one touch had covered another beforehand. Suitcases checked and standing side by side.One night, perhaps, the same dream,grown hazy by morning. Every beginning is only a sequel,after all,and the book of eventsis always open halfway through.]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>Poetry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinuxShell]]></title>
    <url>%2F2017%2F10%2F24%2FLinuxShell%2F</url>
    <content type="text"><![CDATA[参考： 《Linux Shell脚本攻略》 《鸟哥的Linux私房菜》 vi vim在Linux中使用文本编辑器来编辑你的Linux参数配置文件是一件很重要的事情，因此系统管理员至少应该熟悉一种文本编辑器。 在Linux中，绝大部分的配置文件都是以ASCII(键盘上可找到)的纯文本形式。因此利用简单的文本编辑器就能修改。 ASCII（发音：/ˈæski/ ass-kee[1]，American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统。从[0000 0000 - 0111 1111]共128个字符。 vi文本编辑器 所有的Unix Like系统都会内置vi文本编辑器，其他的文本编辑器则不一定存在 很多软件的编辑接口都会主动调用vi(如 crontab等命令) vim是vi的高级版本 vim具有程序编辑的能力，可以主动以字体颜色辨别语法的正确性，方便程序设计 程序简单，编辑速度相当快速 vi中的tab键所得结果与空格符不一样 vi中，数字是很有意义的 数字通常代表重复做几次，或去到第几个的意思 vim保存、恢复与打开文件时的警告信息当我们使用vim时，vim会在当前目录下再创建一个名为filename.swp的暂存文件。 由于vim的工作被不正常中断，导致暂存盘无法通过正常流程来结束，所以暂存文件就不会消失。此时编辑文件就会出现某些异常情况。 可能有其他人或程序同时在编辑这个文件 在前一个vim的环境中，可能因为某些不明原因导致vim中断(crashed) vim的三种模式vim包括三种模式： 一般模式 编辑模式 命令行模式 一般模式 命令 说明 x 向后删除一个字符 X 向前删除一个字符 nx,nX 向前/后 删除n个字符 dd 删除当前行 D 删除当前行所有字符，使之成为空行 ndd 删除光标所在行的向下n行 d1G 删除光标所在行到第一行 dG 删除光标所在行到最后一行 yy 复制光标所在行 y1G 复制光标所在行到第一行 yG 复制光标所在行到最后一行 ynj 复制光标所在行和向下n行 dnj 删除光标所在行和向下n行 p 将复制的数据粘贴到光标下一行 P 将复制的数据粘贴到光标上一行 J 将光标所在行与下一行结合成一行 u undo,恢复前一个操作 ctrl+r 重做上一个操作 . 重复前一个操作 编辑模式 命令 说明 i 在当前光标所在处插入文字 I 在光标所在行第一个非空字符插入文字 a 在当前光标后插入文字 A 在当前光标所在行最后插入文字 o 在光标所在行的下一行行首插入字符 O 在光标所在行的上一行行首插入字符 r 替换光标所在那一个字符 R 一直替换光标所指的文字，直到退出 Esc 退出，回到一般模式 命令模式 命令 说明 h 方向左 j 方向下 k 方向上 l 方向右 + 光标移到下一行的第一个非空字符 - 光标移到当前行的第一个非空字符 0 光标移到当前行的第一个字符 $ 光标移到当前行的最后一个字符 n空格 光标在当前行向右移动n个字符 G 光标移到最后一行的第一个非空字符 gg 光标移到第一行的第一个非空字符，相当于1G nG 光标移到第n行的第一个非空字符 /word 在光标之后查找word字符串 ?word 在光标之前查找word字符串 n/N 重复前一个查找 :s/word1/word2 在光标当前行将word1替换成word2 :n1,n2s/word1/word2/g 在n1行-n2行间将word1替换成word2 %s/word1/word2/gc 全局将word1替换成word2，在替换前让用户确认(confirm) :w 保存到文件 :w file2 保存到file2文件 :r file3 从file3文件读取数据并写入 :wq/:x 保存并退出 :q 退出 :q! 强制退出 :!cmd 执行命令 :r!cmd 将执行命令写入 :set nu 显示行号 :set nonu 取消行号 :n1,n2w file4 将n1行-n2行的内容保存到file4文件 vim环境设置与记录因为vim会主动将你曾经的行为记录下来，好方便下次操作。这个文件是自动生成的。 ~/vim.info ~/vim.rc 整体vim设置 /etc/vimrc 此外，每个Distribution对vim的默认黄金都不太相同。所以你可能需要设置成你自己的工作方式。 参数 说明 :set nu :set nonu 行号设定 :set hlsearch :set nohlsearch 高亮设定 :set autoindent :set noautoindent 自动缩排设定 :set backup 自动备份设定 :set ruler 状态栏设定 :set showmode 模式显示设定，如INSERT :set backspace=(012) 设定退格(backspace)值 :set all 显示所有环境参数 :set 显示与系统默认值不同的参数 :syntax on/off 程序语法显示 :set bg=dark/light 设定背景颜色 栗子： 123456789vim /root/.vimrc"这是注释"set nuset rulerset bg=darksyntax onset hlsearch vim注意事项 中文编码问题修改语系编码： LANG=zh_CN.utf-8 Linux与Dos的换行字符 Linux的换行(Enter)为LF符号($) Dos的换行(Enter)为CRLF符号(^M$) 不同系统之间复制纯文本文件可能会有问题，此时可以转换： unix2dos file newfile dos2unix file newfile 语系编码转换iconv - convert text from one character encoding to another 1234#iconv -f 源编码 -t 新编码 filename [-o newfile]#-o，转换到新文件iconf -f big4 -t utf8 old.big5 -o new.utf8 Unicode UTF-8 ASCII参考： ASCII: https://zh.wikipedia.org/wiki/ASCII Unicode: https://zh.wikipedia.org/wiki/Unicode Unicode计算机处理的是数字(二进制文件)。他们在存储字符时要给每个字符分配一个数值。 早期的编码系统称为 ASCII（美国信息交换标准码）， 一共有128（0-127）个值，每个值用7bit 保存。ASCII可以满足小写、大写、数字标点符号和一些控制字符的处理。 人们曾尝试将ASCII字符扩展到8bit，这种新的被称为“扩充ASCII”的编码一直没有成为国际性标准。 为了克服ASCII和扩充ASCII先天上的不足，Unicode Consortiun（多语言软件生产商群体）创建了一种能够提供广泛字符集的通用编码系统，称为Unicode。 Unicode最初设置为2Byte的字符集。但版本3的Unicode用的是4Byte编码，并且与ASCII与扩充的ASCII完全兼容。 现在被称为Basic Latin（基本拉丁文）的ASCII字符集就是前25位全部置零的Unicode码。现在被称为 Latin-1（拉丁文1）的扩充ASCII字符集就是前24位全部置零的Unicode码。 Unicode中的每个字符或符号由一个32bit数来定义，因此这种编码可以定义高达2的32次方(4 294 067 296)个字符或符号。它的记法使用了十六进制数字，格式如下： 1234U-XXXXXXXX#每个 X 都是一个十六进制的数字#因此，它的数值从U-00000000到U-FFFFFFFF ASCII美国信息交换码（American Standard Code of Information Internet，ASCII）是一种7bit码，设计来为128个大多数是美国英语里使用的符号提供编码。今天的ASCII码已成为Unicode的一部分，它占据了Unicode中的前128个码（00000000-0000007F）。 ASCII的一些特点： space(20-sp)字符，是一个可打印的字符，打印出一个空格 大写字母从(41-A)开始，小写字母从(61-a)开始。按ASCII比较时，大写字母的数值会小于小写字母 大写字母与小写字母在他们的7bit编码中只有1bit不同，A(1000001)，a(1100001)，两者相差(20)十六进制 小写字母并没有紧跟在大写字母后面，这两者之间还有几个标点符号(5B-60) 数字从(30-0)开始 从00到1F这最开始的32个字符加上最后一个字符(7F)全都是非打印字符。字符(00)被用作定界符，已定义字符串的结束。字符(7F)是删除字符，它被某些编程语言用来删除前一个字符。剩下的非打印字符称为控制字符，用于数据通信 ASCII控制字符： 二进制 十进制 十六进制 缩写 Unicode表示法 脱出字符表示法 名称/意义 0000 0000 0 00 NUL ␀ ^@ 空字符（Null） 0000 0001 1 01 SOH ␁ ^A 标题开始 0000 0010 2 02 STX ␂ ^B 本文开始 0000 0011 3 03 ETX ␃ ^C 本文结束 0000 0100 4 04 EOT ␄ ^D 传输结束 0000 0101 5 05 ENQ ␅ ^E 请求 0000 0110 6 06 ACK ␆ ^F 确认回应 0000 0111 7 07 BEL ␇ ^G 响铃 0000 1000 8 08 BS ␈ ^H 退格 0000 1001 9 09 HT ␉ ^I 水平定位符号 0000 1010 10 0A LF ␊ ^J 换行键 0000 1011 11 0B VT ␋ ^K 垂直定位符号 0000 1100 12 0C FF ␌ ^L 换页键 0000 1101 13 0D CR ␍ ^M Enter键 0000 1110 14 0E SO ␎ ^N 取消变换（Shift out） 0000 1111 15 0F SI ␏ ^O 启用变换（Shift in） 0001 0000 16 10 DLE ␐ ^P 跳出数据通讯 0001 0001 17 11 DC1 ␑ ^Q 设备控制一（XON 激活软件速度控制） 0001 0010 18 12 DC2 ␒ ^R 设备控制二 0001 0011 19 13 DC3 ␓ ^S 设备控制三（XOFF 停用软件速度控制） 0001 0100 20 14 DC4 ␔ ^T 设备控制四 0001 0101 21 15 NAK ␕ ^U 确认失败回应 0001 0110 22 16 SYN ␖ ^V 同步用暂停 0001 0111 23 17 ETB ␗ ^W 区块传输结束 0001 1000 24 18 CAN ␘ ^X 取消 0001 1001 25 19 EM ␙ ^Y 连接介质中断 0001 1010 26 1A SUB ␚ ^Z 替换 0001 1011 27 1B ESC ␛ ^[ 退出键 0001 1100 28 1C FS ␜ ^\ 文件分区符 0001 1101 29 1D GS ␝ ^] 组群分隔符 0001 1110 30 1E RS ␞ ^^ 记录分隔符 0001 1111 31 1F US ␟ ^_ 单元分隔符 0111 1111 127 7F DEL ␡ ^? 删除 ASCII可显示字符: 进制 十进制 十六进制 图形 0010 0000 32 20 (space) 0010 0001 33 21 ! 0010 0010 34 22 “ 0010 0011 35 23 # 0010 0100 36 24 $ 0010 0101 37 25 % 0010 0110 38 26 &amp; 0010 0111 39 27 ‘ 0010 1000 40 28 ( 0010 1001 41 29 ) 0010 1010 42 2A * 0010 1011 43 2B + 0010 1100 44 2C , 0010 1101 45 2D - 0010 1110 46 2E . 0010 1111 47 2F / 0011 0000 48 30 0 0011 0001 49 31 1 0011 0010 50 32 2 0011 0011 51 33 3 0011 0100 52 34 4 0011 0101 53 35 5 0011 0110 54 36 6 0011 0111 55 37 7 0011 1000 56 38 8 0011 1001 57 39 9 0011 1010 58 3A : 0011 1011 59 3B ; 0011 1100 60 3C &lt; 0011 1101 61 3D = 0011 1110 62 3E &gt; 0011 1111 63 3F ? 0100 0000 64 40 @ 0100 0001 65 41 A 0100 0010 66 42 B 0100 0011 67 43 C 0100 0100 68 44 D 0100 0101 69 45 E 0100 0110 70 46 F 0100 0111 71 47 G 0100 1000 72 48 H 0100 1001 73 49 I 0100 1010 74 4A J 0100 1011 75 4B K 0100 1100 76 4C L 0100 1101 77 4D M 0100 1110 78 4E N 0100 1111 79 4F O 0101 0000 80 50 P 0101 0001 81 51 Q 0101 0010 82 52 R 0101 0011 83 53 S 0101 0100 84 54 T 0101 0101 85 55 U 0101 0110 86 56 V 0101 0111 87 57 W 0101 1000 88 58 X 0101 1001 89 59 Y 0101 1010 90 5A Z 0101 1011 91 5B [ 0101 1100 92 5C \ 0101 1101 93 5D ] 0101 1110 94 5E ^ 0101 1111 95 5F _ 0110 0000 96 60 ` 0110 0001 97 61 a 0110 0010 98 62 b 0110 0011 99 63 c 0110 0100 100 64 d 0110 0101 101 65 e 0110 0110 102 66 f 0110 0111 103 67 g 0110 1000 104 68 h 0110 1001 105 69 i 0110 1010 106 6A j 0110 1011 107 6B k 0110 1100 108 6C l 0110 1101 109 6D m 0110 1110 110 6E n 0110 1111 111 6F o 0111 0000 112 70 p 0111 0001 113 71 q 0111 0010 114 72 r 0111 0011 115 73 s 0111 0100 116 74 t 0111 0101 117 75 u 0111 0110 118 76 v 0111 0111 119 77 w 0111 1000 120 78 x 0111 1001 121 79 y 0111 1010 122 7A z 0111 1011 123 7B { 0111 1100 124 7C l(管道线) 0111 1101 125 7D } 0111 1110 126 7E ~ ASCII缺点：ASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数目字和英式标点符号。因此现在的软件系统大多采用Unicode。 UTF-8UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。它可以用来表示Unicode标准中的任何字符，且其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部分修改，即可继续使用。 UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。 UTF-8的编码规则很简单，只有二条: 对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的 对于n字节的符号（n&gt;1），第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码 Bash bash与shell管理整个计算机硬件的其实是操作系统的内核(kernel)。这个内核是需要被保护的，所以一般用户就只能通过shell来跟内核通信，让内核达到我们想要达到的工作。 硬件、内核与shell我们必须通过shell，将我们输入的命令与内核通信，让内核可以控制硬件正确无误的工作。 操作系统其实是一组软件。由于这组软件在控制整个硬件与管理系统的活动监测，如果这组软件能被用户随意操作，若用户应用不当，将会使得整个系统奔溃。因为操纵系统管理的就是整个硬件功能，所以当然不能够被随便一些没有管理能力的终端用户随意使用。但我们总是需要让用户操作系统的，所以就有了在操作系统上面发展的应用程序。用户可以通过应用程序来指挥内核，让内核达到我们所需要的硬件任务。 也就是说，只要能够操作应用程序的接口都能够称为shell。狭义的shell指的是命令行方面的软件，包括bash等。广义的shell则包括图形界面的软件。 命令行shell 各Distribution的命令行界面都一样 远程管理非常快速 Linux的任督二脉 系统合法shell与/etc/shells由于shell依据发布者的不同就有许多版本，例如Bourne SHell（sh）、C SHell、K SHell、TCSH等。 Linux默认使用的这一版本就是Bourne Again SHell(bash)，这个shell是Bourne SHell的增强版，也是基于GNU框架下发展出来的。 检查系统可用shell: cat /etc/shells合法shell要写入etc/shells，系统某些服务在运行过程中，会去检查用户能够使用的shell。 查看用户shell权限： cat /etc/passwd，最后一行便是默认shell。 bash shellbash是GNU计划中重要的工具软件之一，目前也是Linux distributions 的标准shell。bash主要兼容于sh，并且依据一些用户的需求而加强shell 版本。 bash的优点： 命令记忆能力history 命令与文件补全功能tab 命令别名设置功能alias 作业控制、前台、后台控制(job control, foreground, background) 使用前台、后台的控制可以让作业进行得更为顺利。至于作业控制(jobs)的用途更广，可以让我们随时将工作丢到后台中执行，而不怕不小心使用ctrl+c来中断该进程 程序脚本shell script 通配符(Wildcard) type命令type命令用于判断一个命令是內建命令还是外部命令(非bash提供)。 1234567type lstype -t ls#file，外部命令#alias，别名#builtin，內建命令 shell变量变量就是以一组文字或符号等，来替代一些设置或者是一串保留的数据。 变量显示与设置 echo: 显示变量 echo $PATH unset: 取消变量 unset $ZHANG 变量设置规则 变量与变量内容以一个等号=连接，如myname=zhang 等号两边不能有空格符，否则错误 变量名称只能是英文字母和数字，开头字符不能是数字 变量内容若有空格，可使用双引号或单引号 双引号内的特殊符号，保有原本的特性 单引号内的特殊字符则仅为一般字符 转义字符\\，将特殊符号变成一般字符 在一串命令中，还需要使用其他命令，使用反单引号 反引号``内的命令将被优先执行，而其执行结果将作为外部的输入信息 若该变量为了增加变量内容时，可用$变量名称 或${变量}累加内容 myname=${myname}xxx 若该变量需要在其他子进程执行，请以export来使变量变成环境变量 通常大写字符为系统默认变量，自行设置变量可以使用小写字符，方便判断 什么是子进程？在我目前这个shell下，去打开另一个新的shell。新的那个shell就是子进程。在一般状态下，父进程定义的变量是无法在子进程内使用的，要通过export将变量变成环境变量后才可以。 注意单引号、双引号和反引号。 环境变量环境变量可以帮我们达到很多功能，包括主文件夹的变换、提示符的显示、执行文件查找的路径等。 env: 显示环境变量 set: 查看所有变量 包括环境变量和自定义变量 1234567#HOME，用户主目录#SHELL，当前环境使用的shell#HISTSIZE，历史命令#PATH，执行文件查找路径#LANG，语系#$PS1，命令提示符#PS2，第二行提示符 设置$PS1，$PS2: 12345678910111213141516171819202122232425\d #可显示出『星期 月 日』的日期格式，如：&quot;Mon Feb 2&quot;\H #完整的主机名\h #仅取主机名在第一个小数点之前的名字\t #显示时间，24小时格式的『HH:MM:SS』\T #显示时间，为12小时格式的『HH:MM:SS』\A #显示时间，为24小时格式的『HH:MM』\@ #显示时间，为12小时格式的『am/pm』\u #目前使用者的账号名称，如『root』\v #BASH的版本信息\w #完整工作路径名，由根目录写起的目录名称。但家目录会以 ~ 取代\W #利用basename函数取得工作目录名称，所以仅会列出最后一个目录名。\# #下达的第几个命令\$ #提示字符，root时，提示字符为#；否则就是$ $钱字号本身也是变量，代表当前shell的PID –&gt; echo $$ ?问号也是一个特殊变量，代表上一个运行命令的回传值 –&gt; echo $? 0 命令运行成功 errorcode 命令运行错误 语系变量locale - get locale-specific information. 设置LANG的时候，其他的语系变量就会被这个变量所替代。 变量键盘读取、数组与声明 read： 读取来自键盘输入的变量 declare,typeset: 声明变量类型 变量的默认类型为字符串 若不指定变量类型，则1+2就是一个字符串而不是计算式 数组变量类型 var[1]=’varray1’ var[2]=’varray2’ echo “${${var[1]}, ${var[2]}}” bash shell操作环境自定义我们登录主机的时候屏幕上面会有一些说明文字，并且登录的时候还可以给用户提供一些信息或者欢迎文字，或环境变量和命令别名等。 路径与命令查找顺序命令的运行顺序： 以绝对/相对路径执行命令 由alias找到该命令来执行 由bash内置的（builtin）命令来执行 通过$PATH这个变量的顺序找到的第一个命令来执行 bash登录与欢迎消息 /etc/issue –&gt; 终端登录消息 CentOS Linux 7 (core)….. /etc/motd –&gt; 用户登录后取得一些消息 Welcome to aliyun ECS bash环境配置文件操作系统有一些环境配置文件的存在，让bash在启动时直接读取这些配置文件，以规划好bash的操作环境。这些配置文件又可以分为全体系统的配置文件以及用户个人偏好配置文件。 命令别名、自定义的变量在你注销bash后就会失效。所以你想要保留你的设置，就得要将这些设置写入配置文件才行。 login shell 取得bash需要完整的登录流程 non-login shell 取得bash接口的方法不需要登录 bash shell快捷键 Ctrl+C –&gt; 终止当前命令 Ctri+D –&gt; 输入结束(EOF) Ctri+M –&gt; Enter Ctrl+S –&gt; 暂停屏幕输出 Ctrl+Q –&gt; 恢复屏幕输出 Ctrl+U –&gt; 在提示字符下，将整列命令删除 Ctrl+Z –&gt; 暂停目前命令 通配符与特殊符号通配符： 符号 说明 * 代表0-∞个 任意字符 ? 代表一定有一个 任意字符 [-] 中括号内任一字符 [^] 非中括号内字符 bash常见特殊符号，理论上文件名不要用到上述字符。 符号 说明 # 注释 \ 转义字符 1 管道线 ; 连续命令分隔符 ~ 用户主目录 $ 取变量前导符 &amp; 将命令放入后台 ! 逻辑非 / 目录符号 &gt;, &gt;&gt; 输出定向 &lt;, &lt;&lt; 输入定向 ‘’ 单引号 “” 双引号 () 子shell {} 命令区块混合 重定向数据流重定向就是将某个命令执行后应该要出现在屏幕上的数据传输到其他的地方，如文件或设备。 标准输入(stdin)，代码为0，使用&lt;或者&lt;&lt; 标准输出(stdout)，代码为1，使用&gt;或者&gt;&gt; 标准错误(stderr)，代买为2，使用2&gt;或者2&gt;&gt; &gt;表示以覆盖方式写入，&gt;&gt;表示以追加方式写入 管道管道命令使用 “ | “ 这个界定符号。管道命令” | “ 仅能处理经由前面一个命令传来的正确信息。所以对stderror没有直接处理能力。 在每个管道后面接的第一个数据必定是命令，而且这个命令必须要能够接收standard input的数据才行，这样的命令才可以是管道命令。 Bash特殊符号在编写shellscripts的时候，特殊符号也有其重要的功能。 符号 描述 栗子 #! shellban，申明脚本所使用的shell #!/bin/bash \ 转义字符 \n l 管道 stdout l grep &gt;,&gt;&gt; 输出定向 &gt; 1.txt &lt;,&lt;&lt; 输入定向 &lt; 1.txt 2&gt; 错误定向 2&gt; error.txt ; 连续命令分隔符 cmd1;cmd2 &amp;&amp; 与，只有当前命令完成后才执行后一个命令 cmd1 &amp;&amp; cmd2 ll 或，或此或彼 cmd1 ll cmd2 ~ 用户家目录 cd ~ # 注释符 #comments $ 取用变量前导符 $PATH或${PATH} &amp; 工作控制，将命令放入后台(bg) command&amp; * ? [] [-] [^] 通配符 .sh ?.sh [a-z].txt [^zhang].txt ! 逻辑非 != = 两边无空格 赋值符号 name=zhang = 两边有空格 比较符号 if [ $name = zhang ] $0 执行文件脚本名 /root/zhang.sh \$1, \$2 第1,2个…变量 ./zhang.sh start $# 参数个数 if [ $# -ne 2 ]；then echo &#39;Usage: $0 arg1 arg2&#39; $@ 代表$1,$2,$3…之意 每个变量是独立的 $* 代表$1c$2c$3…之意 c为分割字符，默认为空格键 $? 命令状态码，成功为0 $? $$ 当前shell的PID echo $$ ‘单引号’ 单引号内特殊字符仅为一般字符 echo &#39;$host&#39;--$host “双引号” 双引号内特殊符号，可保有原本特性 echo &quot;$host&quot; --localhost `反引号` 运行命令 反引号内命令先执行 () 以子shell方式执行 $(date) {} 命令区块的组合 PS1 命令提示符 $PS1 PS2 第二行以后的提示字符 $PS2 shift 移动参数 shift后面可以接数字，代表拿掉最前面的几个参数 set 查看所有变量 set unset 取消变量 unset name，没有$符号 export 使某变量成为环境变量 export name，没有$符号 source source命令通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录 source file shell scriptshell script 有点像早期的批处理程序，即将一些命令汇整起来一次执行.但shell script拥有更强大的功能，可以进行类似程序(program)的编写，并且不需要经过编译(compile)就能执行。 shell script介绍shell script是利用shell的功能写的一个程序(program)。这个程序是使用纯文本文件，将一些shell的语法与命令(含外部命令)写在里面，搭配正则表达式、命令管道与数据流重定向等功能，还提供了数组、循环、条件与逻辑判断等重要功能， 以达到我们所想要的处理目的。 shell script用在系统管理上面是很好的一项工具，但用在处理大量数值运算上就不够好。因为shell script的速度较慢，且使用的cpu资源较多，造成主机资源的分配不良。 使用shell script的优势： 自动化管理的重要依据 追踪与管理系统的重要工具 简单入侵检测功能 连续命令单一化 简单的数据处理 跨平台支持与学习历程较短 shell script注意事项： 命令的执行是从上到下从左到右，分析与执行 命令的执行中：命令、参数间的多个空白都会被忽略掉 空白行也将被忽略，tab按键所得的空白同样视为空格键 读取到一个Enter符号(CR)，就尝试开始执行该行命令 一行内容太多，则可以使用\[Enter]来扩展到下一行 任何加在#后面的内容都将被视为注释而被忽略 shell script文件的执行方式： 直接命令执行 .sh文件必须具有可读和可执行权限，使用绝对路径或相对路径来执行 以bash进程来执行 bash xx.sh sh xx.sh shell script执行方式的区别： 直接执行，script是在子进程的bash中执行的。当子进程完成后，子进程内的各项变量或操作将会结束而不会传回到父进程中。 source来执，在父进程中执行 编写一个shell script一个良好的shell script应该纪录好如下信息： script的功能 script的版本信息 script的作者 script的版权声明方式 script的History（历史记录） script内较特殊的命令，使用绝对路径的方式来执行 script执行时需要的环境变量预先声明与设置 在较为特殊的程序代码部分，建议务必要加上批注说明 shell script判断式当我要检测系统上某些文件或相关属性时，使用test命令。 1test -e /root/test.txt &amp;&amp; echo 'Exist' || 'Not exist' 文件类型判断： 选项 说明 -e 是否存在 -f 是否存在文件 -d 是否存在目录 -b 是否存在block device -c 是否存在character device -S 是否存在Socket文件 -p 是否存在pipe文件 -L 是否存在链接文件 文件权限判断： 选项 说明 -r 是否可读 -w 是否可写 -x 是否可执行 -u 是否具有SUID -g 是够具有SGID -k 是否具有Sticky bit -s 是否为非空白文件 文件之间的比较： 选项 说明 -nt newer than -ot old than -ef 是否为同一个文件 整数之间的比较： 选项 说明 -eq equal -ne not equal -gt greater than -lt less than -ge greater or equal -le less or equal 字符串之间的比较： 选项 说明 -z 是否为空 -n 非空 str1 = str2 是否相等 != 不等于 多重条件判断： 选项 说明 -a and -o or ! 非 判断符号[]: 如果需要在bash中使用中括号来作为shell的判断式时，必须要注意中括号的两端需要有空格符来分隔。 中括号内的变量，每个最好都用双引号括起来 中括号内的常量，最好都以单或双引号括起来 shell script的默认变量: $0,$1…12/root/test.sh opt1 opt2 opt3 $0 $1 $2 $3 执行文件的脚本名就是$0 文件后接的第一个参数就是$1，以此类推 $#，表示参数个数 $@，表示”$1”, “$2”… shift，参数变量号码偏移 shift n，代表拿掉前面几个参数的意思 条件判断语句 if…then语句if…then 是最常见的条件判断式。 单层条件判断： 123if [ confition ]; then xxxfi 多层条件判断： 12345if [ condition ]; then xxx;else xxx;fi 1234567if [ confition1 ]; then xxx;elif [ condition2 ]; then xxx;else xxx;fi case…esac语句有多个既定变量内容，那么只需要针对这几个变量来设置状况就好。 12345678910111213141516171819202122232425262728293031case $变量名 in"$var1") xxx ;;"$var2") xxx ;;"...") xxx ;;;esac####栗子#/etc/init.d/networkcase "$1" instart) xxx ;;stop) xxx ;;restart) xxx ;;status) xxx ;;esac function功能什么是函数？函数可以在shell script 当中做出一个类似自定义执行命令的东西。最大的动能是，可以简化很多的程序代码。 因为shell script的执行方式是由上而下、由左而右。因此在shell script当中，function的定义一定要在程序的最前面，这样才能够在执行时被找到可用的程序段。 1234567891011function fname () &#123;&#125;####栗子function Zhang() &#123; echo $1 $2&#125;Zhang "$1" "$2" 循环(loop)语句 while do done(不定循环)while是当condition条件成立时，就进行循环，condition条件不成立就停止。 1234while [ condition1 ]do xxxdone until do done(不定循环)until是当condition条件成立时，终止循环；否则就持续进行循环的循环。 1234until [ condition ]do xxxdone for do done(固定循环)1234567891011for i in con1 con2 con3 ...do xxxdone####栗子for i in 192.168.1.&#123;1,2,3&#125;do ping -c 1 $idone for do done的数值处理： 1234567891011for ((初始值;限制值；步长))do xxxdone####栗子for ((i=0;i&lt;10;i++))do echo $idone shell script的追踪与调试(debug)最好在shell script执行之前先行调试。 123456789sh [-nvx] xxx.sh#-v 运行脚本前，先将脚本内容输入到屏幕#-n 仅查询语法问题#-x 边显示边执行当然也可以把这几个调试参数写到shellbang中#!/bin/bash -x 小试牛刀简介123456789101112#bash(Bourne Again Shell)，shell环境使得用户能与操作系统的内核进行交互操作#!/bin/bash#date#descriptioncmd1; cmd2cmd3#sh /path/xx.sh#Bash还有一个历史记录文件 ~/.bash_history 终端打印(echo)12345678910111213141516171819202122#终端作为交互式工具，用户可以通过它与shell环境进行交互echo '$var'echo $varecho -e "1\t2\t3"echo -e '\e[1;31m Red color \e[0m' #彩色echo &#123;1..10&#125; #输出1到10echo &#123;A..H&#125; #for i in &#123;a..z&#125;cat &lt;&lt; EOF112233EOF# \转义字符printf "%-5s %-10s $-4.2f\n" 001 Zhang 56.789#格式替代符%s %d %c %f, -左对齐 玩转变量和环境变量123456789101112131415161718192021#Bash中，每一个变量默认值值都是字符串形式#环境变量和自定义变量echo $SHELLecho $UIDvar=value #这是赋值#var = value这是相等操作echo $varecho $&#123;var&#125;echo $&#123;#var&#125; #字符数#export用来设置环境变量，此后，任何shell中的程序都会继承环境变量ZHANG=Gentlemanexport ZHANGPATH="$PATH:/home/zhang/bin"export $PATH 通过shell进行数学运算1234567891011121314151617181920212223242526272829303132#let, expr, bc, [], (())#要注意默认是字符串类型哦n1=1;n2=2let sum=n1+n2let n1++;let n2-=1sum=$[ n1 + n2 ]sum2=$(( sum + 3 ))sum=`expr 3 + 4`#浮点计算 bcecho "8 * 1.1" | bc#设置小数点精度echo "scale=2; 3/8" | bc#进制转换num=100echo "obase=2; $num" | bcnum=1100100echo "obase=10; ibase=2; $num" | bc#平方和平方根echo "sqrt(100)" | bcecho "10^2" | bc 文件描述符重定向12345678910111213141516#最常用的文件描述符是 stdin(0), stdout(1), stderr(2); 通过内容过滤将输出重定向到文件echo "This is a sample text 1" &gt; temp.txt #覆盖echo "This is sample text 2" &gt;&gt; temp.txt #追加ls + &gt;stdout.txt 2&gt;stderr.txtcmd 2&gt;&amp;1 /dev/null == com &amp;&gt; /dev/null #null设备也被称为黑洞#当一个command发生错误并退回时，它会返回一个非0的状态码echo $?#tee命令，一方面可将数据重定向到文件，另一方面还可提供一份重定向数据的副本作为后续命令的stdin#tee默认覆盖文件，-a选项追加cat temp.txt | tee tee.txt | cat -n 数组和关联数组123456789101112131415161718192021#数组借助索引将多个独立的数据存储为一个集合#普通数组只能使用整数作为数组索引，而关联数组可以使用字符串作为数组索引#还可将数组定义成一组索引-值(index-value)arr=(1 two 3 four 5)echo $&#123;arr[0]&#125;arr[0]=Oneindex=3echo $&#123;arr[$index] #arr[3]echo $&#123;arr[*]&#125;echo $&#123;#arr[*]&#125; #arr-length#关联数组可用任意文本作为数组索引declare -A ass_arrass_arr=([index1]=val1 [index2]=val2 ...) #内嵌索引-值ass_arr[index3]=val3 #独立索引-值echo $&#123;!ass_arr[*]&#125; #列出数组索引 别名(alias)123456789101112#alias作用是暂时的，关闭终端后别名就失效；#为使别名一直保持，可将其写入 ~/.bashrc，因为每一个新的shell都会执行~/.bashrc中的命令#新设置的别名将取代已有别名alias vi=vim;unalias viecho "alias ll='ls -l --color=auto'" &gt;&gt; ~/.bashrc#\对别名命令进行转义，执行原本的命令。避免攻击者利用别名将某些特权命令替换成别有用心的命令\vi test.sh 获取、设置日期和延时(date)1234567891011121314151617181920212223242526#很多应用程序需要以不同的格式打印日期，设置日期和时间，以及根据日期和时间执行操作;#延时通常用于在程序执行过程中提供一段等待时间;#在Unix-like系统中，日期被存储为一个整数，其大小为世界标准时间1970年1月1日0时0分0秒起所流逝的秒数；#这种计时方式被称之为 纪元时或Unix时间；#通过纪元时间，可知道两个日期之间相隔了多少秒#编写以循环方式运行的监视脚本时，设置时间间隔是必不可少的date +%s#!/bin/bashstart=$(date +%s)commandssleep 1end=$(date +%s)diff=$((end - start))echo "$diff seconds"#显示指定时间date +%F -d -1daysdate +%H -d -3hours#将时间转换为原子时间date -d '2018-02-07 14:05:53' +%s 调试脚本(sh)12345#调试功能能在出现一些异常情况时生成运行信息#!/bin/bash -xvsh -xsh -n 函数和参数(function)123456789101112131415161718192021function fname()&#123;statements&#125;fname()&#123;echo $1, $2 #访问第参数1和参数2,$n第n个参数echo "$@" #以列表的形式一次性打印所有参数echo "$*" #类似于$@，但参数被作为单个实体return 0 #f返回值&#125;fname 1 22 333 #返回上面定义的变量#递归函数，能够调用自身，不断地生成新的进程，最终会造成xx#导出函数，使用export导出，这样函数作用域就可以扩展到子进程export -f fname#读取命令返回值echo $? 读取命令序列输出(` `, $() )12345678910#输入通常是stdin，输出stderr或stdout,这些命令称为 过滤器(filter)。我们使用 管道(pipe) 来连接每一个过滤器cmd1 | cmd2 | cmd3#子shell，子shell生成独立的进程，不会对当前shell有任何影响，所做改变仅限于子shell内zhang=$(ls | cat -n)#反引用zhang=`ls | cat -n` 读取字符(read)123456789101112131415161718#read是一个重要的从标准输入中读取文本的命令#可以使用read以交互的形式来读取用户的输入read -n 5 zhang #读取字符数echo $zhangread -s passwd #不回显echo $passwdread -t 5 zhang #超时时间echo $zhangread -p zhang #显示提示信息echo $zhangread -d ":" zhang #定界符结束输入123：echo $zhang 字段分隔符和迭代器12345678910111213141516171819#内部字段分隔符(Internal Field Separator, IFS)是shell中的一个重要概念#IFS的默认值为空白字符(换行符、制表符、空格)awk -F: '&#123;print $1,$3&#125;' /etc/passwd #IFS=":"#对一些列值进行迭代，循环非常有用for i in &#123;1..10&#125;docmddonewhile conditiondocmddoneuntil conditiondocmddone 比较与测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#程序中的流程控制是由比较和测试语句来处理的if condition1 || condition2then cmd1elif condition3 &amp;&amp; condition4then cmd2else cmd3fi#算术比较if [ $num -ge 10 -a $num -lt 20 ]-eq-gt-ge-lt-le-a-o#文件系统相关if [ -f $file1 -o -x $file2]-x-w-r-f-d-e-b #block-l#字符串比较[[ $str1 = $str2]]= #=号旁有空格--是比较关系；=号旁没空格，是赋值语句!=&gt;&lt;-z #空字符-n #非空字符#使用test命令来执行条件检测if [ $num -eq 0 ] -- if test $num -eq 0 命令之乐简介各种命令可谓Unix-Like系统中优美的部分，它能帮我们搞定各种繁杂的任务。一旦你尝试过Linux提供的这些利器，你一定会感到惊讶：以前没有这些命令的时候，自己是什么熬过来的。最钟爱的莫过于 grep, awk, sed, find 命令了！ 本章将会为你介绍一些最有趣同时也是最实用的命令。 用cat进行拼接12345678#cat命令通常用于读取、显示或拼接文件内容，不过它所具备的能力远不止此#cat(concatenate, 拼接)cat file1 file2 ···echo "Ahaha" | cat - file1 file2 #-指stdin文本文件名cat -s file3 -- cat file3 | tr -s '\n' #压缩空白行cat -T test.py #将制表符显示为 ^I, 避免制表符和连续空格误用, 产生错误缩进cat -n file4 #显示行号 录制与回放终端会话(script)当你需要准备一个命令行教程时，如果将我们输入命令后的一切按照先后次序记录下来，再进行回放，是不是很nice！通过 script, scriptreplay 命令, 把终端会话记录到文件，并回放。 123456789#-t,将时间数据输出到标准错误； -a,追加输出script -t 2&gt; timing.log -a output.session #两个文件随意取名, 如不将错误重定向会显示在屏幕上导致很乱输入命令cmd2···exit #退出录制scriptreplay -t timing.log output.session #播放 文件查找与文件列表(find)find 是Unix/Linux命令行工具箱中最棒的工具之一。find 命令沿着文件层次结构向下遍历，匹配符合条件的文件，并执行相应的操作。 find - search for files in a directory hierarchy 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#基于文件名及正则表达式搜索find /home/zhang #列出/home/zhang目录及其子目录线所有文件和文件夹find /home/zhang -name "*.txt"find . -name "*.sh" -o -iname "zhang*" #匹配多个find /home/zhang -path "201710*" #-path将文件路径作为一个整体进行匹配find . -regex ".*\(\.txt|\.[0-9]+\)$" #匹配以.txt或数字结尾的文件#使用-maxdepth, -mindepth参数，来限制find的遍历深度#-type, 根据文件类型搜索。 f(普通文件)，d(目录)，b(块设备)，l(符号链接)，s(套接字)等find /home -maxdepth 1 -type f(d) #参数顺序也会影响find的查找效率#根据文件类型搜索find /dev -type b #查看/dev及其子目录下设备文件find / -maxdepth 1 -type l #查找/下链接文件#根据文件时间进行搜索#Unix/Linux文件系统中的每一个文件都有三种时间戳(timestamp),-表示小于，+表示大于#Unix中并没有所谓的 "创建时间" 的概念#访问时间(-atime,以天为单位； -amin,以分钟为单位):用户最近一次访问文件时间；#修改时间(-mtime,以天为单位； -mmin,以分钟为单位):文件最后一次修改时间；#变化时间(-ctime,以天为单位； -cmin,以分钟为单位):文件元数据(如权限，所有权)最后一次变化时间；find /home/zhang -type f -mtime 7 #7天前被修改的普通文件find /home/zhang -type f -amin -10 #搜索10分钟内被修改的普通文件find . -type f -newer file1.txt #找出比file1.txt新的文件#基于文件大小的搜索#b(块，512字节), c(字节), w(字，2字节), k(千字节), M(兆字节), G(吉字节)find . -type -f -size +100k#删除匹配的文件find . -type f -name "*.swp" -delete#基于文件权限和所有权的匹配find . -type f -perm 644find /var/apache -type f -name "*.php" -perm 644 #搜索基于权限的文件find /var -maxdepth 2 -type f -user zhang #搜索基于用户的文件#执行命令或动作#find命令可以借助-exec与其他命令进行结合#&#123;&#125;是一个特殊字符串，将替换为相应文件名find . -type f -perm 764 -user zhang -exec chmod 644 &#123;&#125; \; #将所属用户zhang，权限764的文件权限修改为644find . -type f -mmin +30 -name "*.txt" -exec cp &#123;&#125; &#123;&#125;.old \; #复制最近30内修改的名字为.txt的文件#-exec结合多个命令#我们无法在-exec参数中直接使用多个命令，不过我们可以把多个命令写到一个shellscript中，然后执行-exec ./test.sh &#123;&#125; \;find . -type f -name "*.sh" -mmin -10 -exec sh &#123;&#125; \;#让find跳过特定目录-prune#利用find搭配tar打包#查找7天内的文件并打包#建议使用绝对路径，管道无效，所有要定向到文件find /dir/path/zhang -type -f -mmtime -7 &gt; /dir/path/zhang/zhang.list &amp;&amp; tar -T /dir/path/zhang/zhang.list -czvf /dir/path/zhang123.tar.gz#检查是否正确tar -tf /dir/path/zhang123.tar.gz#不能使用find -exec tar，这样打包以后只有最后一个文件 利用stat命令查看atime, mtime, ctimestat - display file or file system status 12345stat 1.txt#Access:#Modify:#Change: 利用touch命令修改atime, mtime, ctimetouch - change file timestamps 1234#-a change only the access time#-m change only the modification time#-d instead of current time#-t instead of current time 玩转xargsxargs - build and execute command lines from standard input 1234567891011121314151617181920212223242526272829303132#xargs能够处理stdin并将其转换为特定命令的命令行参数#也可以将单行或多行输入文本转换成其他格式(如多行变单行)cmd | xargs#将多行输入转换为单行输出echo -e "1\n2\n3" | xargs #将换行符替换为空格#将单行输入转换成多行输出echo "1 2 3" | xargs -n 1 #每行一个参数echo "hahaZhahaZhahaZhaha" | xargs -n 2 -d Z #-d指定分隔符#读取stdin，将格式化参数传递给命令cat test.txt | xargs -n 1 ./zhang.sh #zhang.sh arg1; zhang.sh arg2... 每次提供一个参数cat test.txt | xargs -n X ./zhang.sh #X为参数个数，一次提供全部参数#指定替换字符串cat test.txt | xargs -I &#123;&#125; ./zhang.sh &#123;&#125;#结合find使用xargsfind . -type f -name "*.txt" -print0 | xargs -0 ls #-print0无换行输出, -0将\0作为输入界定符#统计某文件行数find /path -type f -name "*.c" -print0 | xargs -0 wc -l#结合stdin，运用while和子shellcat file.txt | while read arg; do cat $arg; done == cat file.txt | xargs - &#123;&#125; cat &#123;&#125;cmd0 | (cmd1; cmd2; cmd3) | cmd4 #子shell 用tr进行转换tr - translate or delete characters 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#tr命令经常用来编写优美的单行命令#tr可对来自stdin的字符 进行替换、删除以及压缩echo "AH WONDERFUL" | tr 'A-Z' 'a-z' #转换大小写echo "AH WONDERFUL" | tr 'A-Z' 'a-b' --&gt; ab bbbbbbbbb#tr [option] set1 set2#如果两个字符集长度不相等，那么set2会不断重复其最后一个字符，直到长度与set1相同echo 12345 | tr '0-9' '9876543210' #数字加密echo 87654 | tr '9876543210' '0-9' #数字解密echo 'He is a cool boy, and she is a beautiful girl' | tr 'A-Za-z' 'NOPQRSRUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm' #加密echo 'Ur vf n pbby obl, naq fur' | tr 'NOPQRSRUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm' 'A-Za-z' #解密cat 1.txt | tr '\t' ' ' #将制表符转换为空格#删除字符echo "Hello 530 World" | tr -d '0-9' #-d删除，删除数字Hello Worldecho "Hello 520 World" | tr -d -c '0-9' #-c补集 520#压缩字符，将连续的重复字符压缩为单个字符echo "GNU's not Unix" | tr -s ' ' #-s压缩，压缩空格GNU's not Unixecho -e "1\n2\n3\n4\n5" &gt; sum.txtcat sum.txt | echo $[ $(tr '\n' '+') 0 ] -- echo $[1+2+3+4+5+0]#tr字符类\a 终端鸣响\b 退格\f 换页\n 换行\r 回车\t 水平制表符\v 垂直制表符string1-stringN #从字符1到字符N升序过程中的所有字符[字符*次数][:alnum:] #所有字母和数字[:alpha:] #所有字母[:digit:] #所有数字[:lower:] #所有小写字母[:upper:] #所有大写字母[:graph:] #所有可打印字符，不含空格[:print:] #所有可打印字符，包含空格[:blank:] #所有水平排列的空白字符[:cntrl:] #所有控制字符[:punct:] #所有标点字符[:space:] #所有空白字符[:xdigit:] #所有十六进制数[=字符] #指定字符 校验和 与 核实文件完整性(md5sum)12345678910111213141516171819202122#校验和(checksum)程序从文件中生成校验和密钥，然后利用校验和密钥核实文件的完整性#校验和对于编写备份脚本或系统维护脚本非常重要，因为它们都会涉及通过网络传输文件#通过使用校验和核实，我们就可以识别那些在网络传输过程中出现损坏的文件，并重传，从而确保数据完整性#校验和对于核实数据完整性非常有用#广泛使用的校验和技术有：md5sum, sha1sum#对单个文件进行校验md5sum sum.txt &gt; sum.md5#302c28003d487124d97c242de94da856 sum.txtmd5sum -c sum.md5 #-c检查#sum.txt: 确定#对目录进行校验#对目录计算校验和意味着我们需要对目录中的所有文件以递归的方式进行计算yum install -y md5deepmd5deep -r ./dir &gt; dir.md5 #recursive递归md5sum -c dir.md5#可以将测试dir下某个文件更改一下，校验的时候会报错 排序、单一、重复(sort,uniq)12345678910111213141516171819202122232425262728293031#sort - 对文本文件进行行排序#uniq - 删除排序文件中的重复行echo -e "333\n1" &gt; 1.txt; echo -e "22\n22" &gt; 2.txtsort 1.txt 2.txt -o ./sorted.txt#1#22#22#333cat sortec.txt | uniq#1#22#333sort -n #按数字进行排序sort -r #逆向排序sort -M #按月份排序sort -C #检查是否排序sort -b #忽略空白#依据键或列进行排序sort -k 2 data.txt #依据第二列来排序#uniq要么使用管道，要么使用排过序的文件作文输入uniq -u sorted.txt #只显示唯一的行(即没有重复出现的行)uniq -d sorted.txt #只显示重复的行uniq -s 2 -w 2 sorted.txt #-s忽略前2个字符，-w指定用于比较的最大字符数 临时文件命名、随机数123456#在编写shell脚本时，我们经常需要存储临时文件。最适合存储临时数据的位置是 /tmp#/tmp目录中的内容会在系统重启后被清空filename=$RANDOM #RANDOM返回一个随机数filename2=$$ #当前shell的PIDfilename3=$((date +%F)) #通过日期命令 分割文件和数据(split)123456789101112131415161718192021#某些情况下，需要把文件分割成多个更小的片段dd if=/dev/zero bs=100k count=1 of=./data.file #生成一个大小100k内容全是0的文件split -b 20k data.file #-d指定分割大小#data.file xaa xab xac xad xae,这五个文件都为20k#我测试了一下，几个文件加起来数据没变，几个文件总行数没变#单位有 k, m, G, c(byte), w(word)#-d以数字为后缀， -a指定后缀长度split data.file -b 20k -d -a 2 spt #增加前缀名'spt'#data.file spt00 spt01 spt02 spt03 spt04split -l 10 data.file #-l按行数来分割文件#split只能根据大小或行数分割文件#csplit可以根据文件本身特点进行分割-f #指定分割后文件前缀-n #指定分割后文件后缀数字个数-b #指定后缀格式 根据扩展名切分文件名12345678910111213141516171819202122232425262728#借助%操作符将名称从 “名称.扩展名” 格式中提取出来file="zhang.txt"name1=$&#123;file%.*&#125; #删除位于%右侧的通配符(.*)所匹配的字符串，通配符从右向左进行匹配#zhang#*号通配符，.号#%属于非贪婪匹配(non-greedy),它会匹配通配符最短结果#%%属于贪婪匹配(greedy)，它会匹配符号条件的最长字符串name2=$&#123;file#*.&#125; #删除位于#右侧的通配符(*.)所匹配的字符串，通配符从左向右进行匹配#txt# #属于非贪婪匹配# ##属于贪婪匹配#栗子URL=“www.google.com”echo $&#123;URL%.*&#125; #非贪婪匹配，移除最右边.及其后面内容www.googleecho $&#123;URL%%.*&#125; #贪婪匹配wwwecho $&#123;URL#*.&#125; #非贪婪匹配，移除最左边.及其前面内容google.comecho $&#123;URL##*.&#125; #贪婪匹配com 批量重命名和移动综合运用find、rename、mv命令。 拼写检查与词典操作123456#Linux大多数发行版都含有一份词典文件，另外还有一个被称为aspell的拼写检查命令#words --&gt; /usr/share/dict/linux.wordsgrep "^good" /usr/share/dict/linux.wordsaspell 交互输入自动化1234567891011121314151617181920212223242526272829303132#写一个读取交互式输入脚本vi jiaohu.sh#!/bin/bashread -p "Input a number:" numread -p "Input name:" nameecho "You have enterd number:$num, name:$name"echo -e "1\nzhang" | ./jiaohu.shYou have entered number:1, name:hello#orecho -e "1\nzhang" &gt; input.txt./jiaohu.sh &lt; input.txt#交互式输入自动化#用expect实现自动化yum install -y expectvim auto_expect.sh#!/bin/expectspawn ./jiaohu.sh #spawn指定需要自动化哪一个命令expect "Input a number:" #expect提供需要等待的消息send "1\n" #send是要发送的消息expect "Input name:"send "zhang"expect eof #expect eof指明命令交互结束./auto_expect.sh 以文件之名简介Unix将操作系统中的一切都视为文件。 生成任意大小的文件(dd)由于各种原因，可能需要生成一个包含随机数据的文件。 12345#dd命令会克隆给定的输入内容，然后将一模一样的副本写到输出#如果不指定if，dd会从stdin中读取输入；如果不指定of，dd会输出到stdout#/dev/zero是一个字符设备，它会不断返回0值字节(\0)dd if=/dev/zero of=junk.data bs=1M count=1 文本文件的交集与差集12345678910111213#comm命令用于两个文件之间的比较#交集(intersection),差集(set difference), 求差#comm必须使用排过序的文件作为输入echo -e "1\n2\n3" &gt; A.txt &amp;&amp; echo -e "3\n2\n3" &gt; B.txtsort -n A.txt -o A.txt &amp;&amp; sort -n B.txt -o B.txtcomm A.txt B.txt#输出第一列为A独有，第二列为B独有，第三列为交集comm A.txt B.txt -1 -2#-1从输出中删除第一列，-2删除第二列，-3删除第三列 查找并删除重复文件1234#重复文件指的是那些虽然名字不同但内容却一模一样的文件ls -lS #以文件大小排序，识别大小相等的文件md5sum #接下来计算这些文件的校验和 创建长路径目录1mkdir -p /home/zhang/1/22/333 2&gt;/dev/null 文件权限、所有权和粘滞位123456789101112131415161718192021222324252627282930313233343536373839404142#用户(user)，用户组(group)，其他用户(other)ll ./*#d目录，c字符设备，b块设备，l符号链接，s套接字，p管道，-普通文件#用户还有一个称为setuid(S)的特殊权限，它出现在用户的x位置#setuid权限允许用户以其拥有者的权限来执行可执行文件，即便这个文件是由其他用户运行的-rwSrw-r--#组也拥有一个setgid(S)权限，它出现在组的x位置#它允许以同该目录拥有者所在组相同的有效组权限来运行可执行文件-rwxrwSr--#目录有一个特殊权限，叫做粘滞位(sticky bit)(T或t)，出现在其他用户的x位置#当一个目录设置了粘滞位，只有创建该目录的用户才能删除目录中的文件,即便group和other有w权限-rwxr--rwTchmod u=rwx g=rw o=r file1chmod u+x g-w file2chmod 744 file3chmod a+x . -R #以递归方式设置权限chown user.group . -R #以递归方式设置所有权chmod a+t dir1 #设置粘滞位chmod +s fiel4chown root.root file4chmod +s file4./file4 #每次file4都是以root运行#setuid的使用不是无限制的，它只能应用在Linux ELF格式二进制，而不能用于脚本文件。 创建不可修改文件123456#不可修改(immutable),是保护文件不被修改的安全手段之一。#一旦文件被设置为不可修改，任何用户(包括root)都不能修改，除非将其不可修改属性移除chattr #修改文件在Linux第二扩展文件系统(E2fs)上的特有属性chattr +i file1 #这样就无法删除file1chattr -i file1 批量生成空白文件123456789#touch命令可用来生成空白文件，如果文件存在，则可以用它修改文件的时间戳for name in &#123;1..100&#125;.txt;dotouch $namedonetouch -a/-m #更改文件访问/修改时间touch -d "Thu Oct 31 14:20:13 CST 2017" file1 #指定特定时间戳 查找符号链接及其指向目标1234567#符号链接(软链接)只不过是指向其他文件的指针ln -s /usr/bin /binls -l / | grep "^l"find / -maxdepth 1 -type lreadlink /bin #找出链接目标 列举文件类型统计信息1234#在Unix/Linux系统中，文件类型并不是由文件扩展名决定的file /etc/passwdfile -b /etc/passwd 环回文件与挂载(mount)1234567891011#环回文件系统是指那些在文件中而非物理设备中创建的文件系统dd if=/dev/zero of=loopback.file bs=1G count=1mkfs.ext4 loopback.filemount -o loop loopback.file /mnt/loopback #-o loop来挂载环回文件df -humount /mnt/loopback#将ISO文件作为环回文件挂载mount -o loop linux.iso /mnt/iso 生成ISO文件以及混合ISO12345678#可引导光盘自身具备引导能力，也可以运行操作系统或其他软件。不可引导光盘则做不到这些。cat /dev/cdrom &gt; /dev/sdc #sdc指U盘dd if=/dev/cdrom of=/dev/sdc #将ISO写入usb存储设备mkisofs -V "Label" -o /dev/sdc /dev/cdromcdrecord -v dev=/dev/cdrom image.iso 查找文件差异并进行修补diff - compare files line by line 1234567891011121314#补丁文件(patch file)#diff命令可以生成差异文件diff -u file1 file2 #一体化形式输出diff -u file1 file2 &gt; diff.patchpatch -p1 file1 &lt; diff.patch #得到file2patch -p1 file2 &lt; diff.patch #得到file1patch -R file1 &lt; diff.patch; patch -R file2 &lt; diff.patch #还原#diff也能够以递归的形式作用于目录，它对目录中所有内容生成差异输出diff -Naur dir1 dir2#-N将所有确实文件视为空文件， -a将所有文件视为文本文件#-u生成一体化输出， -r遍历目录下所有文件 head与tail123456head file1; tail file1 #head与tail默认打印10行head -n 5 file1; tail -n 6 file1 #指定行数head -n -5 file1 #打印除了最后5行外所有行tail -n +(5+1) file1 #打印除了开始5行外所有行tail -f /var/log/nginx/access.log #--follow，动态关注文件 只列出目录的其他方法1234ls -d .ls -l . | grep &quot;^d&quot;ls -F . | grep &quot;/$&quot;find . -maxdepth 1 -type d pushd和popd1234567891011121314#在命令行中使用pushd和popd快速定位，pushd和popd以栈的方式运作#当没有鼠标时，复制粘贴就不怎么实用了#pushd和popd可以用于在多个目录之间进行切换而无需复制并粘贴目录路径pushd /home/user1; pushd /home/user2; pushd /home/user3 #将路径添加到栈pushd +2 #切换到/home/user3popd #移除最近添加入栈的目录cd /root; cd /home/usercd - #回到上次的目录cd .. #切换到上一级目录cd ~ #切换到用户主目录 统计文件的行数、单词数、字符数1234567#wc(word count)，是一个统计工具wc -l file1 #统计行数wc -w file1 #统计单词数wc -c file #统计字符数wc -L file #打印最长行长度wc file1 #行、单词、字符数 目录树123456789#tree命令是以图形化的树状结构打印文件和目录,在Linux发行版中默认未安装yum install -y treetree /home/zhangtree /home/zhang -P "*.sh" #只标记出.sh文件tree /home/zhang -I "*.sh" #标记出除.sh文件外所有文件tree /home/zhang -h #显示大小tree /home/zhang -H http://localhost -o tree.html #以html形式输出目录树 让文本飞简介shell脚本可以将sed, awk, grep, cut等这类优美的工具组合在一起，用于解决文本处理相关问题。 正则表达式（RE）正则表达式是一种用于文本匹配的形式小巧、具有高度针对性的编程语言。只依靠通配符技术，能够匹配的文本范围相当有限。 正则表达式基本组成 正则表达式 描述 ^ 行起始标记 $ 行尾标记 . 匹配任意一个字符 [] 匹配包含在[]中的任意一个字符 [^] 匹配出[^]之外任意一个字符 [-] 匹配[]中范围内的任意一个字符 ？ 重复0或1次 + 重复&gt;=1次 * 重复&gt;=0次 () 创建一个用于匹配的子串 {n} 重复n次 {n, } 重复&gt;=n次 {n,m} 重复n到m次 \ 转义字符 竖线l 匹配竖线l两边任意一项 POSIX字符类 POSIX字符类(POSIX character class),是一个形如[:…:]的特殊元序列，它用于匹配特定的字符范围。 正则表达式 描述 [:alnum:] 字母与数字字符 [:alpha:] 字母字符 [:blank:] 空格与制表符 [:digit:] 数字字符 [:lower:] 小写字母 [:upper:] 大写字母 [:punct:] 标点符号 [:space:] 所有空白字符 元字符 元字符(meta character)，是一种Perl风格的正则表达式，只有一部分文本处理工具支持它。 正则表达式 描述 \b 单词边界 \B 非单词边界 \d 单个数字字符 \D 单个非数字字符 \w 单个单词字符(数字，字母和_) \W 单个非单词字符 \s 单个空白字符 \S 单个非空白字符 \n 换行符 \r 回车 123456#匹配一个ipv4地址[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;\.[0-9]&#123;1,3&#125;#匹配一个邮箱地址[\w]+@[\w]\.com 用grep在文件中搜索文本grep命令是Unix中用于文本搜索的工具，它能够接受正则表达式和通配符。 1234567891011121314151617grep "匹配文本/通配符" file1 file2... --color=auto #重点标记匹配grep -E "正则表达式" fileegrep "正则" filegrep -v #反向匹配grep -c #统计匹配行数grep -n #打印出匹配的行号grep -o #唯一匹配grep -l "匹配" file1 file2 #返回匹配的文件名grep -R #递归匹配grep -i #忽略大小写grep -e "匹配1" -e "匹配2" #匹配多个样式grep -f match.txt file1 #从match.txt文件读取匹配grep "匹配" --include=*.&#123;sh,txt&#125; --exclude=*.log --exclude-dir=/home/user -r /home #包括或排除文件-A/-B n #输出匹配 之后/之前 n行-c n #输出匹配 前后 n行 用cut按列切分文件cut是一个将文本按列进行切分的小工具，它也可以指定每列定界符。在cut的术语中，每列都是一个字段。 1234567#制表符'\t' 是cut默认的定界符cut -d' ' -f1 1.txt #-d指定分隔符，-f打印第几个字段cut -f1,2,3 #打印1,2，3列-c字符； -b字节；cut -c 1-5 1.txt #打印1-5字符cut -c -2 1.txt #打印前2个字符cut -c 3- #打印第3个字符到行尾 统计特定文件词频1234#单词解析可以用 关联数组,正则表达式配合sed,awk,grep等工具来完成#关联数组中，将单词作为数组索引，单词次数作为数组值egrep -o "\b[:alpha:]+\b" #匹配单词 sed入门sed是stream editor(流编辑器)的缩写，它是文本处理中非常重要的工具。能够完美地配合正则表达式使用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#sed - stream editor for filtering and transforming text#字符/在sed中最为定界符使用#替换#sed 's/匹配样式/替代字符串/'sed 's/pattern/repalce/' file #替换sed -i 's/pattern/repalce/' file #将替换应用于fileecho "1.txt" &gt; 1.txt &amp;&amp; sed 's/txt/haha' 1.txt #在输出中用haha替换txtsed -i 's/txt/haha/' 1.txt #将1.txt文件中的txt用haha替换掉#-i选项替换原文件echo "hahaha" | sed 's/ha/HA/g' #全部替换echo "hahaha" | sed 's/ha/HA/2g' #指定位置替换，从第2处开替换全局#移除匹配样式的行sed '/pattern/dsed '/^$/d' ##移除空白行#在sed中用&amp;标记已匹配字符串echo "A wonderful goal" | sed 's/\w\+/[&amp;]/g' #\w\+匹配每一个单词#子串匹配标记\1,\2...echo "1st 2nd 3rd" | sed 's/\(\w\+\) \(\w\+\) \(\w\+\)/\2 \1 \3/'2nd 1st 3rd#将\2和\1交换次序，(),+等在sed中要转义，否则要报错#组合多个表达式sed 'expression1; expression2; ...echo "aabbcc" | sed 's/a/A/; s/b/B/; s/c/C/g'AaBbCC#双引号 " " 内的特殊符号（如$等），可以保有原本的特性#单引号 ' ' 内的特殊字符则仅为一般字符（纯文本）#引用text=helloecho 'hello world' | sed "s/$text/HELLO/"HELLO world awk入门awk被设计用于数据流，它可以对列和行进行操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#awk ‘begin&#123;print "start"&#125; pattern &#123;command&#125; end&#123;print "end"&#125;’ fileawk '&#123;sum += $1&#125;; &#123;print sum&#125;'#awk脚本由:begin块、end块和能使用模式(pattern)匹配的通用语句块 组成#3个部分都是可选的#awk也可以从stdin中读取内容cat /etc/passwd | awk -F: '&#123;print $1&#125;' #-F指定界定符#awk中的特殊变量#NR：记录数量(number of records)，对应于当前行号#NF：字段数量(number of fields)，对应于当前行的字段数#$0：执行过程中当前行的文本内容#$1,$2...$NF：第1个/2个.../最后一个 字段的内容echo -e "L1 1\nL2 22\nL3 333" | awk '&#123;print NR NF $0 $1 $2&#125;'# NR NF $0 $1 $2 $NF=最后一个=$2 1 2 L1 1 L1 1 1 2 2 L2 2 L2 2 2 3 2 L3 3 L3 3 3#将外部变量传递给awk#-v选项可将外部值传递给awk# -v var=val --assign=var=valvar='12345'echo | awk -v v1=$var '&#123;print v1&#125;'#多个变量var1=111; var2=222echo | awk '&#123;print v1,v2&#125;' v1=$var1 v2=$var2#变量来自文件而非标准输入awk '&#123;print v1,v2&#125;' v1=$var1 v2=$var2 file#用样式对awk进行过滤处理awk 'NR &lt; 3,NR==4' 1.txt #行号&lt;5的行awk '/linux/' 1.txt #匹配带有linux的行（可用re）awk '!/linux/' 1.txt #!匹配不带linux的行#设置定界符awk -F: '&#123;print $1&#125;' /etc/passwdawk '&#123;FS=":"&#125; &#123;print $1&#125;' /etc/passwdawk '&#123;FS=":"; print $1&#125;' /etc/passwd#从awk中读取命令输出，用getline读取行echo | awk '&#123;"grep root /etc/passwd" | getlin out; print out&#125;'root:x:0:0:root:/root:/bin/bash#在awk中使用循环awk '&#123;for(i=1;i&lt;4;i++) &#123;print $i&#125;&#125;' 2.txt #输出第1,2,3列 对文件中的行、单词、字符进行迭代123456789101112131415161718192021222324252627282930313233#迭代文件中的每一行echo -e "1\n22\n333" | while read line;do echo $line;donegrep "bash" /etc/passwd | while read line;do echo $line;done#1#22#333#迭代一行中的每一个单词echo "1 22 333" | while read line;do for word in $line;do echo $word;done;done#1#22#333#迭代一个单词中的每一个字符echo "abc" | while read line;do for word in $line; do for((i=0;i&lt;$&#123;#word&#125;;i++)); do echo $&#123;word:i:1&#125;; done; done;done#写成一行echo "abc" | while read line; do for word in $line; do for((i=0;i&lt;$&#123;#word&#125;;i++)); do echo $&#123;word:i:1&#125;; done; done; done#a#b#c#$&#123;#word&#125;返回变量word的长度 按列合并文件(paste)可以使用paste命令实现列拼接12345678#paste - merge(整合) lines of filesecho -e "1\n2\n3" &gt; 1.txt &amp;&amp; echo -e "Line1\nLine2\nLine3" &gt; 2.txtpaste 1.txt 2.txt1 Line12 Line23 Line3#默认定界符是制表符，用-d指定paste 1.txt 2.txt -d',' 打印文件或行中的第n个单词或n列12awk -F':' '&#123;print $1,$3&#125;' file1cut -d':' -f 1,3 file1 打印不同行或样式之间的文本123456awk 'NR==1,NR==10' /etc/passwdawk 'NR==1,NR==10' /etc/passwd | awk -F":" '&#123;print $1,$NF&#125;' #打印特定行内的特定列awk '/start_pattern/, /end_pattern/' file #打印start到end之间的内容,可使用reawk '/root/, /zhang/' /etc/passwd #打印root到zhang之间内容awk '/^ro.?t'/, /bash$/' /etc/pass 以逆序形式打印行可以使用awk, tac完成。tac就是反过来的cat。 123#tac - 反转显示文件中的行，行内的内容无法用tac反向排列tac 1.txtawk '&#123;lifo[NR]=$0; lno=NR&#125; END&#123; for(;lno&gt;-1;lno--) &#123;print lifo[lno]&#125;;&#125;' 1.txt 解析文本中的电子邮件和URL从给定的文件中解析出所需要的文本是我们从事文本处理时的一项任务。 grep, egrep, fgrep - print lines matching a pattern 123456789#egrep#匹配一个邮箱地址egrep -o '[a-zA-Z0-9.]+@[0-9a-zA-Z.]+\.[a-zA-Z]&#123;2,4&#125;' emails.txt#匹配一个URL地址egrep -o "http://[a-zA-Z0-9.]+\.[a-zA-Z]&#123;2,3&#125;" urls.txt 打印某个样式之前/之后n行(grep)123grep "zhang" /etc/passwd -A 5 #Atergrep "zhang" /etc/passwd -B 5 #Beforegrep "zhang" /etc/passwd -C 5 #前后五行都打印 在文件中移除包含某个单词的句子只要能写出正确的正则表达式(Regular Expression)，那就手到擒来 1sed 's/[^.]*handsome boy[^.]*\.//g' file.txt #句子以.结束 文本切片与参数操作12345678910111213141516#替换变量内容中的部分文字var="One two three"echo $&#123;var/t/T&#125; #只替换了一个#One Two three#指定字符串起始位置和长度#$&#123;变量:开始部分:长度&#125;$&#123;vari:start:length&#125;echo &#123;var:0:2&#125; #Onecho &#123;var:1:6&#125; #ne two#起始字符的索引是0,将最后一个字符索引记为-1echo $&#123;var:(-1)&#125; #eecho $&#123;var:(-3):3&#125; #ree 一团乱麻？没这回事入门本章会研究一些用于解析网站内容、下载数据、发送数据表单以及网站颇为任务自动化之类的实例。我们可以仅用几行脚本就将很多原本需要通过浏览器交互进行的活动管理自动化。通过命令行工具利用HTTP协议所提供的功能，我们可以用脚本解决大部分Web自动化的问题。 网站下载(wget,curl)使用一些命令行下载工具，从给定的URL中下载文件或网页。 wget是一个用于文件下载的命令行工具，选项多且用法灵活。 123456789101112131415161718192021222324252627282930313233343536373839#Wget - The non-interactive(非交互式) network downloaderwget URL1 URL2...wget http://xxx.com/nginx-1.12.0.tag.gzwget https://xxx/a.rpm http://xxxx/bb.rpm#指定文件名，指定信息输出(wget默认是stdout)wget http://mirrors.aliyun.com/repo/Centos-7.repo -O aliyun.repo -o ./wget.logwget URL -t 5 #-t，重试次数#下载限速wget --limit-rate=10m URL #下载限速wget -Q 100m URL #指定下载配额#端点续传#wget进行的下载在完成前被中断，从断点开始下载wget -c URL#用cURL下载#cURL是一个比wget更强大的高级命令工具#和wget不同，curl并不将下载数据写入文件，而是写入stdout，因此必须重定向到文件#复制或镜像整个网站#wget有一个选项可以使其像爬虫一样以递归方式手机网页上所有URL链接，并逐个下载#这样一来就可以下载一个网站的所有页面wget --mirror URL#-m(--mirror) -N -r -l inf --no-remove-listing 的缩写形式。或 wget -r -N -l DEPTH URL#-r递归下载，-l指定递归深度，-N(timestamp)只获取比本地时间新的文件#访问需要认证的HTTP或FTP页面wget --user "username" --password "pass" URL#如未在命令行内输入密码，则会由网页提示手动输入 以格式化纯文本下载网页(links)网页其实就是包含HTML标记和其他诸如Javascript，CSS等元素的HTML页面。HTML标记是网页的基础，也许需要解析网页来查找特定的内容。 links,是一个基于命令行的Web浏览器 123456789101112#links - lynx-like alternative character mode WWW browser#在命令行中浏览一个网页links www.baidu.com#以ASCII形式下载网页links --dump URL &gt; URL.txt#打开本地html文件links 1.html cURL入门cURL支持包括HTTP、HTTPS、FTP在内的众多协议。它还支持POST、cookie、认证、从指定偏移处下载部分文件、参照页(referer)、用户代理字符串、扩展头部(extra header)、限速、文件大小限制、进度条等特性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#curl - transfer a URL#cURL通常将下载文件输出到stdout，将进度信息输出到stderr#要想避免显示进度信息，可使用--silent#curl可用来下载、发送各种HTTP请求、指定HTTP头部等操作curl URL --silent #输出到stdout#-O写入文件，文件名从URL中解析curl http://www.baidu.com/index.html -O --silent #创建index.html#-o将数据写入指定文件curl URL -o baidu.html --progress #--progress显示进度条links baidu.html#端点续传#和wget不同，cURL包含更高级的下载恢复特性，能够从特定的文件偏移处继续下载#curl可以通过指定一个偏移量来下载部分文件手动：curl URL/file -C offset #偏移量以Byte为单位的整数自动：curl -C -URL #自动续传#用cURL设置参照页字符串, --referer#参照页(referer)是位于HTTP头部中的一个字符串，用来标识用户从哪个页面到达当前页面的#如果用户点击网页A中某个链接，转到了网页B。那么网页B头部的referer会包含网页A的URLcurl --referer Referer_URL target_URLcurl --referer http://www.baidu.com http://jianshu.com#用cURL设置cookie, --cookie#可以用curl来存储HTTP操作过程中使用到的cookie#cookie用key=value形式，指定多个用 分号 分隔curl URL --cookie "user=AAA;name=bbb"curl URL --cookie-jar cookie.txt #将cookie另存为#用cURL设置用户代理字符串, --user-agent#如果不指定代理，一些需要用户代理的网页就无法显示curl URL --user-agent(-A) "Mozilla"#用-H "头部信息"传递多个头部信息curl -H "Host:www.haha.com" -H "Accept-language: en" URL#限定cURL可占用的带宽curl URL --limit-rate 10m#指定最大下载量curl URL --max-filesize 大小(Bytes)#用cURL进行认证，-u username:password指定用户名和密码curl -u user:pass URLcurl -u user URL #手动输入密码#只打印响应头部信息(无数据部分), -Icurl -I URL 从命令行访问163邮箱12curl -u user http://mail.163.com#手动输入密码 制作图片抓取器及下载工具可以用脚本解析图像文件并将图片自动下载下来。 1234567curl -s URL | grep -o "&lt;img src=[^&gt;]*&gt;" | sed 's/&lt;img src=//g; s/&gt;//g' &gt; img.list#匹配图片的URL，可能还需要细化修改#不同的URL可能有不同的规则，根据实际情况取出img的URL#下载图片wget $URL 或 curl -s -O $URL 查找网站中的无效链接(lynx)将查找无效链接的工作自动化，那就比纯手动厉害多了！ 123456789lynx -traversal URL #会将URL中所有链接生成到reject.dat文件中sort -u reject.dat | while read linkdo output=`curl -I $link -s | grep "HTTP/.*OK"` if [[ -z $output ]] then echo $link fidone &lt; links.txt 跟踪网站变更(curl+diff)可以编写一个定期运行的变更跟踪器(change tracker)，一旦发生变更，跟踪器便会发出声音或发送提示信息。在不同时间检索网站，然后利用 diff 命令进行比对。 123curl URL --silent -o `date +%F`.html #第一次curl URL --silent -o `date +%F`.html #第二次diff -u 第一次 第二次 以POST方式发送网页并读取响应POST 和 GET 是HTTP协议中用于发送或检索信息的两种请求类型。在GET请求方式中，利用网页的URL来发送参数(“键-值”)；而POST方式用于提交表单，如提交用户名、密码以及检索登录页面等。 1234curl URL -d “postarg=AABBCC” #-d,http post datacurl URL -d "post1=key1&amp;post2=key2&amp;post3..." #指定多个数据wget URL -post-data "post1=key1" Plan B 简介提取快照和备份数据都是重要的工作，我们可以通过shell脚本来实现备份自动化。归档和压缩对于SA来说同样很重要，有多种压缩格式。加密是一种保护数据的方法，为了减少加密数据的大小，文件在加密前通常需要先归档和压缩。 用tar归档tar命令可以用来归档文件(tar archives tar)。可以将多个文件和文件夹打包为单个文件，同时还能保留所有的文件属性。由tar命令创建的文件通常称为tarball。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#归档文件，-c(create file)tar -cf 1.tar [sources] #-f(specify filename)指定文件名#文件名必须紧跟在-f之后tar -cvf txt.tar *.txt #-v(verbose)详细信息#向已归档文件中添加文件，-rtar -rvf txt.tar *.html#列出归档文件中的内容，-ttar -tf txt.tar #列出归档内容tar -tvf txt.tar #列出内容详细信息#从归档文件中提取文件或文件夹，-x(exact)tar -xf txt.tar #默认提取到当前目录#-C指定提取目录tar -xvf txt.tar -C /dir/path#只提取归档中特定文件tar -xf txt.tar 1.txt 1.html -C /tmp #只会提取1.txt和1.html文件#在tar中使用stdin和stdouttar -cvf - *.text | tar -xvf - -C /tmp#拼接两个归档文件，-Atar -Af txt.tar html.tartar -tvf txt.tat #验证是否成功#添加选项，可以将指定的任意文件加入到归档文件中。如果同名文件已存在，不会覆盖源文件，那么结果就是归档中包含了多个同名文件#通过检查时间戳来更新对党文件中的内容，-u#只有比归档文件中同名文件 更新(newer) 才添加tar -uvf html.tar 1.html#比较归档文件与文件系统中的内容，-dtar -df txt.tar 1.txt 2.txt#从归档文件中删除文件，--deletetar -f txt.tar --delete 1.txt 2.txt#从归档文件中排除部分文件,--excludetar -cf all.tar ./* --exclude="*.html" #排除.html文件tar -cvf txt.tar *.txt --exclude="1.txt"#打印总字节数,--totalstar -cf all.txt ./* --totals#压缩tar归档文件，指定不同压缩格式#-z, .tar.gz#-j, .tar.bz2#--lzma, .tar.lzma,#.tar.lzotar -czvf txt.tar.gzip *.txttar -xzvf txt.tar -C /dir/path#tar后删除原文件tar -czvf txt.tar.gz ./txt --remove-files 用cpio归档cpio是类似于tar的另一种归档格式。它多用于RPM软件包、Linux内核和initramfs文件等。cpio通过stdin获取输入，并将归档写入stdout。 12345678touch file&#123;1..4&#125;echo file1 file2 file3 file4 | cpio -ov file.cpio#-o指定输出，-v打印归档文件列表#-i指定输入，-t列出归档中文件cpio -it &lt; file.cpio 用gunzip或gzip压缩gzip是GNU/Linux下常用压缩格式。gzip,gunzip都可处理gzip压缩文件类型。gzip只能够压缩单个文件，而无法对目录和多个文件进行归档。因此需要先交给tar，然后再用gzip压缩 12345678910111213141516171819202122232425gzip file #file.gz，会覆盖原文件gunzip file.gz #file，也会删除原文件#列出压缩文件的属性信息，-lgzip -l file.gz#指定gzip的压缩级别，--fast或--best--fast 最低压缩比，最快速度完成--best 最高压缩比，最慢速度完成#将gzip与归档文件结合，-ztar -czvf txt.tar.gzip ./*.txt#-a指定从文件扩展名自动判断压缩格式tar -cavf txt.tar.gzip ./*.txt#tar只能从命令行中接收有限个文件，要解决这个问题，可以写一个循环并添加-r选项#解压缩，-xtar -xzvf txt.tar.gziptar -xavf txt.tar.gzip -C /dir/path 用bunzip或bzip压缩bzip2通常能够生成比gzip更小(压缩比更高)的文件。 1234567891011121314151617181920212223bzip2 file #file.bz2,同理会覆盖原文件bzip2 file -k #保留原文件bunzip2 file.bz2 #解压缩bunzip file.bz2 -k#从stdin读入并写到stdoutcat file | bzip2 -c &gt; file.bz2#将bzip2与归档文件结合，-jtar -cvjf 1.tar.bz2 ./1.*tar -cavf 1.tar.bz2 ./1.* #-a根据文件扩展名自动判断压缩格式tar -xjvf 1.tar.bz2tar -xavf 1.tar.bz2 -C /tmp#压缩比#从1级(速度最快，压缩率最低)到9级bzip -9 -k file#对成千上万的文件进行归档，需要借助 循环和-r选项 lzma压缩lzma是一个较新的压缩工具，它提供了比gzip或bzip2更好的压缩率。xz, unxz, xzcat, lzma, unlzma, lzcat - Compress or decompress .xz and .lzma files 1234567891011121314151617lzma file #file.lzma,同样也会删除原文件lzma file -k #保留原文件unlzma file.lzma#从stdin读入并写入stdoutcat file | lzma -C &gt; file.lzma#与tar相结合,--lzmatar -cvf 1.tar.lzma ./1.* --lzmatar -cavf 1.tat.lzma ./1.* #自动判断tar -xvf 1.tar.lzma --lzmatar -xavf 1.tar.lzma -C /tmp#压缩率#从1级到9级(压缩级别最高，速度最慢)#对成千上万的文件，需要使用循环和-r选项 zip归档和压缩zip在Linux下不如gzip,bzip2那么广泛，但在Internet上的文件通常都采用这种格式。zip - package and compress (archive) files 12345678910111213141516171819zip file.zip fileunzip file.zip#与lzma,gzip,bzip2相比，zip完成后不会删除原文件#对目录和文件进行递归操作,-rzip -r dir.zip /root/test ./file#向归档文件中增加内容，-uzip dir.zip -u newfile#从压缩文件中删除内容，-dzip -d dir.zip file#列出归档文件中内容unzip -l dir.zip 超高压缩率的squashfs文件系统squashfs是一种只读型的超高压缩率文件系统。这种文件系统能够将 2GB-3GB的数据压缩成一个700MB的文件。你有没有想过Linux Live CD是怎样运行的？当Live CD启动后，它会加载一个完整的Linux环境。这就是利用了一种被称为squashfs的只读型压缩文件系统。它将根文件系统保存在一个压缩过的文件系统文件中。这个文件可以使用环回的形式来挂载并对其中的文件进行访问。一次当进程需要某些文件，可以将它们解压，然后载入内存中使用。如果需要构建一个定制的Live OS，或是需要超高压缩率的文件并且无需解压就可以访问文件，那么squashfs的相关知识就能派上用场。要解压个头较大的压缩文件，需要花费不少时间。但如果将文件以环回形式挂载，速度就飞快，因为只有出现访问请求的时候，对应的那部分压缩文件才会被解压缩。而普通的解压缩方式是首先解压缩所有的数据。 环回文件系统就是指那些在文件中而非物理设备中创建的文件系统。比如我们可以创建一个文件，然后把这个文件格式化为我们常见ntfs、exfat或者ext4等文件系统格式，然后把它挂载在一个目录上使用。 如果你有一张Ubuntu CD，可以在CDRom Root/casper/filesystem.squashfs中找到文件.squashfs。squashfs在内部采用了gzip和lzma这类压缩算法。 mksquashfs - tool to create and append to squashfs filesystems 1234567891011121314151617181920yum install squashfs-tools -y#创建squashfs文件mksquashfs source compressfile.squashfsmksquashfs /etc etc.squashfs#/etc(67M) --&gt; etc.suqashfs(18M)#要挂载squashfs文件，利用环回形式进行挂载mkdir /mnt/squashmount -o loop etc.squashfs /mnt/squash#此处挂载使用etc.squashfs文件系统#如果直接查看etc.squashfs，就是一个普通文件，但是挂载以后所有文件都出现了umount /mnt/squash#在创建squashfs文件时排除指定文件，-emksquashfs /etc etc.squashfs -e /etc/passwd /etc/shadow /etc/*.txt#在挂载之后就没有相关文件了 加密工具与散列加密技术主要用于防止数据遭受未经授权的访问。Linux下某些工具用于执行加密和解密，使用加密算法散列值来验证数据完整性。 crypt, gpg, base64, md5sum, sha1sum, openssl的用法 ccyptccrypt是为了取代UNIX crypt而设计的，这个实用工具可用于文件和数据流加密及解密。 ccrypt - encrypt and decrypt files and streams 12345678910ccrypt 1.txt #会要求输入口令(encryption key)#之后会生成1.txt.cpt覆盖原文件#更改key,-xccrypt -x 1.txt.cpt #输入old key和new key#解密，-d(--decrypt)ccrypt -d 1.txt.cpt #输入key解密 gpggpg(GNU privacy guard,GNU隐私保护)，是一种应用广泛的加密方案。它采用签名密钥技术保护文件内容，只有经过认证的用户才能访问数据。我们对gpg签名早已耳熟能详。 gpg - OpenPGP encryption and signing tool 12345#加密，-c(--symmetric)对称加密gpg -c file #会要求输入口令(Passphrase)，生成file.gpg#解密gpg file.gpg base64base64是一组类似的编码方案(encoding scheme)，它通过将ASCII字符转换成以64为基数的形式(radix-64 representation)来用ASCII字符串描述二进制数据。base64可用来对 编码和解码 base64字符串。 base64 - base64 encode/decode data and print to standard output 123456#将文件编码为base64格式base64 file &gt; outputfilecat file | base64 &gt; outputfile#解码,-dbase64 -d outputfile &gt; file md5sum与sha1summd5sum 和 sha1sum 都是单向散列算法(unidirecrional hash algorithm)，均无法逆推出原始数据。它们通常用于验证数据完整性或为特定数据生成唯一的密钥，因为通过分析文件内容，它们可以为每个文件生成一个唯一的密钥。 这种类型的散列算法是存储密码的理想方案。密码使用其对应的散列值来存储。如果某个用户需要认证，读取该用户提供的密码并转换成散列值，然后将其与之前存储的散列值进行比对。将密码以明文的形式存储是非常危险的事情，它面临密码泄露的危险。而因为 md5sum和sha1sum 是单向散列算法，所以密码使用散列值存储是很安全的。 123456789101112echo "1.txt" &gt; 1.txtmd5sum 1.txt #生成密钥到stdout#39061daa34ca3de20df03a88c52530ea 1.txtsha1sum file #生成密钥到stdout#659fcbc505db207c03b5c4c0b6981d63286abe21 1.txt#查看/etc/shadow中密码的散列值awk 'NR==1' /etc/shadow | awk -F: '&#123;print $2&#125;' #root密码散列#$6$BxpV48gPsjuq6.pF$wE7pUDwtOI.v64kd5folG68yUt2UAQDTUGgKa5Iz69GaupEoRAdCeerP8nRKXo48c4azutUCGhnDgzd1qe8YX0 shadowlike散列(salted散列)shadow密码通常都是salted密码，所谓SALT就是额外的一个字符串，用来起一个混淆的作用，使加密更加不同里被破解。salt由一些随机位组成，被用作密钥生成函数的输入之一，以生成密码的salted散列值。 12345678910111213#/etc/passwd里面的密码散列类型就是salted散列#查看root密码对应的散列值head -1 /etc/shadowroot:$6$ZlHRCZG2iRwQUXAu$RAEDH97nPdZB2RK20npua6Qf6jB7osatoC99ow3LtPQ6aORdLISYC7/4iTYU162emkQLt4ZafdgjyAeoSB7IU0::0:99999:7:::#openssl - OpenSSL command line tool#shadow密码是使用openssl生成#将SALT_STRING替换为随机字符串，同时将pass替换成你想测试的密码openssl -1 -salt SALT_STRING passwd 用rsync备份系统rsync借助差异计算以及压缩技术来最小化数据传输量。相较于cp命令，它的优势在于使用了高效的差异算法(difference algorithm)。它还支持网络数据传输。在进行复制的同时，rsync会比较源端和目的端的文件，只有当文件有更新是才进行复制。默认情况下，rsync并不会在目的端删除源端已不存在的文件。 rsync - a fast, versatile, remote (and local) file-copying toolinotifywait - wait for changes to files using inotify 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#-a进行归档，-v详细信息rsync -av source destinationrsync -av /etc /tmp#异地cprsync -av source username@host:PATHrsync -av username@host:PATH destination#rsync借助于ssh，可以使用ssh无秘钥认证rsync -av /etc zhang@192.168.1.11:~#-z, --compress compress file data during the transferrsync -avz zhang@192.168.1.11:/etc /tmp#注意，路径格式rsync /etc /tmp #整个/etc目录rsync /etc/ /tmp #/etc目录下所有内容#显示进度，--progressrsync -avz --progress /etc /tmp#排除部分文件，--excludersync -avz /etc /tmp --exclude=/etc/nginx --exclude "*.txt"#更新rsync时，删除不存在的文件，--delete#默认情况下，rsync并不会在目的端删除源端已不存在的文件rsync -avz /etc zhang@192.168.1.1:~ --delete#定期调度crontab -e0 */10 * * * rsync -avz /etc user@host:PATH#实时同步，inotifywait+rsyncyum install inotify-tools -y#-m(monitor),-r(recursive),-q(--quiet)静默模式，-e(event)vi inotify_rsync.shinotifywait -mrq -e creat,delete,modify,move --exclude "^.*\.filepart$" /etc | while read filedorsync -az --exclude=".*" --exclude="*.swp" --exclude=".filepart" --delete /etc /tmp &gt; /dev/null 2&gt;$1done 用Git备份版本控制维护和恢复变更最好的方法是使用版本控制系统。由于代码变更频繁，版本控制系统多用于软件开发和代码维护。Git(GNU it)是有名气也是最高效的版本控制系统。我们可在非编程环境下用Git备份普通文件。 git - the stupid content tracker 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354mkdir /home/zhang/gittestcd /home/zhang/gittest#在源主机中添加用户信息git config --global user.name "username" #设置用户名git config --global user.email "someone@example.com" #设置邮箱#创建一个空的Git版本库或初始化一个老版本git init#记录变更到版本库git commit#添加远程git目录并同步备份git remote add origin user@host:/home/zhang/gittest#为git跟踪(git tracking)添加或删除文件#add,添加内容至索引git add *#git add *.txt; git add *.ph #添加部分文件#删除不需要跟踪的文件和文件夹#rm,从工作去和索引删除文件git rm file#git rm *.txt#检查点或创建备份点(check point)git commit -m "Commit Message"#push,更新远程git push#用Git恢复数据#log,显示提交日志git log#返回之前某个版本或状态git checkout xxxxxxxx(Commit ID)#clone,克隆一个版本库到本地git clone URLgit clone user@host:PATH 用dd克隆磁盘dd命令能用于克隆任何类型的磁盘，如硬盘、闪存、CD、DVD及软盘。可能需要创建所有分区的副本而不仅仅是复制内容，包括硬盘分区、引导记录、分区表等信息。 使用dd的时候，要留意参数的顺序。错误的参数会损毁全部数据。dd基本上算是一个比特流复制器(bitstream duplicator),它可以将来自磁盘的比特流写入文件，也可以将来自文件的比特流写入硬盘。 dd - convert and copy a file 123456789101112dd if=source of=target bs=block_size count=count#bs块大小，count块数dd if=/tmp/centos7.iso of=/dev/sdc#/dev/zero是一个字符设备，它总是返回字符'\0'dd if=/dev/zero of=./file bs=10m count=100#用环回(loop back)方法可将任何由dd生产的文件镜像进行挂载mount -o loop file /mnt 无网不利简介网络是计算机系统中重要的部分。我们以Tcp/Ip为协议栈，所有操作都是基于它进行的。 一些使用网络的应用通过打开并连接到防火墙端口进行运作，而有的管理任务可以通过网络进行。 网络小知识网络接口(Interface)用来连接网络。在每个系统中，默认都有一个称之为环回接口的lo，这个接口指向当前主机本身。操作系统维护者一个被称为路由表(routing table)的表格，它包含了分组如何转发以及通过网络中的哪些节点转发的消息。metric是路由算法用以确定到达目的地的最佳路径的计量标准，如路径长度。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#显示网络接口、子网掩码等详细信息ifconfig #/sbin/ifconfig#显示某个特定接口ifconfig eth0#提取IP地址ifconfig eth0 | egrep -o "inet [^ ]*" | grep -o "[0-9.]*"#设置网络接口的IP地址和子网掩码ifconfig eht0 192.168.1.11ifconfig eth0 192.168.1.11 netmask 255.255.255.0#远程的时候，千万别乱改IP，不然连不上你就要去机房了#MAC地址欺骗ifoconfig eth0 hw ether 11:22:33:44:55:66#域名服务器与DNScat /etc/resolv.conf#添加域名服务器echo "name 114.114.114.114" &gt;&gt; /etc/resolv.conf#nameserver 114.114.114.114#一个域名可以分配多个地址，DNS只会返回其中一个#要想获得域名所有IP地址，需要使用DNS查找工具#DNS查找工具host www.baidu.comnslookup www.baidu.com#自定义解析cat /etc/hostsecho "192.168.1.11 www.zhang.me" &gt;&gt; /etc/hosts#设置默认网关，显示路由表信息#路由表routeroute -n #以数字形式显示地址#设置默认网关route add default gw $ip $interfaceroute add default gw 192.168.1.1 eht0#显示分组途经的所有网关地址traceroute www.baidu.com pingping使用 网际控制报文协议(Internet Control Message Protocol,ICMP)的echo分组。如果分组能够送达且该主机为活动主机，那它就会发送一条回应。一旦主机不可到达，ping返回错误信息”Destination Host Unreachable”。 123456ping 192.168.1.1#往返时间(Round Trip Time,RTT)#发送分组数量ping $URL -c 6 列出网络上所有活动主机当涉及大型局域网时，可能需要检查网络上的其他主机的活动状态。一台非活动主机可能是：没有开机；网络连接有问题；主机禁ping；防火墙问题。 当我们要检测ip时，在一个脚本中，每一次ping都是依次执行。即使所有的ip地址都是彼此独立，由于编写的是顺式程序(sequential program)，ping命令也只能按顺序执行。每次执行一个ping命令。都要经历一段延迟——“发送echo分组，并接收或等待回应超时”。 要是处理几百个ip地址的话，这个延时就真不短了。我们可以使用并行方式来加速所有ping命令的执行。可以将ping命令中的循环体放入( )&amp; 中，( ) 使其中的命令可作为子shell来执行，&amp; 使之在后台继续运行。 1234567891011121314151617181920#编写G一个并行方式的ping脚本fo ip in 192.168.1.&#123;1..255&#125;do ( ping $ip -c2 &amp;&gt; /dev/null; if[ $? -eq 0 ] then echo "$ip is alive" fi )&amp;waitdone#wait命令是脚本只有在所有子进程或后台进程全部终止或完成后才能结束#使用fping,-a显示活动主机，-g生成目标列表,-u显示无法到达主机fping -a 192.168.0.0/24 -g 2&gt; /dev/nullfping -a 192.168.0.1 192.168.3.255 -g 2&gt; ./unreach.txt#将unreach主机找出cat unreach.txt | egrep -o "to [0-9.]+$" | grep -o "[0-9.]*" 传输文件有很多不同的方法可以在网络节点上传输文件，常见的协议有FTP, SFTP, RSYNC, SCP。 通过FTP传输文件可使用lftp命令；通过SSH传输文件可使用sftp；RSYNC使用SSH与rsync命令；scp通过SSH进行传输。 文件传输协议(File Transfer Protocol, FTP)，使用21端口。FTP是明文传输，So…需要远程主机上启用了FTP服务器才能使用FTP。 1234567lftp user@ftp-host#输入密码后便可以操作如下命令cd -- lcd(本地)mkdirget filename #下载文件put filename #上传文件quit #退出 SFTP(Secure FTP,安全FTP)，运行在SSH连接之上。利用SSH连接模拟FTP接口。它不需要源端运行FTP服务器，不要运行OpenSSH。SFTP是一个交互式命令，提供了命令提示符。 rsync广泛用于网络文件与系统快照的备份。 SCP(Secure Copy,安全复制)，远程文件复制工具。通过SSH加密通过进行传输。123456789scp SOURCE DESTINATIONscp /path/file user@host:PATHscp usr@host:/dir/file /home/zhang#需要输入密码，可以用SSH无秘钥认证#-r递归复制,-p保持文件权限和模式scp -r /etc user@host:/tmpscp -rp user@host:/var/www /var SSH无秘钥认证特别是在定时任务传输备份文件时，无秘钥认证就很方便了。SSH服务默认在22端口，你可以在配置文件中修改。 具体步骤： 创建SSH密钥(公钥和私钥)； 将客户端公钥上传给需要连接的主机，并写入~/.ssh/authorized_keys文件； 修改相关目录(700)和文件权限(600)； 1234567ssh-keygen -t rsa#后续操作默认即可#生成~/.ssh/id_rsa.pub和id_rsa#写入远程主机ssh user@host "cat &gt;&gt; ~/.ssh/authorized_keys" &lt; ~/.ssh/id_rsa.pub 用SSH在远程主机上运行命令1234567891011121314151617#连接远程主机ssh user@host#非默认端口ssh user@host -p 2211#在远程主机中运行命令ssh user@host 'command'ssh user@host 'cmd1'; 'com2'...ssh user@host 'whoami'#-C压缩功能，当带宽有限时ssh -C user@host 'cmd' 在本地挂载远程驱动器(sshfs)在执行读写数据操作时，通过本地挂载远程主机文件系统。利用SSH和sshfs来实现这一功能。sshfs是FUSE文件系统的一个扩展，FUSE允许其支持的操作系统像使用本地文件系统一样挂载各类数据。sshfs允许将远程文件系统挂载到本地挂载点上。 相当于便捷的NFS，但并不需要搭建NFS服务。 SSHFS - filesystem client based on ssh 1234#挂载远程文件到本地ssh user@host:PATH /mnt/sshfsumout /mnt/sshfs 网络流量和端口分析应用程序在主机上打开端口，然后与远程主机中打开的端口实现通信。出于安全方面的考虑，必须留意系统中打开及关闭的端口。 恶意软件和rootkit可能会利用特定的端口及服务运行在系统之中，从而进行攻击。通过分析开放端口列表以及运行在端口上的服务，我们便可以分析并检查恶意软件，保证主机安全。 了解及使用各种端口分析工具。 lsof - list open fileslsof列出系统中开放端口以及运行在端口上的服务的详细信息; netstat查看开放端口与服务netstat - 显示网络连接，路由表，接口状态，伪装连接，网络链路信息和组播成员组; iftop - display bandwidth usage on an interface by hostiftop - 展示带宽使用情况； ifstat - handy utility to read network interface statisticsifstat - 展示某时刻网络状态； nload - displays the current network usagenload - 可查看系统总带宽； nethogs - Net top tool grouping bandwidth per processnethogs- 可查看每个进程流量情况；ethtool - query or control network driver and hardware settingsethtool - 检查网卡支持的带宽 12345678910111213141516171819202122#lsof的每一项都对应着一个打开了特定端口的服务lsof -i#查看开放端口和服务netstat -nltp#查看网络实时状态iftop#查看当前网络状态ifstat#查看系统带宽nload#查看进程流量nethogs 当个好管家简介操作系统(Operation System,OS)，是由一系列用于不同目的、服务于不同任务的系统软件组成。日志记录(logging)和监视是很重要的，能帮助我们从大量数据中收集信息。 监视系统活动的各种命令，日志技术及其使用方法。 统计磁盘使用情况(df+du+fdisk)磁盘空间是一种有限资源，我们需要了解磁盘的可用空间。 df, du, fdisk是Linux中的磁盘管理三板斧df(disk free): 报告文件系统磁盘空间的使用情况;du(disk usage): 报告磁盘空间使用情况; 使用du时，要确保对其遍历的目录和文件拥有适合的读权限。fdisk: Linux分区表操作工具软件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748du file1 #默认以字节为单位#-a,显示目录下所有文件大小du -a /home/zhangdu /home/zhang #只显示目录大小#-h,以可读形式打印du -h /home/zhang#-c,显示使用总量du -c file1 /dir2du -c *.txt *.sh#-s，打印摘要du -s /dirdu -sh /home/zhang#-b,-k,-m,-B，用特定单位打印du -k file1du -m file2#--exclude,从磁盘统计中排除部分文件du --exclude="*.swap" -sh /home/zhang#--max-depth,指定最大遍历深度du -h --max-depth n /dirdu -h --max-depth=2 /home/zhang#-x,将/mnt中所有挂载点排除在磁盘统计之外du -xh /dir#找出目录中最大的文件du -ak /dir | sort -nrk 1 | head -n 5#此输出包含了目录大小，需要细化#利用find替du过滤文件find /dir -type f --exec du -ak &#123;&#125; \; | sort -nrk 1 | head#df,磁盘可用空间信息df -h 计算命令执行时间当测试一个应用程序或比较不同的算法时，程序的执行时间非常重要。所以需要计算命令执行时间。 所有的Unix-Like操作系统都包含time命令，可将time放在需要计算执行时间的命令前。 time命令有个可执行二进制文件位于/usr/bin/time，还有一个shell built-in命令也叫作time；当运行time时，默认调用的是shell built-in命令。內建time命令选项有限；因此，如果我们需要使用另外的功能，就应该使用/usr/bin/time命令。 123456789101112131415161718192021222324#计算命令执行时间time commandtime ls#real,挂钟时间(wall clock time),命令从开始执行到结束的时间；#user,指进程花费在用户模式(user-mode)中的CPU时间。这是唯一用于执行进程所花费的时间；#sys，指进程花费在内核模式(in the kernel)中的CPU时间。它代表在内核中执行系统调用所使用的时间。#-o,将命令执行时间写入文件/usr/bin/time -o exetime.txt ls /#-a,不影响原文件/usr/bin/time -a -o exetime.txt ls /home#-f,格式化时间输出#时间格式字符串#real %e#user %U#sys %S/usr/bin/time -f "FORMAT STRING" command/usr/bin/time -f "Rtme: %e" -a -o timing.log uname/usr/bin/time -f "Rtime: %e\nUtime: %U\nStime: %S" -ao timing.log uname 当前登录用户、启动日志、启动故障的相关信息(w+who+lastb+last)收集与操作系统、当前登录用户、主机运行时间、启动故障等相关信息很有用处。 1234567891011121314151617181920#获取当前登录用户who #显示已经登录的用户w #显示已经登录的用户以及他们在做什么#会显示用户使用的伪终端(pseudo TTY)，对应设备文件出现在/dev/pts/n#列出登录主机的用户列表users#查看系统运行时间uptime#显示用户登录列表last#获取某个用户登录信息last zhang#获取重启会话信息last reboot#获取失败的用户登录信息lastb 打印10条最常使用的命令(history)终端是用来访问shell的工具，在shell中我们可以输入并执行命令。我们可以找出在shell中运行最多的命令。 ~/.bash_history，默认保留1000个最近执行命令。或者history命令。 1cat .bash_history | sort -n | uniq -c | sorn -nr | head 列出占用CPU最多的进程CPU时间是一项重要资源，有时需要跟踪占用CPU周期最多的进程。对于需要处理大量请求的服务器来说，CPU是极其重要的资源。通过监视某个时期内CPU的使用情况，可以找出长期占用CPU的进程并对其进行优化，或是调试其他问题。 用ps命令收集系统中进程的详细信息。ps - report a snapshot of the current processes 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#-e,以标准语法显示每个进程ps -eps -ef#ax,以BSD语法显示每个进程ps axpa axu#获取安全信息#ps -eo euser,ruser,suser,fuser,f,comm,pcpu,label#comm显示命令，pcpu显示CPU使用率ps -eo comm,pcpu#监视并计算一小时内CPU使用情况的shell脚本secs=3600unit_time=60steps=$(($secs / $unit_time))echo "Whatching CPU usage..."for((i=0; i&lt;steps; i++))do ps -eo comm,pcpu | tail -n +2 &gt;&gt; /tmp/cpu_usage.$$ sleep $unit_timedoneecho "CPU eaters: "cat /tmp/cpu_usage.$$ | \awk '&#123;process[$1]+=$2&#125;END&#123; for (i in process) &#123; printf("%-20s %s",i,process[i]); &#125;&#125;' | sort -nrk 2 | head#tail -n +K，从第K行开始输出。上面输出第一行是 COMAND 和 %CPU#$1,command; $2,%CPU#process[$1]是一个关联函数，相当于arr[command]#arr[command]=arr[command]+ $2，计算同一命令的累积时间#i指命令，process[i]指命令运行时间 用watch监视命令输出可能需要在在某段时期内以固定的间隔时间不短监视某个命令的输出。可利用watch命令。 watch - execute a program periodically, showing output fullscreen 123456789101112131415#watch命令可以用来在终端以固定的间隔监视命令输出，默认2秒间隔watch commandwatch 'command'watch lswatch 'ls -l'#-n,指定时间间隔watch -n 5 'yum update -y'#-d，突出(highlighting)watch输出中的差异watch -d -n 1'dd if=/dev/zero of=/tmp/zero.test' 对文件及目录访问进行记录(inotifywait)记录重要文件及目录访问，对于追踪文件和目录的变化很有帮助。inotifywait命令可以用来收集有关文件访问的信息。inotifywait和rsync用户实时同步哦！ inotifywait - wait for changes to files using inotify 1234567891011yum install -y inotify-tools#-q,减少冗余信息inotifywait -m -r -q -e create,move,delete /dirinotifywait -m -r -q -e create,move,modify,delete /home/zhang &gt;&gt; inotifywait.log#利用inotifywait检测，rsync同步inotifywait -mrq -e create,move,modify,delete /dir --exclude="*.swap" | while read filedorsync -av --exclude="*.swqp" --delete /dir user@host:PATH &gt; /dev/null 2&gt;&amp;1done 用logrotate管理日志文件日志文件是Linux系统维护中必不可少的组成部分。日志文件可以帮助跟踪系统中多种服务所发生的事件，这有助于排除系统问题。但随着时间推移，日志文件会变得越来越大。因而必须对日志文件进行管理。 我们可以利用一种称为“轮询(rotation)”的技术来限制日志文件的体积。一旦日志文件超过了限定大小，就要对它的内容进行抽取(strip)，同时将日志文件的旧条目归档到文件中。 logratate是每一位Linux系统管理员都应该了解的命令。它能够将日志文件大大小限制在给定的SIZE内。logrotate配置文件位于/etc/logrotate.d logrotate ‐ rotates, compresses, and mails system logs 123456789101112vim /etc/logrotated.d/custom/var/log/custom.log &#123; missingok #日志文件丢失，则忽略 notifempty #仅当源日志文件非空时才进行轮替 size 30k #限制实施轮替的日志文件大小 compress #压缩旧日志 weekly #轮询时间，daily,weekly,yearly rotate 7 #保留旧日志数量 create 0600 root root #创建的日志文件模式，用户和用户组#还有一些其他选项&#125; 用sys记录日志在Linux系统中，在/var/log中创建并写入日志信息的是由被称为syslog的协议处理的。它由守护进程syslogd负责执行。每一个标准应用进程都可以用syslog记录日志信息。 syslog处理/var/log下的多个日志文件。但是当logger发送消息时，它用标记字符串来确定应该纪录到哪一个日志文件中。syslogd使用与日志相关联的TAG来决定应该将其记录到哪一个文件中。可以从/etc/rsyslog.d/目录的配置文件中看到与日志文件相关联的标记字符串。 Linux中一些重要日志文件： /var/log/boot.log， 系统启动信息；/var/log/message， 内核启动信息；/var/log/auth.log， 用户认证日志；/var/log/dmesg， 系统启动信息；/var/log/mail.log， 邮件服务器日志。 logger - a shell command interface to the syslog 123456#logger命令，默认记录日志信息到/var/log/messageslogger "test log message to messages"tail -n 1 /var/log/message#-t，指定特定TAGlogger -t TAG "test log message to messages" 管理重任简介GNU/Linux的生态系统是由运行的程序、服务、连接的设备、文件系统、用户等组成。按照我们需要的方式对整个系统有一个微观并对操作系统进行整体上的管理，这就是系统管理的主要目的。 收集进程信息(top+ps+pgrep)进程是程序运行实例(runing instance)。同一程序的多个实例可以同时运行，但他们的进程ID却互不相同。 进程管理相关的重要命令是： top, display Linux processes; ps, report a snapshot of the current processes; pgrep, look up or signal processes based on name and other attributes. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#ps命令#-f, 显示更多进程信息ps -f#-e,every; -a,allps -efps -ax#-o, 指定想要的列ps -e -o parameter1,parameter2...ps -eo comm,pcpu,pmem#pccpu CPU占用率#pid 进程ID#ppid 父进程ID#pmem 内存使用率#comm 命令名#cmd 简单命令#user 启动进程的用户#nice 优先级#time 累积的CPU时间#etime 进程启动后度过的时间#tty 所关联的TTY设备#euid 有效用户ID#stat 进程状态#--sort,根据参数对ps输出进行排序#+升序，-降序ps -eo comm,pcpu,pmem --sort -pcpups -eo comm,pcpu,pmem --sort -pcpu,+pmem#-C, 给定命令全名找出PIDps -C cmd -o comm,pid#-u, 指定有效用户列表#-U, 指定真实用户列表ps -u root -U zhang -o user,pcpu#-t, 用TTY过滤输出ps -t TTY1,TTY2...ps -t pts/0,pts/1 -ef#-L, 显示进程相关信息#LWP线程ID， NLWP线程数量ps -efL#pgrep命令, 获得一个特定命令的PID列表#它只需要命令的一部分即可pgrep cmdpgre inotifpgrep bas#-d, 指定定界符pgrep rsync -d ":"#-u, 指定进程的用户pgrep -u root,zhang rsync#-c, 返回匹配的进程数量pgrep -c rsync#top命令top 杀死进程以及发送响应信息(kill+killall+trap)在Unix-Like环境中与进程有关的一个重要概念就是信号。信号是一种进程间通信机制，它用来中断运行的进程以执行某些操作。终止程序也是通过使用信号技术来实现的。 像ctrl+C,ctrl+Z这种作业都属于信号。 kill 命令可用来向进程发送信号; trap 命令用来处理所接收的信号; killall 以名字方式来杀死进程. 12345678910111213141516171819202122232425#列出所有可用信号kill -l#-s, 发送信号#信号名称和信号数都可以kill -信号数 PIDkill -s SIGNAL PID#常用信号#SIGHUP 1 --对控制进程或终端进行挂起检测(hangup detection);#SIGINT 2 --当按下ctrl+c时发送该信号;#SIGKILL 9 --强行杀死进程;#SIGTERM 15 --终止进程;#SIGTSTP 20 --当按下crtl+z时发送该信号.#killall, 通过命令名终止进程killall -s SIGNAL PNamekillall -信号数 PName#trap, 捕捉并响应信号trap 'signal-handler-func' SIGNAL LIST which, whereis, file, whatis与平均负载which hows the full path of (shell) commands。找出某个命令的位置;whereis locate the binary, source, and manual page files for a command。不仅返回命令路径，还能打印命令手册的位置以及命令源代码路径;file determine file type。用来确定文件类型;whatis display manual page descriptions。输出简短描述信息;平均负载(load average),是系统运行总负载量的一个重要参数。它指明了系统中可运行进程总量的平均值。平均负载由三个值来指定，第一个指明1分钟内的平均值，第二个指明5分钟内的平均值，第三个指明15分钟内的平均值。 单核CPU，类似于单车道，负载在 0.00-1.00 之间正常； 多核CPU，类似于多车道，负载在 核数*(0.00-1.00) 之间正常； 安全的系统负载，单核应该在 0.7 以下； 12345#查看平均负载uptimecat /proc/loadavg#0.00 0.01 0.05 1/355 44955#分母355表示系统进程总数, 分子表示正在运行的进程数, 最后一个数字表示最近运行进程ID 向用户终端发送消息系统管理员可能需要向网络中所有主机上的所有用户或特定用户的终端发送消息。`wallrsync -av –exclude=”*.s命令用来向所有当前登录用户的终端写入消息。 在Linux系统中，终端是作为设备存在的。因此那些打开的终端在dev/pts/中都会与对应的设备节点文件。向特定设备写入数据将会在对应的终端显示出消息。 12345echo "It's just a test" | wall#查看用户对应的/dev/pts/, 并向某一个用户终端发送信息ll /dev/pts | awk '&#123;print $3,$6&#125;'echo"Haha" &gt; /dev/pts/[1,2,3...] 收集系统信息包括主机名、内核版本、Linux发行版本、CPU信息、内存信息、磁盘分区信息等。 123456789101112131415161718192021222324252627#主机名hostnameuname -n#内核版本，架构uname -runame -muname -a#Linux发行版本cat /etc/redhat-release#CPU相关信息lscpucat /proc/cpuinfocat /proc/cpuinfo | grep 'model name'#内存详细信息free -hcat /proc/meminfo#分区信息cat /proc/partitionsfdisk -l#系统详细信息lshw 用/proc收集信息在GNU/Linux操作系统中，/proc是一个位于内存中的伪文件系统(in-memory pseudo filesystem)。它的引用是为了提供一个可以从用户空间(user space)读取系统参数的接口。 可以对/proc中的文件和子目录进行cat来获取信息，所有内容都是易读的格式化文本。 /proc/下的数字目录，包含了对应进程的相关信息；/proc/environ，包含于进程相关联的环境变量；/proc/cwd，是一个到进程工作目录的符号链接；/proc/fbcat，包含了由进程所使用的文件描述符。 用cron进行调度GNU/Linux系统包含了各种用于调度任务的工具。cron就是其中之一，它通过守护进程crond使得任务能够以固定的时间间隔在系统后台自动运行。cron利用的是一个被称为“cron表(cron table)”的文件，这个文件中存储了需要执行的脚本或命令的调度列表以及执行时间。 12345678910111213141516171819202122232425262728#分 时 日 月 周#* * * * * cmd#分钟(0-59)#小时(0-23)#天(1-31)#月(1-12)#工作日(0-7)，0和7都代表周天#命令#*号,所有值#,号,范围。1,3,5,7,9#-号,连续范文。1-10#/号,*/10;0-8/20#栗子crontab -e* 0-6 * * * /home/zhang/test.sh1,3,5,7,9 * * * * /home/zhang/test.sh*/5 * * * * /home/zhang/test.sh#-l,查看cron表crontab -l#-r,移除cron表crontab -r cron的高级写法栗子： 1234567891011121314151617181920@reboot #在启动的时候运行一次#其实@reboot类似于rc.local，开机启动@yearly == @annually == 0 0 1 1 * #一年一次@monthly == 0 0 1 * * #每月一次@weekly == 0 0 * * 0 #每周一次@daily == @midnight == 0 0 * * * #每天一次@hourly == 0 * * * * #每小时一次crontab -e@reboot /bin/mongod -f /etc/mongod_27018.confvim /etc/rc.d/rc.local/bin/mongod -f /etc/mongod_27018.confchmod a+x /etc/rc.d/rc.local 用户管理常用命令123456789101112131415161718192021222324252627282930313233343536#添加用户useradd#删除用户userdel--remove-all-file删除与用户相关的所有文件#修改shellchsh#修改用户属性usermod#修改密码过期时间chage#修改密码passwd#登录到一个新组newgrp#添加、删除组groupaddgroupdel#指纹finger]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作与人生]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%B7%A5%E4%BD%9C%E4%B8%8E%E4%BA%BA%E7%94%9F%2F</url>
    <content type="text"><![CDATA[我现在已经活到了人生的中途，拿一日来比喻人的一生，现在正是中午。人在童年时从朦胧中醒来．需要一些时间来克服清晨的软弱，然后就要投入工作；在正午时分，他的精力最为充沛，但已隐隐感到疲惫；到了黄昏时节，就要总结一日的工作，准备沉入永恒的休息。按我这种说法，工作是人一生的主题。这个想法不是人人都能同意的。我知道在中国，农村的人把生儿育女看作是一生的主题。把儿女养大，自己就死掉，给他们空出地方来——这是很流行的想法。在城市里则另有一种想法，但不知是不是很流行：它把取得社会地位看作一生的主题。站在北京八宝山的骨灰墙前，可以体会到这种想法。我在那里看到一位已故的大叔墓上写着：副系主任、支部副书记、副教授、某某教研室副主任，等等。假如能把这些“副”字去掉个把，对这位大叔当然更好一些，但这些“副”字最能证明有这样一种想法。顺便说一句，我到美国的公墓里看过，发现他们的墓碑上只写两件事：一是生卒年月。二是某年至某年服兵役；这就是说，他们以为人的一生只有这两件事值得记述：这位上帝的子民曾经来到尘世，以及这位公民曾去为国尽忠，写别的都是多余的，我觉得这种想法比较质朴……恐怕在一份青年刊物上写这些墓前的景物是太过伤感，还是及早回到正题上来罢。 我想要把自己对人生的看法推荐给青年朋友们：人从工作中可以得到乐趣，这是一种巨大的好处。相比之下，从金钱、权力、生育子女方面可以得到的快乐，总要受到制约。举例来说，现在把生育作为生活的主题，首先是不合时宜；其次，人在生育力方面比兔子大为不如，更不要说和黄花鱼相比较；在这方面很难取得无穷无尽的成就。我对权力没有兴趣，对钱有一些兴趣，但也不愿为它去受罪——做我想做的事(这件事对我来说，就是写小说)，并且把它做好，这就是我的目标。我想，和我志趣相投的人总不会是一个都没有。 根据我的经验，人在年轻时，最头疼的一件事就是决定自己这一生要做什么。在这方面，我倒没有什么具体的建议：干什么都可以，但最好不要写小说，这是和我抢饭碗。当然，假如你执意要写，我也没理由反对。总而言之，干什么都是好的；但要干出个样子来，这才是人的价值和尊严所在。人在工作时，不单要用到手、腿和腰，还要用脑子和自己的心胸。我总觉得国人对这后一方面不够重视，这样就会把工作看成是受罪。失掉了快乐最主要的源泉，对生活的态度也会因之变得灰暗…… 人活在世上，不但有身体，还有头脑和心胸——对此请勿从解剖学上理解。人脑是怎样的一种东西，科学还不能说清楚。心胸是怎么回事就更难说清。对我自己来说，心胸是我在生活中想要达到的最低目标。某件事有悖于我的心胸，我就认为它不值得一做；某个人有悖于我的心胸，我就觉得他不值得一交；某种生活有悖于我的心胸，我就会以为它不值得一过。罗素先生曾言，对人来说，不加检点的生活，确实不值得一过。我同意他的意见：不加检点的生活，属于不能接受的生活之一种。人必须过他可以接受的生活，这恰恰是他改变一切的动力。人有了心胸，就可以用它来改变自己的生活。 中国人喜欢接受这样的想法：只要能活着就是好的，活成什么样子无所谓。从一些电影的名字就可以看出来：《活着》、《找乐》……我对这种想法是断然地不赞成。因为抱有这种想法的人就可能活成任何一种糟糕的样子，从而使生活本身失去意义。高尚、清洁、充满乐趣的生活是好的，人们很容易得到共识。卑下、肮脏、贫乏的生活是不好的，这也能得到共识。但只有这两条远远不够。我以写作为生，我知道某种文章好，也知道某种文章坏。仅知道这两条尚不足以开始写作。还有更加重要的一条，那就是：某种样子的文章对我来说不可取，绝不能让它从我笔下写出来，冠以我的名字登在报刊上。以小喻大，这也是我对生活的态度。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>王小波</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[告别信]]></title>
    <url>%2F2017%2F09%2F04%2F%E5%91%8A%E5%88%AB%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[1999年，72岁的马尔克斯患上淋巴癌后，写了一封信向读者告别。如果有一刹那，上帝忘记我是一只布偶并赋予我片刻生命，我可能不会说出我心中的一切所想，但我必定会思考我所说的一切。 我会评价事物，按其意义大小而非价值多少。 我会少睡觉，多思考。因为我知道，每当我们闭上一分钟眼睛，我们也就同时失去了60秒。当他人停滞时我会前行，当他人入梦时我会清醒，当他人讲话时我会倾听，就像享受一支美味的巧克力冰激凌！ 如果上帝赏我一段生命，我会简单装束，伏在阳光下，袒露的不仅是身体，还有我的魂灵。 上帝呀，如果我有一颗心，我会将仇恨写在冰上，然后期待太阳的升起；我会用凡高的梦在星星上画一首贝内德第的诗，而塞莱特的歌会是将是我献给月亮的小夜曲。我会用泪水浇灌玫瑰，以此体味花刺的痛苦和花瓣的亲吻…… 上帝呀，如果我有一段生命……我不会放过哪怕是一天，而不对我所爱的人说我爱他们。我会使每个男人和女人都了解他们皆我所爱，我要怀着爱而生活。 对于大人，我会向他们证明，那种认为因衰老而失去爱的想法是多么错误，我们是因为失去爱而衰老而不是与之相反。对于孩子，我会给他们插上翅膀而让他们自己学会飞翔；对于老人，我会教给他们死亡的来临不是因为衰老而是因为遗忘。 人呀，我从你们身上学会了太多的东西… …我知道，人们都想伫立在颠峰上，殊不知，真正的幸福恰恰就在于攀登险阻的过程。我懂得，当婴儿用小拳头第一次抓住爸爸的手指时，他也就永远地抓住了它。 我明白，一个人只有在帮助他人站起时才有权利俯视他。我能够从你们身上学到的东西是如此之多，可事实上已经意义寥寥，因为当人们将我敛入棺木时，我正在死去。 永远说你感到的，做你想到的吧！如果我知道今天是我最后一次看你入睡，我会热烈地拥抱你，祈求上帝守护你的灵魂。如果我知道这是最后一次看你离开家门，我会给你一个拥抱一个吻，然后重新叫住你，再度拥抱亲吻。如果我知道这是最后一次听到你的声音，我会录下你的每个字句，以便可以一遍又一遍永无穷尽地倾听。如果我知道这是看到你的最后几分钟，我会说”我爱你”，而不是傻傻地以为你早已知道。 永远有一个明天，生活给我们另一个机会将事情做好，可是如果我搞错了，今天就是我们所剩的全部，我会对你说我多么爱你，我永远不会忘记你。 明天从不向任何人作保证，无论青年或老人，今天可能就是你最后一次看到你所爱的人。因此，别再等待了，今天就开始！因为如果明天永远不来，你也许会遗憾今天没来得及微笑，拥抱，亲吻，会遗憾自己忙碌得只能把它们归为一个最后的愿望。保护周围你爱的人吧，告诉他们你多么需要他们。爱他们，善待他们，用些时间对他们说：”对不起”，”原谅我”，”劳驾”，”谢谢”，以及你知道的所有爱的话语。 没有人会因为你秘而不宣的思想而记住你。向上帝祈求力量和智慧来表达它们吧，向你的朋友证明，他们对你来说是多么的重要。]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>马尔克斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2017%2F09%2F01%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[参考: Markdown-wiki Markdown官网 Markdown中文文档 Markdown语法 果冻虾仁 关于MarkdownMarkdown 是一种轻量级标记语言。它允许人们使用易读易写的纯文本格式编写文档，然后转换成有效的XHTML(或者HTML)文档。 Markdown语法 首行缩进12345678#一个空格&amp;ensp;#两个空格&amp;emsp;#不断行空白格&amp;nbsp; 栗子： &ensp;一个空格； &emsp;两个空格； &nbsp;不断行空白格； 段落与换行 段落的前后必须是空行 空行是指行内什么都没有，或者只有空白符（空格或制表符） 相邻两行文本，如果中间没有空行，会显示在一行中（换行符被转换为空格） 如果需要在段内加入换行 可以在前一行的末尾加入至少两个空格，然后换行写其它的文字 Markdown中的多数区块都需要在两个空行之间 粗体和斜体语法： 1234*斜体*, _斜体_**粗体*****粗斜体***~~删除线~~ 显示效果： 斜体, 斜体 粗体 粗斜体 删除线 分级标题Setext形式大标题： 123456一级大标题========二级大标题-------- atx形式普通标题： 12345# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题 超链接MarkDown支持两种形式的链接语法：行内式和参考式。 行内式语法说明：[ ] 里面写链接文字，( ) 里面写链接地址，()中的” “可以指定title属性。 代码： 欢迎来到 [简书](www.jianshu.com &quot;Jianshu&quot;) 效果： 欢迎来到 简书 参考式参考式超链接一般用在学术论文上面，或某一个链接在文章中多处使用，那么引用的方式创建链接将非常好，它可以让你对链接进行统一的管理。 语法说明： 123参考式链接分为两部分，文中的写法[链接文字][链接标记]，在文本任意位置添加[链接标记]:链接地址 “链接标题”，链接地址与链接标题前有一个空格如果链接文字本身可以作为链接标记，也可以写成[链接文字][][链接文字]：链接地址的形式 代码： 123456简书里面有 [简书早报][1]、[简书晚报][2]以及 [简黛玉][3][简黛玉 美人][3] 是一个[才女][][1]:http://www.jianshu.com &quot;Jianshu&quot;[2]:http://www.jianshu.com &quot;EveningPaper&quot;[3]:http://www.jianshu.com[才女]:http://www.jianshu.com 效果： 简书里面有 简书早报、简书晚报以及简黛玉简黛玉 美人 是一个才女 自动链接MarkDown支持以比较简短的自动链接形式来处理网址和电子邮件，只要用&lt;&gt;包起来，MarkDown就会自动把它转成链接。 代码： 12&lt;http://example.com&gt;&lt;address@example.com&gt; 锚点MarkDown Extra只支持在标题后插入锚点，其他地方无效。锚点中的标题如果有空格，则锚点无效。现在的锚点支持中文标题。 代码： 12345678910锚点连接页内标题[标题一](#Title1)[标题二](#Title2)[标题三](#标题3)# Title1## Title2### 标题3 列表无序列表使用 * ，+ ，- 表示无序列表 代码： 123- 无序列表1- 无序列表2- 无序列表3 效果： 无序列表1 无序列表2 无序列表3 有序列表有序列表使用数字接着英文点 代码： 1231. 有序列表12. 有序列表23. 有序列表3 效果： 有序列表1 有序列表2 有序列表3 定义型列表定义型列表由名词和解释组成。一行写上定义，紧跟一行写上解释。解释的写法：紧跟一个缩进（Tab） 列表缩进列表项目标记通常是放在最左边，但是其实也可以缩进，最多3个空格，项目标记后则一定要接着至少一个空格或制表符。 代码： 123* 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。* 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 效果： 轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 引用引用需要在被引用的文本前加上&gt;符号 代码： 12&gt; 引用1&gt; 引用2 效果： 引用1引用2 引用的多层嵌套区块引用可以嵌套（如引用的引用），只要根据层次加上不同数量的 &gt;符号 代码： 123&gt;&gt;&gt; 请问MarkDown怎么用？&gt;&gt; 自己看教程！&gt; 教程在哪里？ 效果： 请问MarkDown怎么用？ 自己看教程！ 教程在哪里？ 插入图像图片的创建方式与超链接类似。 代码： ![](http://zhangxx5678.lofter.com/post/39b969_df4f526#) 内容目录在段落中填写 [TOC] 以显示全文内容结构目录 注脚在需要添加注脚的文字后加上注脚名字 [^注脚名字]，称为加注。然后在文中的任意位置（一般最后）添加脚注，脚注前必须有对应的脚注名字。注脚与注脚间必须空一行！注脚自动被搬运到最后面，请到文章末尾查看，并且脚注后的链接可以直接跳转会到加注的地方 代码： 123使用 MarkDown[^1]可以提高书写效率，直接转换成 HTML[^2][^1]:MarkDown是一种纯文本标记语言[^2]:HTML超文本标记语言 效果： 使用 MarkDown^1可以提高书写效率，直接转换成 HTML^2 分割线可以在一行中用 三个以上的 *,-,_ 建立一个分割线，行内不能有其他东西。 代码： 12345671. * * *2.3. ***4.5. - - -6.7. --- 效果： GitHub中的表情Github的Markdown语法支持添加emoji表情，输入不同的符号码（两个冒号包围的字符）可以显示出不同的表情。 比如:blush:,显示效果为 :blush: 每个表情对应的符号码：https://www.webpagefx.com/tools/emoji-cheat-sheet/ 或者，果冻虾仁的整理：https://github.com/guodongxiaren/README/blob/master/emoji.md diff语法版本控制系统中都少不了diff功能——展示一个文件内容的增加与删除。 绿色(+)表示新增 红色(-)表示删除 语法效果与代码高亮类似，在三个反引号后面写上diff。在内容中+表示新增，-表示删除。 123456+ 111+ 11+ 1- 222- 22- 2 扩展语法Markdown标准 本身所包含的功能有限，所以产生了许多第三方扩展语法，如 GFW, GitHub Flavored Markdown Tasklist代码： 12345- [ ] Monday- [ ] Tuesday- [ ] Wednesday- [ ] Tuesday- [ ] Friday 效果： Monday Tuesday Wednesday Tuesday Friday 表格 不管是哪种方式，第一行为表头，第二行为分割表头和主体部分，第三行开始每一行为一个表格行； 列与列之间用管道符号 | 隔开； 第二行还可以为不同的列指定对其方向，默认左对齐，在 - 右边加上 : 就右对齐 代码： 12345学号 | 姓名 | 分数- | - | -001 | 张三 | 78002 | 李四 | 67003 | 王五 | 99 学号 姓名 分数 001 张三 78 002 李四 67 003 王五 99 代码块和高亮代码块插入代码的方式有两种，一种是利用缩进(Tab)，另一种是利用反引号 `` 和 ``` ``` 代码： 1Python语言的输出函数 `Print()` 怎么使用？ 效果： Python语言的输出函数 Print() 怎么使用？ 123import osfrom flask import Flaskapp = Flask(app) 高亮在 ``` 之后添加代码的语言 代码： ```pythonimport osfrom flask import Flaskapp = Flask(app)``` 效果： 123import osfrom flask import Flaskapp = Flask(app) 流程图 流程图语法参考 LaTeX公式关于LaTEX： 是一种跨平台的基于TEX的排版系统，对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学、化学类文档。 关于MathJax： MathJax是一种跨浏览器JavaScript库，它使用MathML，LaTeX和ASCIIMathML 标记在Web浏览器中显示数学符号。MathJax作为Apache License下的开源软件。 MathJax语法 语法$表示行内公式质能守恒公式 $E=mc^2$ 方程式 效果：$E=mc^2$ $$表示整行公式 代码： 12345$$\sum_&#123;i=1&#125;^n a_i=0$$$$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$$$\sum^&#123;j-1&#125;_&#123;k=0&#125;&#123;\widehat&#123;\gamma&#125;_&#123;kj&#125; z_k&#125;$$ 效果： $$\sum_{i=1}^n a_i=0$$ $$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$ $$\sum^{j-1}{k=0}{\widehat{\gamma}{kj} z_k}$$ Markdown编辑器介绍一些常用的书写、编辑Markdown的工具。 MarkdownPad Windows (windows); Texts (Windows, osX); MarkPad (Windows); Haroopad (Windows, osX, Linux); ReText (Linux); 等等 格式转换Markdown文档可以方便地转换为 HTML, Word, PDF 等文件格式。可以利用 软件 或者 命令 转换文件。 转换为 HTML 转换为 PDF 转换为 Word]]></content>
      <categories>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx]]></title>
    <url>%2F2017%2F09%2F01%2FNginx%2F</url>
    <content type="text"><![CDATA[参考： Nginx官方文档 Nginx-Wikipedia Nginx-repo 环境： CentOS7x86_64; Nginx1.12.1 Nginx介绍Nginx（发音同engine x）是一个 Web服务器，也可以用作反向代理，负载平衡器和 HTTP缓存。它能反向代理 HTTP, HTTPS, SMTP, POP3, IMAP 的协议连接。基于BSD-like协议发行，支持多种操作系统。 作为HTTP服务软件的后起之秀，Nginx有很多优点： 在性能上，Nginx占用的系统资源更少，支持更多的并发连接（特别是小静态文件场景下），达到更高的访问效率； 在功能上，Nginx不仅是一个优秀的Web服务软件，还可以作为反向代理 负载均衡及缓存使用。它类似于LVS负载均衡及HAProxy等专业代理软件，又类似于Squid等专业缓存服务软件； 在安装配置上，Nginx方便、简单、灵活。 Nginx功能丰富，可作为HTTP服务器、反向代理服务器、邮件服务器。支持FastCGI, SSL, Virtual Host, URL Rewrite, Gzip等功能，并支持很多第三方模块扩展。 与PHP的集成自PHP-5.3.3起，PHP-FPM加入到了PHP核心，编译时加上–enable-fpm即可提供支持。PHP-FPM以守护进程在后台运行，Nginx响应请求后，自行处理静态请求，PHP请求则经过fastcgi_pass交由PHP-FPM处理，处理完毕后返回。Nginx和PHP-FPM的组合，是一种稳定、高效的PHP运行方式，效率要比传统的Apache和mod_php高出不少。 Nginx的重要特性： 可针对静态资源高速高并发访问及缓存；可使用反向代理加速，并且可进行数据缓存；具有简单负载均衡、节点健康检查和容错功能；支持远程FastCGI、Uwsgi、SCGI、Memcached Servers的加速和缓存；支持SSL、TLS、SNI；具有模块化的架构：过滤器包括gzip压缩、ranges支持、chunked响应、XSLT、SSI及图像缩放等功能。在SSI过滤器中，一个包含多个SSI的页面，如果FastCGI或反向代理处理，可被并行处理；它具备的其他WWW服务特性：支持基于名字、端口及IP的多虚拟主机站点；支持Keep-alived和pipelined连接；可进行修改Nginx配置，并且在代码上线时，可平滑重启，不中断业务访问；可自定义访问日志格式，临时缓冲些日志操作，快速日志轮询及通过rsyslog处理日志；可利用信号控制Nginx进程；支持 3xx-5xx HTTP状态码重定向；支持rewrite模块，支持URI重写及正则表达式匹配；支持基于客户端IP地址和HTTP基本认证的访问控制；支持PUT、DELETE、MKCOL、COPY及MOVE等较特殊的HTTP请求方法；支持FLV流和MP4流技术产品应用；支持HTTP响应速率限制；支持同一IP地址的并发连接或请求数连接；支持邮件服务器代理； Nginx常用功能http代理于反向代理Nginx在做反向代理时，提供性能稳定，并且能够提供配置灵活的转发功能。Nginx可以根据不同的正则匹配，采取不同的转发策略，比如图片文件结尾的走文件服务器，动态页面走web服务器，只要你正则写的没问题，又有相对应的服务器解决方案，你就可以随心所欲的玩。并且Nginx对返回结果进行错误页跳转，异常判断等。如果被分发的服务器存在异常，他可以将请求重新转发给另外一台服务器，然后自动去除异常服务器。 负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的啦，你可以参照所有的负载均衡算法，给他一一找出来做下实现。 web缓存Nginx可以对不同的文件做不同的缓存处理，配置灵活，并且支持FastCGI_Cache，主要用于对FastCGI的动态程序进行缓存。配合着第三方的ngx_cache_purge，对制定的URL缓存内容可以的进行增删管理。 web服务Nginx作为Web服务器的主要应用场景包括： 使用Nginx运行HTML、JS、CSS、小图片等静态数据； 结合FastCGI运行PHP等动态程序（如fastcgi_pass）； 结合Tomcat/Resin等支持Java动态程序（如proxy_pass）。 Nginx安装RPM源安装:12345678yum install -y gcc gcc-c++ make libtool zlib zlib-devel openssl openssl-devel pcre pcre-devel 安装依赖rpm -ivm http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm 安装RPM源#安装Nginxyum install -y nginx#查询安装rpm -q nginx 添加Nginx yum repository安装12345678910vim /etc/yum.repos.d/nginx.repo#必须唯一[nginx]name=nginx-repobaseurl=http://nginx.org/packages/$OS/$OSRELEASE/$basearch/gpgcheck=0enabled=1 源码安装12345678910#建议解压于此目录cd /usr/local/srcwget http://xxx.xx.com/nginx.tar.gztar -zxvf nginx.tar.gzcd ./nginx./configure --prefix=/usr/localmake&amp;&amp;make install Nginx配置*.confNginx配置文件主要分为四部分： main(全局设置)； server(主机设置)； upstream(上游服务器设置)，用于反向代理和负载均衡； location(URL匹配特定位置)。 栗子：运行nginx -t检查配置文件有误错误，这很重要! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#main块user nginx;worker_processes 4;client_max_body_size 10Merror_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 102400;&#125;#http块http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - ...'; access_log /var/log/nginx/access.log main; sendfile on; gzip on; keepalive_timeout 60; include /etc/nginx/conf.d/*.conf; #upstream块 upstream up_name&#123; server ip1; server ip2:port; server domain; &#125; server &#123; server_name www.zhang21.cn; listen 80; listen 443; ssl on; ssl_certificate /dir/path/xxx.crt; ssl_certificate_key /dir/path/xxx.key; location / &#123; root /var/www/zhang; index index.php index.html index.htm; allow 192.168.1.0/22; deny all; &#125; location ~ \.php$ &#123; root /var/www/zhang; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125; 全局块：配置影响Nginx全局的指令。一般由运行Nginx服务器的用户组，Nginx进程pid存放路径，日志存放路径，允许生成的worker_processes等。 events块：配置影响Nginx服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网络连接，开启多个网络连接序列化等。 http块：可以嵌套多个server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，mime-type定义，日志自定义，是否使用sendfile传输文件，链接超时时间，单连接请求数等。 server块：配置虚拟主机的相关参数，一个http中可以有多个server。 location块：配置请求的路由，以及各种页面的处理情况。 Nginx http功能模块 模块说明 ngx_http_core_module 一些核心的http参数配置，对应Nginx的配置中的http块 ngx_http_access_module 访问控制模块，用来控制网站用户对Nginx的访问 ngx_http_gzip_module 压缩模块，对Nginx返回的数据压缩，属于性能优化模块 ngx_http_fastcgi_module FastCGI模块，和动态应用相关的模块，如PHP ngx_http_proxy_module proxy代理模块 ngx_http_upstream_module 负载均衡模块，可实现网站的负载均衡及节点的监控检查 ngx_http_rewrite_module URL重写模块 ngx_http_limit_conn_module 限制用户并发连接数及请求数模块 ngx_http_limit_req_module 根据定义的key限制Nginx请求过程的速率 ngx_http_log_module 访问日志模块，以指定的格式记录Nginx访问信息 ngx_http_auth_basic_module web认证模块，设置通过账号，密码访问Nginx ngx_http_ssl_module ssl模块 ngx_http_stub_status_module 记录Nginx基本访问状态信息扥的模块 Nginx的日志时自动切割，并且一行可以记录多个日志格式。 Nginx日志格式 说明 $remote_addr 客户端ip地址 $http_x_forward_for 当前端有代理服务器时，设置web节点记录web节点记录客户端地址的配置 $remote_user 客户端用户名称 $time_local 访问时间和时区 $request 请求的http协议和URL $status 请求状态，如200 $body_bytes_sent 发送给客户端文件主体内容大小 $http_referer 从哪个页面链接访问过来 $http_user_agent 客户端浏览器信息 serverhttp服务上支持若干虚拟主机。每个虚拟主机对应一个server配置项，配置项里面包含该虚拟主机的相关配置。每个server里面可同时有多个server_name。 在提供mail代理服务时，也可建立若干server，每个server通过监听地址或端口来区分。 12345#监听端口，默认80listen 80;listern 443;#listen 88server_name www.zhang21.cn locationlocation是http服务中，某些特定的URL对应的一系列配置项。 root 定义此location的根目录位置，一般放置在server里 index 定义路径下的默认访问的文件名 12345location / &#123; root /dir/path; index index.html index.htm;&#125; location的正则写法location的使用方法： 符号 含义 优先级 用法 = 精确匹配 最高 location = ~ 区分大小写的正则匹配 次次之 location ~ ~* 不区分大小写的正则匹配 次次之 location ~* ^~ 常规字符串匹配 次之 location ^~ / 通用匹配 最低 location / 优先级： = &gt; 完整路径 &gt; ^~ &gt; ~, ~* &gt; 部分路径 &gt; / location使用建议location的使用根据实际情况来定。 但个人觉得至少应该有三个匹配规则： 直接匹配网站跟，通过域名访问网站首页比较频繁 处理静态文件请求，这是Nginx作为http服务器的强项 通用规则，用来转发动态请求到后端的应用服务器(符php-fpm) 根据实际情况的自定义需求 1234567891011121314151617181920212223242526272829303132333435363738394041server &#123; listen 80; listen 443; server_name zhang21.cn www.zhang21.cn; root /dir/path/zhang; ssl on; ssl_certificate /etc/nginx/ssl/zhang.crt; ssl_certificate_key /etc/nginx/ssl/zhang.key; #rewtire ^(.*)$ https://zhang21.cn/$1 permanent; return 301 https://zhang21.cn/$requets_uri location = / &#123; rewrite .*? /index.html last; &#125; location ^~ /static/ &#123; root /dir/path/zhang/static; &#125; location ~* \.(gif|jpg|png|css|js)$ &#123; root /dir/path/zhang/static; &#125; location / &#123; if (!-f $request_filename) &#123; rewrite ^([^\?]+)$ /index.php?q=$1 last; &#125; location ~ \.php$ &#123; root /dir/path; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root/$fastcgi_script_name; include fastcgi_params; &#125;&#125; RewriteNginx的主要功能是实现URL地址重写。Nginx的rewrite规则需要PCRE软件的支持，即通过Perl兼容正则表达式语法进行规则匹配。 Nginx rewrite语法： 123server, location, ifrewrite regex replacement [flag] rewrite的功能就是，使用Nginx提供的全局变量或自定义变量，结合正则表达式(re)和标志位实现URL重写以及重定向 rewrite只能放在server，location，if中，并且只能对域名后边的除去传递的参数外的字符串起作用 如果相对域名或参数字符串起作用，可以使用全局变量匹配，也可使用proxy_pass反向代理 表面看rewrite和location功能有点像，都能实现跳转。主要区别在于： rewrite实在同一域名内更改获取资源的路径 location是对一类路径做访问控制或反向代理，可proxy_pass到其它机器 循环超多10次，返回500 Internal Server Error! flag last 表示完成rewrite，继续向下匹配新的规则 break 停止执行当前虚拟主机的后续rewrite指令集 redirect 返回302临时重定向，地址栏会显示跳转后的地址 permanent 返回301永久重定向，地址栏会显示跳转后的地址 last和break用来实现URL重写，浏览器地址栏的URL地址不变，但在服务器端访问的程序及路径发生了变化 redirect和permanent用来实现URL跳转，浏览器地址栏会显示跳转后的URL地址 Nginx的rewrite功能应用非常广泛： 可调整用户浏览的URL，使其看起来更规范 将动态URL地址伪装成静态地址提供服务 让旧域名跳转到新域名上 根据特殊变量、目录、客户端的信息进行URL跳转 if指令if语法： 123if (condition) &#123; xxx;&#125; if的条件可以是如下内容： 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会被当作false 直接比较变量和内容时，使用=或!= ~正则表达式匹配，~*不区分大小写的正则匹配，!~不匹配 -f和!-f，用来判断是否存在文件 -d和!-d，用来判断是否存在目录 -e和!-e，用来判断时都存在文件或目录 -x和!-x，用来判断文件是否可执行 栗子： 123456789101112131415161718192021222324if ($http_user_agent ~ MSIE) &#123; rewrite ^(.*)$ /msie/$1 break;&#125;if ($http_cookie ~* "id=([^;])(?:;|$)") &#123; set $id $1;&#125;if ($http_method = POST) &#123; return 405;&#125;if (!-f $request_filename) &#123; break; proxy_pass http://zhang;&#125;if ($invalid_referer) &#123; return 403;&#125; Nginx全局变量常用作if判断的全局变量： 变量 描述 备注 $args 等于请求行中的参数 同$query_string $body_bytes_sent 响应是发送的body字节数 xxx $content_length Request Header中的Content-Length字段 内容长度 $content_type Request Header中的Content-Type字段 内容类型 $document_root 当前根路径 xxx $host 请求主机头字段，否则为服务器名称 xxx $hostname 主机名 xxx $http_user_agent 客户端agent信息 xxx $http_cookie 客户端cookie信息 xxx $is_args 如果有$args参数，这个变量等于”?”，否则等于空 xxx $limit_rate 限制连接数度 xxx $remote_addr 客户端IP地址 xxx $remote_port 客户端端口 xxx $remote_user 经过Auth Basic Module验证的用户名 要先开启Nginx认证 $request 用户请求信息 xxx $request_method 客户端请求方法 通常为POST或GET $request_body 记录POST过来的数据信息 xxx $request_filename 当前请求的文件路径 由root或alias指令与URI请求生成 $request_completion 如果请求结束，设置为OK。否则为空 xxx $scheme HTTP方法 如http, https $server_protocol 请求使用的协议 通常为HTTP/1.0或HTTP/1.1 $server_addr 服务器地址 在完成一次系统调用后可以确定这个值 $server_name 服务器名称 xxx $server_port 请求到达服务器的端口号 xxx $status 请求的响应状态码 如200 $request_uri 包含请求参数的原始URI，不包含主机名 如”/foo/bar.php?arg=abc” $uri 不带请求参数的当前URI，不包含主机名 如”/foo/bar.html” 栗子： 12345678http://localhost:88/test1/test2/test.php$hsot: localhost$server_port: 88$request_uri: http://localhost:88/test1/test2/test.php$document_uri: /test1/test2/test.php$document_root: /var/www/test$request_filename: /var/www/test/test1/test2/test.php rewrite实例12345678910111213141516171819202122232425http &#123; log_format main xxxx; rewrite_log on; server &#123; root /var/www/zhang; location / &#123; error_log logs/rewrite.log notice; rewrite '^/images/([a-z]&#123;2&#125;)/([a-z0-9]&#123;5&#125;)/(.*)\.(png|jpg|gif)'/data?file=$3.$4; set $image_file $3; set $image_type $4; &#125; location /data &#123; access_log logs/images.log main; root /data/images; type_file /$arg_file /images404.html; &#125; location = /image404.html &#123; return 404 "Image Not Found\n"; &#125; &#125;&#125; 访问控制 添加用户密码验证有时需要为我们的网站设置访问账号和密码权限。 具体为这两个参数： auth_basic 默认值： auth_basic off; 使用位置：http, server, location, limit_except auth_basic_user_file 使用位置： http, server, location, limit_except 栗子： 12345678910111213141516cd /etc/nginx/conf.dvim test.conflocation / &#123; auth_basic "Zhang"; auth_basic_user_file /etc/nginx/nginx.auth;&#125;vim /etc/nginx/nginx.auth#user:passwdzhang:zhangtest:test 限制IP访问 allow deny 1234567891011server &#123; ... allow IP1 allow IP2; deny all; location / &#123; allow IP 1; deny all; &#125;&#125; 注意： deny一定要加一个IP，否则会直接跳转403，不在往下执行。如果403默认页是在同一域名下，会造成死循环访问 对于allow的IP短，从允许访问的IP段位从小到大排列，如127.0.0.0/24， 10.10.0.0/16 以deny all结尾，表示除了上面允许的，其它都禁止 语法检查在启动或重启Nginx服务前检查语法非常重要，可以防止因配置错误导致网站重启或重载配置对用户的影响。 每次更改Nginx配置文件后都需要重新加载，将配置信息加载到内存中。这样设计的目的是大幅度提升Nginx的访问性。 123nginx -tnginx -s reload Nginx优化 常用优化 隐藏Nginx版本号一般来说，软件漏洞都和版本有关。因此要尽量隐藏对访问用户显示各类敏感信息。 123456789vim /etc/nginc/nginx.conf#nginx版本号默认是开启的#位置：http, server, locationhttp &#123; server_tokens off|on;&#125; 更改Nginx服务默认用户 修改配置文件 123vim /etc/nginx/nginx.confuser nginx; 如果是编译安装，直接在编译的时候指定用户和组 1./configure --user=nginx --group=nginx 优化Nginx进程对应的配置123456789101112vim /etc/nginx/nginx.confworker_process n;#建议n为CPU核数#高并发场合可考虑为核数*2#查看CPU核数cat /proc/cpuinfo | grep processor | wc -llscputop命令，按1显示所有CPU核数 优化绑定不同的Nginx进程到不同的CPU上默认情况下，Nginx的多个进程有可能跑在某一个CPU或CPU的某一核上，导致Nginx进程使用硬件资源不均。所以，要尽可能地分配不同的Nginx进程给不同的CPU处理，达到充分有效利用硬件的多CPU多核资源的目的。 4核CPU配置举例： 12345vim /etc/nginx/nginx.confworker_processes 4;#CPU亲和力参数worker_cpu_affinity 0001 0010 0100 1000; Nginx事件处理模型优化Nginx的连接处理机制在不同的操作系统会采用不同的I/O模型，在Linux下，Nginx使用epoll的I/O多路复用模型，在FreeBSD中使用kqueue的I/O多路复用模型，在solaris中使用/dev/poll方式的I/O多路复用模型，在Windows中使用的是icop。 配置： 12345678#对于linux内核，推荐使用epoll工作模式#Linux下默认epollvim /etc/nginx/nginx.confevents &#123; use epoll;&#125; Nginx单个进程允许的客户端最大连接数请根据服务器性能和程序的内存使用量来合理制定最大连接数。这个连接数包括了所有连接，如代理服务器连接、客户端的连接、实际的并发连接。 Nginx总并发连接数=worker*worker_connections 123456vim /etc/nginx/nginx.confevents &#123; worker_connections 10240;&#125; 仅仅修改了nginx最大连接数可能还不行，由于Linux系统有ulimit限制，所以可能还要做额外操作。 如：nginx: [warn] 10240 worker_connections exceed open file resource limit: 1024。 配置： 123ulimit -aulimit -n 10240 注意，使用ulimit命令修改的值并不是永久生效的。 Nginx worker进程最大打开文件数可能也要注意ulimit系统限制！ 12345vim /etc/nginx/nginx.confevents &#123; worker_rlimit_nofile 65535;&#125; 开启高效文件传输sendfilesendfile()是作用于两个文件描述符之间的数据拷贝，这个拷贝是在内核之中的，被称为零拷贝。sendfile（）比read和write函数要高效很多，因为write和read函数要把数据拷贝到应用层再进行操作。 12#位置：http, server, location, if in locationsendfile on; tcp_nopush激活或禁用Linux上的TCP_CORK socket选项，仅当开启sendfile生效。允许把 http response和文件的开始部分放在一个文件里发布，其积极作用是减少网络报文段的数量。 12位置： http, server, locationtcp_nopush on; Nginx连接参数，连接超时时间keep-alive可以使客户端到服务器端已经建立的连接一致工作不退出，当服务器有持续请求时，keep-alive会使用已经建立的连接提供服务，从而避免服务器重新建立新连接请求处理。 连接超时的作用： 将无用的连接设置为尽快超时，可保护系统资源（CPU、内存、磁盘）连接很多时，及时断掉那些已经建立好但又长时间不做事的连接，以减少其占用的服务器资源。因为服务器维护连接也是消耗资源的黑客和恶意用户攻击网站，也会不断地和服务器建立多个连接，消耗连接数但啥也不干，大量消耗服务器的资源，此时就应该及时断掉这些恶意占用资源的连接LNMP环境中，如果用户请求了动态服务，则Nginx就会建立连接，请求FastCGI服务以及后端的MySQL服务，此时这个Nginx连接就要设置一个超时时间，在用户容忍的时间内返回数据，或者再多等一会后端服务返回数据，具体策略根据具体业务进行具体分析后端的FastCGI服务及MySQL服务也有对连接的超时控制 12位置： http, server, locationkeepalive_timeout 60; 默认情况下当数据发送时，内核并不会马上发送，可能会等待更多的字节组成一个数据包，这样可以提高I/O性能。但是，在每次只发送很少字节的业务场景中，不使用tcp_nodelay功能，等待时间会比较长。 12位置： http, server, locationtcp_nodelay on; 读取客户端请求头数据的超时时间，如果超过这个时间，客户端还没有发送完整的header数据，服务器端将返回“Request time out（408）”错误。 12位置： http, serverclient_header_timeout 20; 读取客户端请求主体的超时时间，如果在这个超时时间内，客户端没有发送任何数据，Nginx将返回“Request time out（408）”错误。 12位置： http, server, locationclient_body_timeout 60; 指定响应客户端的超时时间，为握手后的一个超时。如果超过这个时间，客户端没有任何活动，Nginx将会关闭连接。 12位置： http, server, locationsend_timeout 60; 上传文件大小限制(动态应用)设置为0，表示禁止检查客户端请求主体的大小。 12位置： http, server, locationclient_max_body_size 20m; gzip压缩Nginx gzip压缩模块提供了压缩文件内容的功能，用户请求的内容在发送到用户客户端之前，Nginx服务器会根据一些具体的策略实施压缩，以节约网络出口带宽，同时加快数据传输效率，提升用户体验。 压缩对象： 纯文本内容压缩比很高，如 html, js, css, xml等 被压缩的纯文本文件必须要大于1KB，由于压缩算法的特殊原因，极小的文件压缩后可能反而变大 图片、媒体等文件尽量不要压缩，因为这些文件大都经过压缩，再压缩很可能不会减小很多，或有可能增大，同时还要消耗系统资源 配置： 1234567891011121314151617181920212223242526#压缩功能gzip on;#允许压缩的页面最小字节数gzip_min_length 1K;#申请4个单位为16K的内存作为压缩结果流缓存gzip_buffers 4 16K;#http协议版本gzip_http_version 1.1;#指定压缩比，1压缩比最小，处理速度最快；9压缩比最大，传输速度最快，处理最慢gzip_comp_level 5;#指定压缩类型，对应文件类型参考mime.typesgzip_types text/html text/css;#vary header支持gzip_vary on; 在response header中查看效果： Content-Encofing: gzip expires缓存Nginx expires的功能就是为用户访问的网站内容设定一个过期时间。 当用户第一次访问这些内容时，会把这些内容储存在用户浏览器本地，这样用户第二次及以后继续访问该网站时，浏览器会检查加载已经缓存在用户浏览器本地的内容，而不用去服务器下载，直到缓存的内容过期或被清除为止。 缓存也要根据业务！当网站数据更新时，用户端看到的可能还是旧的已经缓存的内容。 配置： 根据文件扩展名进行判断 12345678location ~ .*\.(gif|png|jpg|swf)$ &#123; expires 10d;&#125;location ~ .*\.(css|js)$ &#123; expires 20d;&#125; 根据目录进行判断 123location ~ ^/(images|static|media)/ &#123; expires 50d;&#125; 在response header中查看： Expires: 缓存过期时间Cache-Control： 缓存总时间 FastCGI相关参数FastCGI参数是配合Nginx向后请求PHP动态引擎服务的相关参数，这里指的是Nginx中的配置参数。 Module ngx_http_fastcgi_module： https://nginx.org/en/docs/http/ngx_http_fastcgi_module.html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#给FastCGI服务器设置地址fastcgi_pass#设置一个将在 $fastcgi_scripts_name 变量结尾的URI之后添加的文件名fastcgi_index#设置一个应该传递给FastCGI服务器的参数，当且仅当fastcgi_param在当前级别上没有定义指令时，这些指令将从上一级继承fastcgi_param#指定在哪种情况下将请求传递给下一个服务器；fastcgi_next_upsteam#表示Nginx服务器和后端FastCGI服务器连接的超时时间，默认值为60秒，这个参数通常不要超过75秒fastcgi_connect_timeout#设置Nginx允许FastCGI服务器端返回数据的超时时间，即在规定时间之内后端服务器必须传完所有的数据。否则，Nginx将断开这个连接fastcgi_send_timeout#设置Nginx从FastCGI服务器端读取响应信息的超时时间，表示连接建立成功后，Nginx等待后端服务器的响应时间，是Nginx已经进入后端的排队之中等候处理的时间fastcgi_read_timeout#这是Nginx FastCGI的缓冲区大小参数，设定用来读取从FastCGI服务器端收到的第一部分响应信息的缓冲区大小，这里的第一部分通常会包含一个小的响应头部fastcgi_buffer_size#设定用来读取从FastCGI服务器端收到的响应信息的缓冲区大小和缓冲区数量fastcgi_buffers#用于设置系统很忙时可以使用的proxy_buffers大小，官方推荐大小为proxy_buffers * 2proxy_busy_buffers_size #用于设置系统很忙时可以使用的fastcgi_buffers大小，官方推荐为 fastcgi_buffers * 2fastcgi_busy_buffers_size#FastCGI临时文件大小fastcgi_temp_file_write_size#表示开启FastCGI缓存并为其指定一个名称fastcgi_cache cachename_nginx#fastcgi_cache缓存目录fastcgi_cache_path#用来指定应答代码的缓存时间fastcgi_cache_valid#设置请求几次之后响应将被缓存fastcgi_cache_min_uses#定义在哪些情况下使用过期缓存fastcgi_cache_use_stale#定义fastcgi_cache的keyfastcgi_cache_key 日志与安全现在Nginx 日志已经自动轮询了，所以感觉没有必要自己切割日志！ 不记录不需要的日志日志写入太频繁会消耗大量的磁盘I/O，降低服务性能。 123location ~ .*\.(js|png|css|gif|jpg) &#123; access_log off;&#125; 日志权限因为nginx master process的UID是root，所以可以修改日志权限。不需要在日志目录上给Nginx用户读或写许可，很多人没注意这个问题，把权限直接给了Nginx用户，这就存在安全隐患。 12chown -R root:root /path/log/nginxchmod -R 700 /path/log/nginx 站点目录及URL访问控制根据扩展名限制程序或文件访问利用Nginx配置禁止访问上传资源目录下的PHP、Shell、Perl、Python程序文件，这样用户即使上传了木马文件也没法执行，从而加强了网站的安全。 对这些的限制必须放在Nginx处理.php, .py, .sh等文件的前面！ 1234567891011121314151617#禁止解析指定目录下的程序location ~ ^/images/.*\.(php|py|sh|pl)$ &#123; deny all;&#125;location ~ ^/static/.*\.(py|php|pl|sh) &#123; deny all;&#125;#禁止访问某些文件location ~* \.(txt|doc)$ &#123; root /var/www/file; deny all;&#125; 禁止访问指定目录123456789101112131415location ~ ^/test/ &#123; deny all;&#125;#禁止访问多个目录location ~ ^/(test|zhang) &#123; deny all;&#125;#返回状态码location ~ ^/haha/ &#123; return 403 "Hahaha";&#125; 禁止非法域名解析访问网站防止用户恶意域名解析。 1234567891011121314151617181920cd /etc/nginx/conf.dvim default.conf#返回HTTP状态码server &#123; listen 80 default_server; server_name _; return 403;&#125;#重定向server &#123; listen 80 default_server; server_name _; rewrite ^(.*) https://www.baidu.com permanent;&#125; 利用default_server，将网站所有请求定向到维护页面。 1234567891011121314151617181920212223242526272829303132333435server &#123; listen 80 default_server; server_name _; root /var/www; location / &#123; rewrite ^(.*) /maintance.html break; &#125;&#125;cd /var/wwwvim maintance.html&lt;html&gt;&lt;head&gt;&lt;meta charset="UTF-8"&gt;&lt;style type="text/css"&gt; h1&#123;text-align: center; color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;br&gt;&lt;br&gt;&lt;body&gt;&lt;h1&gt;网站维护中！&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 图片及目录防盗链 什么是资源盗链简单地说，就是某些不法网站未经许可，通过在其自身网站程序里非法调用其他网站的资源，然后在自己的网站上显示这些调用的资源，达到填充自身网站的效果。 这一举动不仅浪费了调用资源网站的网络流量，还造成其他网站的带宽及服务压力吃紧。 问题： 某公司CDN源站流量没有变化，但CDN加速那边流量超了很多。这么大异常流量，全都是钱呀！ 常见防盗链解决方案 根据HTTP referer 实现防盗链在HTTP 协议中，有一个表头字段叫 referer，使用URL格式来表示是哪里的链接用了当前网页的资源。 通过referer可以检测访问的来源网页，如果是资源文件，可以跟踪到显示它的网页地址，一旦检测出来不是本站，马上进行阻止。 HTTP referer 是header的一部分，当浏览器向web服务器发送请求时，一般会带上referer，告诉服务器我是从哪个页面过来的，服务器借此获得一些信息用于处理。 根据cookie防盗链通过加密技术变换访问路径实现防盗链 Nginx实现防盗链利用referer，针对指定扩展名进行rewrite或其他操作。 请根据实际情况进行域名防盗链！ 123456location ~* \.(jpg|png|gif|wav|mp3|zip|rar)$ &#123; valid_referers none blocked *.zhang.com; if ($invalid_regerer) &#123; rewrite https://www.baidu.com; &#125;&#125; 或者在产品设计上解决防盗链，如为资源加上水印等措施。 错误页面优雅展示我们可以将404、403等错误信息重定向到其他指定的页面，提升网站的用户访问体验！ 123456789101112location / &#123; xxxx; error_page 403 /403.html; error_page 404 /404.jpg; error_page 500 503 504 /50x.html; location = /50x.html &#123; root /var/www/50x.html; &#125;&#125; 目录及文件权限优化为了保证网站安全，所有站点的目录和用户组都为root，所有目录权限是755，所有文件权限是644。虽然这样的全线可以防止黑客上传修改站点的文件，但这样合法的用户便也没有了上传权限。 比较好的方法是将用户上传文件的服务器与读取服务器进行分离，这样就可以进行安全授权。不同的服务所在的目录的权限依据业务功能而不同。 严格控制Nginx目录的访问才能降低网站被入侵的风险！ 反爬虫优化 robots.txt机器人协议robots协议(维基百科)，也称为机器人协议，全称是网络爬虫排除标准（Robots Exclusion Protocol）。网站通过Robots协议告诉搜索引擎那些页面可以抓取，那些页面不能抓取。 robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。 123User-Agent: *Allow: /zhangDisallow: / Nginx反爬虫配置123456789if ($http_user_agent ~* LWP::Simple|BBBike|wget) &#123; return 403;&#125;if （$http_user_agent ~* (Firefox|MSIE) &#123;rewrite ^（.*） http://www.baidu.com:&#125; 限制HTTP请求方法123if ( $request_method !~ ^(GET|POST|HEAD)$ ) &#123; return 501;&#125; CDNCDN的全称是 Content Delivery Network，中文意思是内容分发网络。我们可以利用CDN做网站内容加速。 简单地讲，通过现有的Internet中增加一层新的网络架构，将网站的内容发布到最接近用户的Cache服务器内，通过智能DNS负载均衡技术，判断用户的来源，让用户就近使用与服务器相同线路的带宽访问Cache服务器，取得所需的内容。 例如，北京电信用户访问北京电信Cache服务器上的内容，四川网通用户访问成都网通Cache服务器上的内容。这样可以有效减少数据在网络上传输的时间，提高访问速度。CDN是一套全国或全球的风不是缓存集群，其实质是通过职能DNS判断用户的来源地域及上网线路，为用户选择一个最接近用户地域，以及和用户上网线路相同的服务器节点。因为低于近，线路相同，所以可以大幅度提升用户浏览网站的体验。 CDN的价值： 提升用户体验 阻挡大部分流量攻击 CDN的特点： 通过服务器内存缓存网站数据，提高了企业站点（尤其是含有大量图片、视频等的站点）的访问速度，并大大提高企业站点的稳定性； 用户根据智能DNS技术自动选择最适合的Cache服务器，降低不同运营商之间互联瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问速度； 加快了访问速度，减少了原站点的带宽； 用户访问时从服务器的内存中读取数据，分担了网络流量，同时减轻了原站点负载压力； 使用CDN可以分担源站的网络流量，同时减轻源站的负载压力，并降低黑客入侵及各种DDOS攻击对网站的影响，保证网站有较好的服务质量； 使用CDN的要求首先要说的是，不是所有的网站都可以一上来就能用CDN的。要加速的业务数据应该存在独立的域名。如 pub.zhang21.com，业务内容图片、附件、JS、CSS等静态元素，这样的静态网站域名才能使用CDN。 将域名做CNAME(别名)将如上的pub.zhang21.com配置成CDN的域名。 程序架构优化解耦 是开发人员中流行的一个名词，简单地说就是把一堆程序代码按照业务用途分开，然后提供服务。 例如，注册登录、上传、下载、浏览、商品页信息等都应该是独立的程序服务，只不过在客户端看来是一个整体而已。 分离的最佳方式是分别使用独立的服务器，可以选择改动程序或者在负载均衡器上配置（如Nginx），过滤请求，然后抛给后面对应的服务器。 根据扩展名分发，请求图片就抛给图片服务器； 根据URL路径转发，请求下载就交给下载服务器； 请求动态PHP处理的就交给动态处理器； 不符合以上要求的就交给默认服务器； 使用no-root用户启动Nginx默认情况下，Nginx的Master进程使用的是root用户，worker进程使用的是Nginx指定的普通用户。 使用root用户跑Nginx的Master进程有两个最大问题： 管理权限必须是root，这就使得最小化分配权限原则遇到困难 使用root跑Nginx服务，一旦网站出现漏洞，用户就可以很容易地获取服务器的root权限 控制Nginx并发连接数ngx_http_limit_conn_module这个模块用于限制每个定义的Key值的连接数，特别是单IP的连接数。 不是所有的连接数都会被计数，一个符合要求的连接是整个请求头已经被读取的连接。 用法： 1234567#位置： httplimit_conn_zone key zone=name:size;#位置： http, server, locationlimit_conn zone number; 栗子： 123456789101112http &#123; limit_conn_zone $binary_remote_addr zone=addr:10m; xxx;&#125;server &#123; xxx; location /download/ &#123; limit_conn addr 3; #限制单IP并发连接为3 &#125;&#125; 控制客户端请求Nginx的速率ngx_http_limit_req_module被用来限制每个IP访问没法key的请求速率。 用法： 1234567#位置： httplimit_req_zone key zone=name:size rate=rate;#位置： http, server, locationlimit_req zone=name; 栗子： 123456789http &#123;limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; server &#123; location /search/ &#123; limit_req zone=one burst=5; &#125; &#125;&#125; Nginx反向代理与负载均衡 反向代理与负载均衡严格地说，Nginx仅仅是作为Nginx Proxy反向代理使用的，因为这个反向代理功能表现的效果是负载均衡集群的效果，所以本文称之为Nginx负载均衡。 普通的负载均衡软件，如LVS，其实现的功能只是对请求数据包的转发、传递，从负载均衡下的节点服务器来看，接收到的请求还是来自访问负载均衡器的客户端的真实用户 反向代理服务器在接收访问用户请求后，会代理用户 重新发起请求代理下的节点服务器，最后把数据返回给客户端用户。在节点服务器看来，访问的节点服务器的客户端用户就是反向代理服务器，而非真实的网站访问用户 即，LVS等负载均衡是转发用户请求的数据包，而Nginx反向代理是接收用户请求后重新发起请求后端节点 这里我去看了一下Nginx的access.log，客户端的访问日志全在代理节点上（Nginx-upstream），而后端节点的access.log的来源是前端代理节点的IP Nginx负载均衡的组件实现Nginx负载均衡的组件主要有两个: proyx upstream Nginx_http模块 模块说明 ngx_http_proxy_module proxy代理模块，用于把请求后抛给服务器节点或upstream服务器池 ngx_http_upstream_module 负载均衡模块，可以实现网站的负载均衡功能即节点的健康检查 nginx upstream模块 upstream模块介绍Module ngx_http_upstream_module: https://nginx.org/en/docs/http/ngx_http_upstream_module.html 123Syntax： upstream name &#123; ... &#125;Default: —Context: http Nginx的负载均衡功能依赖于ngx_http_upstream_module模。所支持的代理方式包括： proxy_pass fastcgi_pass memcached_pass uwsgi_pass scgi_pass upstream模块允许Nginx定义一组或多组节点服务器组，使用时可通过proxy_pass代理方式把网站的请求发送到事先定义好的对应upstream组的名字上。 upstream模块内容放置于http{}内: 12345678910111213141516171819202122232425262728293031323334upstream upstream_name &#123; server address [ parameters ]&#125;####栗子http &#123; upstream zhang &#123; server 192.168.1.22:8080 weight=5; server www.zhang.cn weigh=5 max_conns=102400; server 192.168.33 max_fails=2 fail_timeout=20s; server backup.zhang.cn backup; &#125;&#125;server &#123; location / &#123; proxy_pass http://zhang; &#125;&#125;####reslovehttp &#123; resolver 10.0.0.1; upstream u &#123; zone ...; ... server example.com resolve; &#125;&#125; address可以是主机名、域名、ip或Unix Socket，也可以指定端口号 域名时需要解析的哦 parameters代表可选参数, 有如下： backup，表示当前server是备用服务器，只有其它非backup后端服务器都挂掉了或很忙才会分配请求给它 max_conns，限制同时连接到代理服务器的最大数量。默认值为0，表示没有限制。 weight，表示当前server负载权重，权重越大几率愈高 max_fails和fail_timeout一般会关联使用，如果某台server在fail_timeout时间内出现了max_fails次连接失败，那么Nginx会认为其已经挂掉，从而在 fail_timeout 时间内不再去请求它，fail_timeout默认是 10s，max_fails默认是1，即默认情况只要是发生错误就认为服务器挂了，如果将max_fails设置为0，则表示取消这项检查 down，标志服务器永远不可用，可配合ip_hash使用 resolve，监视与服务器域名相对应的ip地址的变化，并自动地修改上游配置，而不用重启Nginx route，设置服务器路由名称 service slow_start，设置服务器将其weight从零恢复到正常值的时间 drain，使服务器进入drain模式，在此模式下，只有绑定到服务器的请求才会被代理 upstream模块参数 说明 weight 服务器权重 max_fails Nginx尝试连接后端主机失败的次数，这个值是配合proxy_next_upstream、fastcgi_next_upstream和memcached_next_upstream这三个参数来使用的。当Nginx接收后端服务器返回这三个参数定义的状态码时，会将这个请求转发给正常工作的的后端服务器。如404、503、503、max_files=1 fail_timeout max_fails和fail_timeout一般会关联使用，如果某台server在fail_timeout时间内出现了max_fails次连接失败，那么Nginx会认为其已经挂掉，从而在fail_timeout时间内不再去请求它，fail_timeout默认是10s，max_fails默认是1，即默认情况只要是发生错误就认为服务器挂了，如果将max_fails设置为0，则表示取消这项检查 backup 表示当前server是备用服务器，只有其它非backup后端服务器都挂掉了或很忙才会分配请求给它 down 标志服务器永远不可用，可配合ip_hash使用 如果是两台Web服务器做高可用，可能就需要Keepalived配合。那使用backup参数通过负载均衡功能就可以实现Web服务器集群了。 upstream模块调度算法调度算法一般分为： 静态调度算法: 即负载均衡器根据自身设置的规则进行分配，不需要考虑后端节点服务器的情况 轮询 权重 ip_hash 动态调度算法: 即负载均衡器会根据后端节点的当前状态来决定是否分发请求，如连接数少或响应时间短的优先获得请求 fair least_conn url_hash 一致性hash 轮询(rr)默认调度算法。按照客户端请求顺序把请求逐一分配到不同的后端节点服务器，相当于LVS中的rr算法。如果后端服务器宕机，宕机的服务器会被自动从节点服务器池中剔除，以使客户端的用户访问不受影响，新的请求分配给正常的服务器。 权重轮询(wrr)权重越大，被转发的请求也就越多。可以根据服务器的配置和性能指定权重大小，有效解决新旧服务器性能不均带来的请求分配问题。 1234upstream weight &#123; server 191.168.1.11 weight=1; server 192.168.1.22 weight=2;&#125; ip_hash每个请求按客户端IP的hash结果分配，当新的请求到达时，先将其客户端ip通过哈希算法得出一个值，在随后的客户端请求中，客户IP的哈希值只要相同，就会被分配到同一台服务器。 该调度算法可以解决动态网页的session共享问题，但有时会导致请求分配不均，因为国内大多数都是NAT上网模式，多个客户端对应一个外部IP，所以这些客户端都会被分配到同一个节点服务器，从而导致请求分配不均。 ip_hash中，后端服务器在负载均衡调度中的状态不能有 weight和backup，有也不会生效 12345upstream iphash &#123; ip_hash; server 192.168.1.11; server 192.168.1.22:8080;&#125; fair根据后端节点服务器的响应时间来分配请求，响应时间短的优先分配。这是更加智能的调度算法。 Nginx本身不支持这种算法，需要upstream_fair模块: https://github.com/gnosek/nginx-upstream-fair 12345upstream fair &#123; server 192.168.1.11; server 192.168.1.22; fair;&#125; least_conn根据后端节点的连接数来决定分配情况，哪个机器少就分发给它。 url_hash根据访问URL的hash结果来分配请求的，让每个URL定向到同一个后端服务器，后端服务器为缓存服务器时效果显著。 Nginx本身不支持url_hash，需要hash。 1234567upstream urlhash &#123; server hahaha1:5678; server hahaha2:5678; hash $request_uri; hash_method md5; #同样不能使用 weight、backup&#125; 一致性hash一致性hash算法一般用于代理后端业务为缓存服务器（如Memcached）的场景，通过将用户请求的URI或者指定字符串进行计算，然后调度到后端的服务器上，此后任何用户查找同一个URI货值指定字符串都会被调度到这一台服务器上，因此后端的每个节点缓存的内容都是不同的。 12345upstream &#123; consistent_hash $request_uri; server xxx; server xxx;&#125; nginx proxy模块 proxy_pass介绍123Syntax: proxy_pass URL;Default: —Context: location, if in location, limit_except proxy_pass指令属于ngx_http_proxy_module模块，此模块可以将请求转发到另一台服务器，在实际的反向代理工作中，会通过location功能匹配指定的URI，然后把接收到服务匹配URI的请求通过proyx_pass抛给定义好的upstream节点池。 12345678910location /download/ &#123; proxy_pass http://download/vedio/;&#125;#这是前端代理节点的设置#交给后端upstream为download的节点location /name/ &#123; rewrite /name/([^/]+) /users?name=$1 break; proyx_pass http://127.0.0.1;&#125; http_proyx模块参数ngx_http_proxy_module: https://nginx.org/en/docs/http/ngx_http_proxy_module.html Nginx的代理功能是通过http_proxy模块来实现的。 proxy模块 说明 proxy_next_upstream 什么情况下将请求传递到下一个upstream proxy_limite_rate 限制从后端服务器读取响应的速率 proyx_set_header 设置http请求header传给后端服务器节点，如：可实现让代理后端的服务器节点获取访问客户端的这是ip client_body_buffer_size 客户端请求主体缓冲区大小 proxy_connect_timeout 代理与后端节点服务器连接的超时时间 proxy_send_timeout 后端节点数据回传的超时时间 proxy_read_timeout 设置Nginx从代理的后端服务器获取信息的时间，表示连接成功建立后，Nginx等待后端服务器的响应时间 proxy_buffer_size 设置缓冲区大小 proxy_buffers 设置缓冲区的数量和大小 proyx_busy_buffers_size 用于设置系统很忙时可以使用的proxy_buffers大小，推荐为proxy_buffers*2 proxy_temp_file_write_size 指定proxy缓存临时文件的大小 Nginx负载均衡配置 配置后端节点12345678910vi /etc/nginx/nginx.confserver &#123; listen 80; root /path/xxx; location / &#123; xxxx; &#125;&#125; 配置反向代理节点12345upstream test &#123; server test1 weight=5; server test2 weight=5; server 192.168.1.33;&#125; 1234567891011121314151617181920vi /etc/nginx/nginx.confserver &#123; listen 8888; server_name www.test.com www.xx.com; location / &#123; proxy_read_timeout 10s; proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_404; proyx_pass http://test;#把用户的请求反向代理定义的upstream服务器池 #proyx_set_header Host $host;在代理后端服务器发送的http请求头中加入host字段信息 #proxy_set_header $remote_addr;后端节点服务器日志获取客户端真实ip，否则全都是代理节点的ip #proyx_connect_timeout 30s; #proxy_buffers_size 4m; #xxx&#125; xxxxx&#125; 与反向代理配置相关的参数除了具有多虚拟主机代理以及节点服务器记录真实用户ip的功能外，Nginx还提供了相当多的作为反向代理和后端节点服务器对话的相关控制参数。 由于参数众多，建议把这些参数都写到另外一个配置文件里，然后用 include 方式包含到虚拟主机配置文件里。其他Nginx参数也同样可以使用此方法。 123456789101112vim /etc/nginx/proxy.confproxy_set_header Host $host;proxy_set_header $remote_addr;proxy_connect_timeout 60s;proxy_read_timeout 10s;proxy_send_timeout 20s;proxy_buffer_size 4m;proxy_buffer_size 4 2m;proxy_temp_file_write_size 2m;proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_404 12345678910vim /etc/nginx/conf.d/test.confserver &#123; listen 80; server_name www.test.com www.xxx.com; location / &#123; include /etc/nginx/proxy.conf; &#125;&#125; proxy_next_upstream参数补充当Nginx接收后端服务器发返回的proxy_next_upstream参数定义的状态码时，会将这个请求转发给正常工作的后端服务器，如500、502、503，此参数可以提升用户访问体验。 1proyx_next_upstream error timeout invalid_header http_500 http_503 http_502 http_504; 根据URL中的目录地址实现代理转发通过Nginx实现动静分离，即通过Nginx反向代理配置规则实现让动态资源和静态资源及其他业务分别由不同的服务器解析，已解决网站性能、安全、用户体验等重要问题。 动静态分离配置upstream.conf 123456789101112131415161718upstream static &#123; server 192.168.1.11; #或server static.com----hosts:static.com 192.168.1.11&#125;upstream upload &#123; server 192.168.1.22;&#125;upstream default &#123; server 192.168.1.33;&#125;#在http中加入,注意位置http &#123; include upstream.conf;&#125; 配置virtual.conf 12345678910111213141516171819202122232425262728293031323334353637#方案1：利用location实现location /static/ &#123; proyx_pass http://static; include proyx.conf;&#125;location /upload/ &#123; proxy_pass http://upload; include proxy.conf;&#125;location / &#123; proxy_pass http://default; include proxy.conf;&#125;========================================#方案2：利用if语句实现if ($request_uri ~* "^/static/(.*)$")&#123; proxy_pass http://static/$1;&#125;if ($request_uri ~* "^/upload/(.*)$")&#123; proxy_pass http://upload/$1;&#125;location / &#123; proxy_pass http://default; include proyx.conf;&#125; URL目录地址转发的应用场景根据HTTP的URL进行转发的应用情况，被称为 第7层（应用层）的负载均衡；而LVS的负载均衡一般用于TCP等的转发，因此被称为第四层（传输层）的负载均衡 。 有时因为需求，需要在代理服务器上通过配置规则，使得匹配不同规则的请求会交给不同的服务器池处理。 根据客户端的设备(user_agent)转发为了让不同客户端设备用户有更好的访问体验，需要在后端架设不同服务器来满足不同的客户端访问。如PC端和移动端，移动端又有安卓、苹果、Pad等。 常规4层负载均衡解决方案架构 在常规4层负载均衡架构下，可以使用不同的域名来实现这个需求。 如，分配移动端访问 wap.xxx.com，PC端访问www.xxx.com。 通过不同域名来引导用户到指定后端服务器，但是这样就分别得记住不同的域名。 第7层负载均衡解决方案 在7层负载均衡架构下，对外只需要用一个域名，如www.xxx.com，然后通过获取用户请求中的设备信息$http_user_agent，根据此信息转给后端合适的服务器处理。 根据$user_agent转发 123456789101112location / &#123; if ($http_user_agent ~* "android") &#123; proxy_pass http://android; &#125;if ($http_user_agent ~* "iphone") &#123; proxy_pass http://iphone; &#125;proxy_pass http://default;include proyx.conf; 根据文件扩展名实现代理转发 1234567891011121314location ~* .*\.(gif|jpg|png|css|js)$ &#123; proyx_pass http://static; include proxy.conf;&#125;#ifif ($request_uri ~* ".*\.php$") &#123; proxy_pass http://php; &#125;if ($request_uri ~* ".*\.(jpg|png|css|js)$") &#123; proxy_pass http://static; &#125; 在开发无法通过程序实现动静分离的时候，运维可以根据资源实体进行动静分离，根据不同实现策略制定后端服务器不同的组。在前端代理服务器上通过路径、扩展名等进行规则匹配，从而实现请求的动态分离。 Nginx负载均衡检测节点状态淘宝技术团队开发了一个Tengine（Nginx分支）模块nginx_upstream_check_module: https://github.com/yaoweibin/nginx_upstream_check_module，用于提供主动式后端服务器健康检查。通过它检测后端realserver的健康状态，如果后端节点不可用，则所有的请求就不会转发到该节点上。 Nginx需要通过打补丁的方式将该模块添加进去。 123456789101112131415161718192021222324252627282930wget https://codeload.github.com/yaoweibin/nginx_upstream_check_module/zip/masterunzip mastercd nginx_upstream_check_module-master #解压后的文件夹cd nginx源码安装包（我是 /usr/local/nginx-1.12.1）patch -p1 &lt; ../nginx_upstream_check_module-master/check_1.12.1+.patch #选择对应的Nginx版本号，我的是1.12.1 #打补丁#编译，注意以前的编译参数./configure --prefix=/usr/local/nginx \--user=nginx --group=nginx \--with-http_ssl_module \--with-http_realip_module \--with-http_addition_module \--with-http_gzip_static_module \--with-http_stub_status_module \--with-http_sub_module \--with-pcre \--add-module=../nginx_upstream_check_module-mastermake#给已经安装的Nginx系统打补丁不用执行make install#make是重新生成Nginx二进制启动命令#备份mv /usr/local/nginx/sbin/nginx&#123;,.bak&#125;#经打过补丁的Nginx二进制程序复制到/usr/local/nginx/sbin/ 下cp /usr/local/nginx-1.12.1/objs/nginx /usr/local/nginx/sbin/nginx -t 配置nginx_upstream_check 配置upstream.conf 12345678upstream zhang &#123; server 192.168.1.7:5678 weight=1; server 192.168.0.99:5678 weight=1; check interval=3000 rise=2 fall=5 timeout=1000 type=http; #每个3秒对负载均衡中所有节点检测一次，请求2次正常标记realserver状态为up； #如果检测5次都失败，则标记realserver状态为down，超时时间为1秒； #检查的协议为HTTP；&#125; 配置/status： 123456location /status &#123; check_status; access_log off; allow 192.168.1.0/24; deny all;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I Like For You To Be Still]]></title>
    <url>%2F2017%2F09%2F01%2FILikeForYouToBeStill%2F</url>
    <content type="text"><![CDATA[《I Like For You To Be Still》 出自聂努达诗集：《二十首情诗和一首绝望的歌》。 I like for you to be stillIt is as though you are absentAnd you hear me from far awayAnd my voice does not touch youIt seems as though your eyes had flown awayAnd it seems that a kiss had sealed your mouthAs all things are filled with my soulYou emerge from the thingsFilled with my soulYou are like my soulA butterfly of dreamAnd you are like the word: Melancholy I like for you to be stillAnd you seem far awayIt sounds as though you are lamentingA butterfly cooing like a doveAnd you hear me from far awayAnd my voice does not reach youLet me come to be still in your silenceAnd let me talk to you with your silenceThat is bright as a lampSimple, as a ringYou are like the nightWith its stillness and constellationsYour silence is that of a starAs remote and candid I like for you to be stillIt is as though you are absentDistant and full of sorrowSo you would’ve diedOne word then, One smile is enoughAnd I’m happy;Happy that it’s not true]]></content>
      <categories>
        <category>Literature</category>
      </categories>
      <tags>
        <tag>Pablo Neruda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Test-My-Site]]></title>
    <url>%2F2017%2F08%2F30%2FTest-My-Site%2F</url>
    <content type="text"><![CDATA[Monday Tuesday Wednesday Thursday Friday Saturday Sunday $$\sideset{^1_2}{^3_4}A$$ $E=mc^2$ $$\sum_{i=1}^n a_i=0$$ $$f(x_1,x_x,\ldots,x_n) = x_1^2 + x_2^2 + \cdots + x_n^2 $$ $$\sum^{j-1}{k=0}{\widehat{\gamma}{kj} z_k}$$]]></content>
      <categories>
        <category>Test</category>
      </categories>
      <tags>
        <tag>Test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
